\RequirePackage{etoolbox}
\csdef{input@path}{%
 {sty/}% 
 {figures/}%
}%

\documentclass{article}

\title{Notes for 547C (draft)}
\author{Alexandre Bouchard-C\^ot\'e}

\input{macros/typesetting-macros} 
\input{macros/basic-math-macros}
\input{macros/notation}



% use to create table of symbols
\makenomenclature


\begin{document}

\maketitle

\emph{Note: I will continue editing these files, so it might be wasteful to print much beyond the contents of the current lecture. Also I might make changes on what is covered beyond the first few chapters.}

\tableofcontents

\newpage

\section{Motivations}

\point{Why learn probability theory?} 
\begin{enumerate}
	\item Probability theory is a fundamental tool in statistics, computer science, physics, econometrics, and many more traditionally quantitative fields. It is also quickly gaining in importance in fields making the quantitative transition, for example biology, linguistics, sociology, and many more. 
	\item The theory is beautiful in its own right. Probability Theory can also be approached as a branch of pure mathematics. 
\end{enumerate}
 
Both of the above points are excellent motivations. However by the nature of this course (we are in a stats department!), I will focus on the first point above and heavily use practical motivations throughout the notes. 

Why is probability a fundamental tool in so many fields? Because Probability Theory is useful for creating \emph{models}. 

\point{Models} are sketches of reality that capture the essential of a problem while being amenable to mathematical analysis. Probability-based models are great at incorporating phenomena like uncertainty and non-linearity. Moreover, compared to other types of models you might be familiar with (e.g. linear algebra or calculus based), they arguable tend to be more resilient to \emph{mis-specification}, i.e. they can recover from certain mismatches between the model and reality (data) in the sense of still giving good predictions.

There are many other motivations. I will just mention two more quickly:

\point{From model to prediction:} in Bayesian statistics, probability theory is essential not only to formulate models, but also to make predictions. Bayesian statisticians construct a probability model in which both the known quantities (data) and the unknown quantities are modelled using random variables. In Bayesian inference, prediction then involves  conditioning on the data.  We will use Bayesian statistics as a recurrent example when talking about conditioning. 

\point{The computational power of randomness.}  Probability Theory arises in a surprising way in the subfields of computer science concerned with the design and analysis of algorithms. Researchers have found since the 1940s many problems where the best way to solve deterministic problems is to introduce artificial randomness in the execution of the algorithm (an idea called \emph{algorithmic randomness}). Consider for example the problem of quickly approximating the volume of an arbitrary convex body. In a 1991 landmark paper \cite{dyer_random_1991}, Dyer, Frieze and Kanna devised the first provably efficient approximation algorithm, which crucially depends on algorithmic randomness to perform random walks. Moreover, it is known that no efficient deterministic algorithms can provide accurate approximations \cite{bfirfiny_computing_nodate}. There are many instances where there are no known deterministic algorithms and where algorithmic randomness is necessary to scale to large problems.  


\section{Foundations}

\point{On being formal:}  An important thing to realize is that despite the fact that Probability Theory is used to model uncertainty, it is as formal as any other fields of mathematics (despite what you may have been led to believe if you took a course using the standard undergraduate way of teaching). Formalization of the field was achieved by Kolmogorov in the 1930s, when he realized that the same tools used to formalize the notions of area and volume (measures) could be applied to probabilities. 

\point{On intuition:} While being formal is useful, intuition is important too. Probability is very connected to the real world so using your intuition is also very useful in making guesses that you can then prove using theory.

\point{Why measure theory?} Measure theory is the standard framework used to formalize probability theory. There are several reasons to learn the basics of measure theory (i.e., as Pollard puts it, to be \emph{user} of measure theory). We will see some of these technical motivations as we go along: 
\begin{enumerate}
	\item Unified treatment of things are taught separately in naive undergraduate probability course: discrete/continuous, univariate/multivariate. 
	\item Certain results about expectations are easier to state and hold more generally under the measure-theoretic definition of expectations.
	\item Establishing independence can in certain case be much easier under the measure theoretic definition of independence.
\end{enumerate}
However in my view the main motivation is that a big chunk of the literature is written in the language of measure theory. This course will prepare you so that you can be fearless. 

The danger is to be too formal and that the notation gets in the way of the intuition. I will avoid this pitfall. I will also skip the tedious details that are not useful in practice, e.g. certain proofs of existence, especially if they do not reveal a technique more broadly applicable 


\subsection{The basic vocabulary of probability}

The axioms of probability, formulated by Kolmogorov in the 1930s, provide a vocabulary and basic set of rules used in everything that follows. We look at how they become alive by showing how they are used to build two simple models (the second, not as simple as it initially looks!).

\point{Recurrent example:} imagine an infinite railroad, modelled by the integers $\Z = \{\dots, -3, -2, -1, 0, 1, 2, 3, \dots\}$. A train starts at position zero. At every time step,the train operator flips a coin. If the coin shows heads, the train moves left by one unit. Otherwise, the train moves to the right by one unit. 

We will look at two versions of this example, version A, where there is fuel limit which restricts the train to 3 moves, and version B, where there is no fuel limit. 

Let us start with version A. 

\point{Outcomes/scenarios:} several scenarios can arise from the dice-driven train ``story'' or ``experiment''. For example the train could go left, right, left. Or it could go left, right, right. We will call these two scenarios, two \emph{outcomes}. The usual letter for one outcome is $\omega$ or $s$. 

To help visualization, we will draw one outcome in the time series example as a graph where the x-axis is time $t$ and the y-axis is the position at time $t$. Here is one example for version A (finite fuel):
\begin{center}
\includegraphics[width=0.5\linewidth]{figures/train1}
\end{center}
and for version B (infinite fuel):
\begin{center}
	\includegraphics[width=0.5\linewidth]{figures/train2}
\end{center}

\point{Definition:} The set of all possible outcomes is called the \emph{sample space}, which I will denote by $\Omega$ or $S$. 

\point{Example:} what is the size or cardinality of $\Omega$ (i.e. the number of elements in it) for our recurrent example version A? I will denote the size by $|\Omega|$. Answer: $2^3$ because there are three bits each free to take two possible values, $2 \cdot 2 \cdot 2$. We conclude that the probability of any given outcome in version A is $1/8$. Note that 
\[ \sum_{\omega \in \Omega} \text{probability of outcome $\omega$} = 1. \]
Here is a picture for the train version A example:
\begin{center}
	\includegraphics[width=0.5\linewidth]{figures/outcomes}
\end{center}

\point{Science-fiction interpretation:} if you read or watch science fiction (e.g. check out ``The man in the High Castle'' by Philip K. Dick), you may think about an outcome as encoding all the information for one universe, and the the set of all outcomes (the sample space) as the \emph{multiverse}. 

\point{Note:} an outcome may contain many bits of information. We only need one of those bits to get two different outcomes. I.e. outcomes can be very similar, but not identical. 

\point{Definition:} a set of outcomes is called an \emph{event}.

\point{Example:} events are often created using a property defining what outcomes are in the event. For example, consider the set of outcomes where the train goes left in the second time step, $A_1$. We use the terminology ``the event that the train goes left in the second time step.''  What is its size? Notice that events that are described by a small number of properties tend to be large, while event that are described by a larger number of properties tend to be smaller. 
\begin{center}
	\includegraphics[width=0.3\linewidth]{figures/event}
\end{center}

\point{Partitions:} often we consider not one event but several events at once. For example we want to categorize outcomes into sub-cases. This can be done using partitions. A \emph{partition} of a set $E$ (for example $E = \Omega$), is a collection\footnote{Just a synonym for set.} of events $B_1, B_2, \dots$ such that (1) $B_i \cap B_j = \emptyset$ for all $i \neq j$ and (2) $\cup B_i = E$. Each $B_i$ is called a block.

\point{Decision trees} are useful to organize a hierarchy of events. Each node in the tree is an event. At the root, we put $\Omega$. When $|\Omega| < \infty$, at the leave we have \emph{singletons}, i.e. sets with only one element. In between, each node of the tree is split into subcases. More formally, say we are at a node corresponding to event $E$. Pick a \emph{partition} $B_1, B_2, \dots$ is of $E$. Then the children of $E$ in the decision tree are defined as the blocks of the partition splitting $E$.
\begin{center}
	\includegraphics[width=0.5\linewidth]{figures/decision-tree}
\end{center}

\point{Additivity:} it is not too hard to see in example A that the probability of \emph{disjoint} events (i.e. non-intersecting events) can be added up. E.g.:
\begin{align*} 
&\text{probability go left in first turn} + \text{probability go right in first two turns} \\ 
&= \text{probability go left in first turn or right in first two turns}. 
\end{align*} 
Both sides are equal to $1/2 + 1/4$.

\point{Note:} it is not too hard to see in example A that the probability of \emph{overlapping} events (i.e. intersecting events) cannot always be added up. E.g.:
\begin{align*} 
	&\text{probability go left in first turn} + \text{probability go left in first two turns} \\
	&\neq \text{probability go left in first turn or left in first two turns}.
\end{align*}
The left hand side is equal to $1/2 + 1/4$ while the right hand side is equal to $1/2$. 

\point{A routine task} in probability problems consists in expressing events of unknown probability in terms of events of known probability. 

\point{Exercise/example:} express, in version A of the train example, the event that the train eventually returns home (position zero), in terms of the events: 
\[ A_t = \text{event that the train goes left in the $t$-th time step},\]
which have known probability. Do it using only the set theoretic operations $\cup$, $\cap$, and set complement $A_t^\complement$.


\subsection{Motivating the axioms of probability}

\point{Difficulty pre-Kolmogorov/pre-1930s:} people were already very good at doing computations for things like version A of the train. But surprisingly, they were running into serious foundational problems when trying to formalize the theory for version B of the problem!

\point{Difficulty:} in version B, we cannot always define probability of events by adding up probability of individual outcomes. We cannot have these three things:
\begin{align*}
	\text{probability of one path} &= \lim_{t \to \infty} (1/2)^t = 0 \\
	\text{probability of $\Omega$} &= 1 \\
	\sum_{\omega \in \Omega} \text{probability of outcome $\omega$} &= \text{probability of $\Omega$}.
\end{align*}
The problem is that each individually appears to make sense (the third one, in the light of the disjoint additivity observation), but they are inconsistent! We will see that the third one is the one causing problems.

\point{Root cause of the problem:} in version $B$, the set $\Omega$ is \emph{uncountably infinite}. Recall that a set is countable if we can come up with a list of all the elements in it. More formally, a set is countable if we can come up with a surjective function $f$ taking as input an integer and returning an element in $B$ (think of the input of $f$ as the position in the list, and the output as the item listed in that position). The terminology surjective means $\{f(i) : i \in 0, 1, 2, \dots\} = \Omega$, i.e. the list is exhaustive. To see why the set $\Omega$ of all infinite train paths is uncountable, argue as follows: suppose on the contrary that there was a list (i.e. suppose you claim to come up with a surjective function $f$ listing all the paths). I will show you there must be at least one path not in your list, an ``outlier.'' Recall that a path is just a list of coin toss, say encoded as 0 and 1. Here is a picture followed by a description:
\begin{center}
	\includegraphics[width=0.5\linewidth]{figures/cantor-diag}
\end{center}
\begin{enumerate}
	\item I start by building the {\bf first} coin toss in my outlier. I look at the first toss of the first element in your list, $f(1)_1$. If I see a 0, I set my first toss to 1. If I see a 1, I set my first toss to 0. 
	\item Then I build the {\bf second} toss in my outlier. I look at the {\bf second} toss of the {\bf second} element in your list, $f(2)_2$. If I see a 0, I set my second toss to 1. If I see a 1, I set my second toss to 0.
	\item {\bf Third} toss: I look at the {\bf third} toss of the {\bf third} element in your list, $f(3)_3$. If I see a 0, I set my third toss to 1. If I see a 1, I set my third toss to 0.
	\item etc.
\end{enumerate}
By construction, the outlier cannot be in your list! We conclude the set $\Omega$ of all infinite coin tosses is not countable (i.e., \emph{uncountable}). 

\point{Remedy:} here is the first part of Kolmogorov's insight (in turn based on insights from the then-nascent theory of measure)
\begin{enumerate}
	\item Give up defining the probability by assigning it to single outcomes.
	\item Instead, assign probability to events. I.e. define $\P(A)$ for  event $A$, not $\P(\omega)$ for outcome $\omega$. 
	\item The function $\P$ is now defined on a much bigger space! Many functions defined on that large space make no sense (e.g. we could have $\P(\Omega) \neq 1$ which does not make sense). Let us use the intuition gained with example version A to extract what are the things we want to assume on $\P$. The list of what to include in order to get an interesting theory is surprisingly short:
	\begin{enumerate}
		\item Additivity of disjoint events.
		\item $\P(\Omega) = 1$.
	\end{enumerate}
\end{enumerate}

\point{Second difficulty:} we need to be careful about how we define the disjoint additivity axiom! If we allowed additivity over uncountable collections of events, we would be back the ``paradox'' at the beginning of the section, i.e. that $\P(\{\omega\}) = 0$ but \[ \P(\cup_{\omega \in \Omega} \{\omega\}) = P(\Omega) = 1. \]

\point{Remedy (continued):} assume disjoint additivity only for countable collections of events. More formally, assume that if $A_1 \in \events, A_2 \in \events, A_3 \in \events, \dots$ is a countable collection of disjoint events ($i\neq j \Longrightarrow A_i \cap A_j = \emptyset$), then 
\[ \P(\cup A_i) = \sum_{i=1}^{\infty} \P(A_i). \]

\point{Third difficulty:} a strange theorem of measure theory states that there exists no probability distribution defined on all events, $\P:2^\Omega \to [0, 1]$ such that $\P(\{\omega\}) = 0$ for all $\omega \in \Omega$.

\point{Intuition:} when $\Omega$ is countably infinite, $2^\Omega$ is very strange! E.g. some events cannot be described by any language (any languages that we know of proceeds by putting letters from a finite alphabet one after another; well, that can only produce a countable set of descriptions. So that mean certain paths in $\Omega$ cannot be described in  version B of the train example)! 

\point{Remedy (continued):} do not attempt to define $\P$ on all subsets of $\Omega$. Define it on a subset of events called a $\sigma$-algebra. Make the $\sigma$-algebra big enough to do all the computations we are interested in doing: taking countable unions, intersections, and complements. As we will see soon this solves our problem, in the sense that there is a $\events \subsetneq 2^\Omega$, $\P : \events \to [0, 1]$ such that $\P(\{\omega\}) = 0$ but for example $\P(A_t) = 1/2$.

 
\subsection{The axioms of probability} 

To summarize the discussion of the last section, a probability space contains three things:
\begin{enumerate}
  \item a set $\Omega$, called the sample space,
  \item a closed collection of events, $\events \subset 2^\Omega$, called a \sigmaalg,
  \item a probability measure (synonym: probability distribution), $\P : \events \to [0, 1]$.
\end{enumerate}
where:
\begin{enumerate}
  \item the \sigmaalg\ satisfies:
  \begin{enumerate}
    \item $\Omega \in \events$,
    \item $A_1 \in \events, A_2 \in \events, A_3 \in \events, \dots, \Longrightarrow \cap_{i=1}^\infty A_i \in \events$,
    \item $A \in \events \Longrightarrow A^\complement \in \events$.
  \end{enumerate} 
  \item and the probability measure satisfies:
  \begin{enumerate}
    \item $\P(\Omega) = 1$,
    \item if $A_1 \in \events, A_2 \in \events, A_3 \in \events, \dots$ is a countable collection of disjoint events ($i\neq j \Longrightarrow A_i \cap A_j = \emptyset$), then 
    \[ \P(\cup A_i) = \sum_{i=1}^{\infty} \P(A_i). \]
  \end{enumerate}
\end{enumerate}

\point{Measure:} a measure space is a very close cousin to a probability space. The axioms are identical, except for two changes:
\begin{enumerate}
	\item We remove the bound $[0, 1]$ and replace by $[0, \infty)$, and use the terminology measure (often denoted $\mu$) instead of probability, $\mu : \events \to [0, \infty)$.
	\item We modify $\P(\Omega) = 1$ into $\mu(\emptyset) = 0$. 
\end{enumerate}


\subsection{Some basic properties}\label{sec:basic-properties}

Some examples of basic properties we can derive from these axioms:
\begin{enumerate}
  \item $\P(A^\complement) = 1 - \P(A)$, \label{property:complement}
  \item if $A$ and $B$ are events that are not necessarily disjoint, $\P(A \cup B) = \P(A) + \P(B) - \P(A \cap B)$. \label{property:additivity-overlap}
\end{enumerate}

\point{Idea:} partition sets into disjoint bits, so that the disjoint additivity can be used. For part~\ref{property:complement}, use that $A$ and $A^\complement$ form a partition of $\Omega$. For part~\ref{property:additivity-overlap}, define the set subtraction as $E\backslash F = \{e \in E : e \notin F\}$, and use the partition $A\backslash B, B \backslash A, A \cap B$.

\point{Exercise:} write down the argument.

\subsection{Probability spaces as models}

\subsubsection{Coin tosses}

\point{Question:} you toss two coins, what is the probability of two heads? Note that you are not able to tell the two coins apart.

\point{Sample space:} which of these should we pick?
\begin{enumerate}
  \item $\Omega_1 = \{(H, H), (H, T), (T, H), (T, T)\}$
  \item $\Omega_2 = \{\{H, H\}, \{H, T\}, \{T, T\}\}$. 
\end{enumerate}

The best answer is $\Omega_1$, but why? Choosing between these two (essentially, selecting one of these two models) is not part of probability theory per se. Use have to use your intuition about the real world here. For example, note that if you painted one coin red and one blue, the setup of this experiment would not have changed. Hence, the model that is most useful is the one that uses lists even though we could not observe this distinction. Probability theory comes in once we have built a model, at which point inference can be carried using mathematical principles. Probability theory can help selecting model though, for example by making certain predictions for a given model, which can then be tested (for example, the long term behavior of frequencies, which is mathematically understood for a wide range of probability models).


\subsubsection{Reliability}\label{sec:reliability}

Reliable systems replicate a critical component (e.g. a power supply in a computer server) so that the whole system works as long as at least one of the two copies works. Consider an assembly line for computer servers. Suppose the first power supply assembly line is observed to put in a power supply that works $60\%$ of the time. The second  power supply assembly line  is observed to put in a power supply that works $70\%$ of the time. At delivery, both power supplies work $40\%$ of the time. 

\point{Exercise:} What is the probability that both power supplies are broken at delivery? Hint: the answer is not $12\%$. 

If you answered $12\%$, you are using a property that is \emph{not} built into (or derivable from) the axioms of probability: namely that if $\P(A \cap B) = \P(A) \P(B)$. It is an extra assumption called \emph{independence} of the events $A$ and $B$. It is kept separate because there are situation where it is useful to describe the world, and others where it is not (can you imagine a scenario where the two assembly lines are not independent?). This contrast with the disjoint union axiom of probability, which models a universal aspect of reality. 


\subsection{Statistical models}

Statistical models are built using probability models. Consider the following example:

\point{Estimation:} consider the train example, but where instead of a standard coin being used to make the decision at each step (50\%-50\% to go left or right), a \emph{biased} coin is used, i.e. where the train goes left with probability $\theta\in [0, 1]$ and right with probability $1 - \theta$. The problem is that we do not know $\theta$! Instead, we try to reconstruct $\theta$ from data (observed paths). This problem, point estimation, is one an important type of problems considered in statistics. 

\point{Frequentist models:} use not one but many probability distributions all defined on a shared space $(\Omega, \events)$:
\[ \text{frequentist model} = \{ \P_\theta : \events \to [0, 1], \theta \in \Theta\}. \]
If the index set $\Theta$ is some subset of $\R^d$, the model is called parametric, otherwise, it is call non-parametric.

\point{Bayesian models:} use only one probability distribution. Augment the space $\Omega$ to include the unknown quantity $\theta \in [0, 1]$, i.e. $\bar \Omega = [0, 1] \times \Omega$. We will go over more detail when we talk about conditioning.


\subsection{Simple examples of $\sigma$-algebra}

We have seen one ``foundational'' motivation for $\sigma$-algebras. We will also see a more useful motivation soon which is that $\sigma$-algebras can encode rules of a game, but we will need to define random variables first. For now, let us look at some examples to make this concept a bit more concrete.

\begin{description}
  \item[Power set:] the power set $2^\Omega$ is always a $\sigma$-algebras on $\Omega$. For example \[\events_0 \defeq \{\{a, b, c\}, \{a, b\}, \{a, c\}, \{b, c\}, \{a\}, \{b\}, \{c\}, \emptyset\}\] is a $\sigma$-algebras on $\Omega \defeq \{a, b, c\}$.
  \item[Another discrete example:] the collection of events \[\events_1 \defeq \{\{a, b, c\}, \{a, b\}, \{c\}, \emptyset\}\] is also a \sigmaalg\ on $\Omega \defeq \{a, b, c\}$.
  \item[A non-example:] the collection $\events_2 \defeq \events_1 \cup \{\{a\}\}$ is not a \sigmaalg. Why? We have $\{a\} \in \events_2$ yet $\{a\}^\complement \notin \events_2$.
\end{description}


\subsection{More interesting examples of $\sigma$-algebra via generation}\label{sec:generated}

Let $\generators$ denote a collection of events. The machinery of this section is useful when the collection $\generators$ is ``broken,'' i.e. when it is not  a \sigmaalg\ (example: $\generators \defeq \events_2$ above). 

We would like to ``repair'' $\generators$ by adding more events until we get a closed collection of event. How can we do this is such a way that the output of the repair process is unique and well-defined? 

\begin{enumerate}
  \item \label{item:inter-sigma} Let $\events$ and $\events'$ be two \sigmaalg\ on $\Omega$. \point{Exercise:} convince yourself that their intersection $\events \cap \events'$ is also a \sigmaalg.
  \item The result in \ref{item:inter-sigma} can be generalized: if we have any collection of $\sigma$-algebras $\{\events_\alpha : \alpha \in I\}$, where $I$ is some index set (not necessarily countable), then \[ \bigcap_{\alpha \in I} \events_\alpha \] is also a \sigmaalg.
  \item Let us pick \[\{\events_\alpha : \alpha \in I\} = \{\events_\alpha : \events_\alpha \text{ is a }\sigma\text{-algebra containing }\generators\},\] then we get that the intersection of the $\events_\alpha$ is a \sigmaalg.
  \item We call this intersection the \sigmaalg\ \emph{generated} by $\generators$, denoted $\sigma(\generators)$. 
  \item Another way to think about $\sigma(\generators)$: the \emph{smallest \sigmaalg\ containing $\generators$}.
\end{enumerate}

\point{Some examples:}
\begin{enumerate}
  \item $\sigma(\events_2) = 2^{\{a,b,c\}}$.
  \item An important example where the generated \sigmaalg\ is smaller than $2^\Omega$: \emph{the Borel \sigmaalg}. To follow this example, it will help to first make the observation that infinite paths can be made in correspondence with the interval $[0, 1)$. This is done via the \emph{binary representation} of real numbers, which works schematically as follows: 
  \begin{center}
  	\includegraphics[width=0.3\linewidth]{figures/binary}
  \end{center} 
  This correspondence shows that many events of interest can be expressed as intervals in $[0, 1)$. For example what is the interval corresponding to ``the train goes left in the first step''? The interval $[0, 1/2)$.
  So to be able to have these intervals in our $\sigma$-algebra, we proceed as follows:
  \begin{enumerate}
    \item Take $\Omega \defeq [0, 1)$, and $\generators_\text{B} \defeq \{F : F\text{ is a finite collection of intervals}\}$.
    \item Convince yourself that $\generators$ is not a \sigmaalg.
    \item We call $\sigma(\generators_\text{B})$ the \emph{Borel \sigmaalg}, denoted $\borels$.
    \item Elements of $\borels$ are called Borel events.
    \item Some measure theory shows that $\borels$ is a proper subset of $2^{[0, 1)}$.
  \end{enumerate}
\end{enumerate}


\subsection{Simple examples of probability measures}

\begin{description}
  \item[Discrete probability measure:] assume you are given a countable sample space $\Omega$ and a function $p: \Omega \to [0, 1]$ called a \PMF. Define, for any event $A \in \events$: \[ \P(A) \defeq \sum_{\omega \in A} p(\omega). \] Check that this definition satisfies the axioms of probability. Note: do not confuse $p$ and $\P$, as they take different types of inputs!
  \item[Conditional probabilities:] fix an event $E$ which you can interpret as some data, with $\P(E) > 0$. Define a new probability distribution as \[ \P'(A) = \frac{P(A \cap E)}{\P(E)}. \] Check $\P'$ is indeed a probability. If $\P$ is some agent's belief before observing $E$, $\P'$ can be interpreted as the optimal way for the agent to update its belief after observing $E$. This updated belief is notated $\P'(\cdot) = \P(\cdot | E)$ and is called the conditional probability given $E$. From now on, if we write $\P(A | E)$ we will assume $\P(E) > 0$ implicitly.
  \item[The uniform probability measure:] some (surprisingly heavy) measure theory shows that there exists a probability measure on $\borels$ such that $\P([a, b)) = b - a$ for all $0 \le a \le b < 1$.
\end{description}

This second example is actually enough to build a rich theory! How? Using random variables and their distributions, which we will cover shortly.


\subsection{Computations for discrete models}\label{sec:computation-discrete}

We already have enough tools to formulate and solve many interesting problems involving \emph{discrete} models (i.e. with $|\Omega| < \infty$). 

\point{Exercises:} You can solve the following problems by explicitly enumerating all the possible scenarios. However, I ask here that you go beyond and that you formalize your reasoning, by (1) setting up a probability model using set theory only, no English allowed,\footnote{Obviously, in general I do recommend a mix of English and math for clarity, but probability beginners tend to rely too much on the former.} and (2), use the axioms of probability to solve the problem. All you need is the following two tools.

\point{Tool 1: chain rule.} For any event $A$, $B$, 
\[\P(A \cap B) = \P(A) \P(B | A) \;\; (= \P(B) \P(A | B)).\] 

\point{Tool 2: law of total probability (version 1). } Suppose $B_1, B_2, \dots$ is a partition of $E$, then
\[ \P(E) = \sum_i \P(E \cap B_i) \;\; \left(= \sum_i \P(B_i) \P(E | B_i)\right). \]

\point{Proposed strategy:} build the model and list the known (conditional) probabilities. Write the (conditional) probability you wish to compute. Then reintroduce the ``missing'' variable in this last probability using the law of total probability and the chain rule.  

\begin{enumerate}
	\item Prove that the two ``tools'' follow from the axioms of probability.
	\item An HIV test has the following two modes of failure:
	\begin{itemize}
		\item When a patient has the disease, the test will still be negative 2\% of the time (false negative)
		\item When a patient does not have the disease, the test will turn positive 1\% of the time (false positive)
	\end{itemize}
	Given that the test is positive, what is the updated (posterior) probability that the patient is indeed affected by HIV?
	% ~33 
	Suppose now 25\% of the population has HIV.
	Given that the test is positive, what is the updated (posterior) probability that the patient is indeed affected by HIV? What happens if the prevalence of the disease is very small instead?
	% ~97%
	\item There are 3 kindergarten classrooms and 9 kindergarten students in a school. The children are lined up and assigned to classrooms in turn. The principal claims the assignment is uniform over all possible assignments. The classrooms 
	have the following capacities: 
	\begin{itemize}
		\item Classroom (a): 4 students 
		\item Classroom (b): 3 students
		\item Classroom (c): 2 students
	\end{itemize} 
	You are fifth in the list of children. What is the probability that you get assigned to class (a)? 
\end{enumerate}


\subsection{Exercise set 1}

\begin{enumerate}
	\item Solve the exercise in Section~\ref{sec:basic-properties}
	\item Solve the exercise in Section~\ref{sec:reliability}
	\item Solve the exercise in Section~\ref{sec:computation-discrete}
\end{enumerate}


\subsection{Solutions for exercise set 1}

\subsubsection{Basic properties}

First, not that additivity for a countable collection implies additivity for a pair, by taking $A_3 = A_4 = A_5 = \emptyset$. Since $A$ and $A^\complement$ are disjoint, $P(A) + P(A^\complement) = 1$, and hence the first simple property. 

For the second property we make use of the following:
\begin{enumerate}
	\item $\{A\backslash B, A \cap B\}$ is a partition of $A$; hence $\P(A\backslash B) + \P(A \cap B) = \P(A)$;
	\item $\{B\backslash A, A \cap B\}$ is a partition of $B$; hence $\P(B\backslash A) + \P(A \cap B) = \P(B)$;
	\item $\{A\backslash B, B \backslash A, A \cap B\}$ is a partition of $A \cup B$;  hence $\P(A\backslash B) + \P(B\backslash A)+ \P(A \cap B) = \P(A \cup B)$.
\end{enumerate}
Now substitute $\P(A\backslash B)$ in the equation in 3 above using equation in 1 above as well as $\P(B\backslash A)$ using equation in 2.


\subsubsection{Reliability problem}

Define $\Omega = \{(0,0), (0,1), (1,0), (1,1)\}$ where element $\omega = (i, j)$ encodes if the first power supply works ($i = 1$, otherwise $i=0$) and if the second power supply works ($j = 1$, otherwise $j=0$). Define $W_k$ as the event that power supply $k\in \{1, 2\}$ works, i.e. $W_1 = \{(1,0), (1,1)\}$ and $W_2 = \{(0,1), (1,1)\}$. We know:
\begin{enumerate}
	\item $\P(W_1 \cap W_2) = 4/10$, (by the way sometimes denoted just $\P(W_1 W_2)$), 
	\item $\P(W_1) = 6/10$,
	\item $\P(W_2) = 7/10$.
\end{enumerate}
The goal is to compute $\P(W_1^\complement W_2^\complement)$. 

First, get rid of the complement. How to do so? Use a result from set theory:

\point{De Morgan's ``Laws'':} distributing complements swap unions to intersections and vice versa, i.e. $(A \cup B)^\complement = A^\complement \cap B^\complement$;  $(A \cap B)^\complement = A^\complement \cup B^\complement$. E.g., using an example from wikipedia, the search query ``NOT (cars OR trucks)'' is the same as ``(NOT cars) AND (NOT trucks)''.

Then we have using De Morgan and the first property from previous question:
\[ \P(W_1^\complement \cap W_2^\complement) = \P((W_1 \cup W_2)^\complement) = 1 - \P(W_1 \cup W_2). \]

Finally, using the second property:
\[ \P(W_1 \cup W_2) = \P(W_1) + \P(W_2) - \P(W_1 \cap W_2) = 6/10 + 7/10 - 4/10 = 9/10. \]

Hence the answer is $1 - 9/10 = 1/10.$


\subsubsection{Discrete computation}

\point{Tools:} Chain rule follows directly from the definitions:
\[ \P(A) \P(B | A) = \P(A) \frac{\P(A \cap B)}{\P(A)} = \P(A \cap B). \]

To establish the law of total probability (version 1), first show that $\{E \cap B_1, E \cap B_2, \dots, \dots\}$ is a partition of $E$. The result then follows from countable additivity.

\point{Bayes rule:} For the HIV test, define the following events: $H$, the event that HIV is present, and $E$ the observation (a test turned positive). We know $\P(H) = 25\% = 0.25$, $\P(E | H^\complement) = 1\%$ and $\P(E^\complement | H) = 2\%$. To help visualize this, you may want to draw a decision tree where the first branching is done according to $H$ and $H^\complement$, and the second branching, by further refining via intersections with  $E$ and $E^\complement$. We seek to compute $\P(H|E)$.

Notice that $H$ and $H^\complement$ form a partition of $\Omega$. Using this fact and the law of total probability, we obtain what is called Bayes rule:
\begin{align*}
	\P(H|E) &= \frac{\P(H \cap E)}{\P(E)} \\
	&= \frac{\P(H) \P(E | H)}{\P(H) \P(E | H) + \P(H^\complement) \P(E | H^\complement)} \\
	&= \frac{0.25 \cdot (1-0.02)}{0.25 \cdot (1-0.02) + 0.75 \cdot 0.01} \approx 97\%
\end{align*}

Now what happens if the prevalence $\rho$ of the disease is very small? Generalizing the equation above, we get:
\[ \P_\rho(H|E) = \frac{1}{1 + \frac{1-\rho}{\rho}\frac{0.01}{1 - 0.02}}, \]
hence as $\rho \downarrow 0$, we get $\P_\rho(H|E) \to 0$, which is kind of surprising.

\point{Combinatorics problem:} 
\[ \Omega = \{(C_1, C_2, C_3):\{C_1, C_2, C_3\} \text{ partitions }\{1, 2, \dots, 9\}, |C_1| = 4, |C_2| = 3, |C_3| = 2\}. \]
The definition of uniform assignment is then, for any $S \subset \Omega$:
\[ \P(S) = \frac{|S|}{|\Omega|}. \]
Here we are interested in an even $T$ which can be described as:
\[ T = \{(C_1, C_2, C_3):\{C_1, C_2, C_3\} \text{ partitions }\{1, 2, \dots, 8\}, |C_1| = 3, |C_2| = 3, |C_3| = 2\}.  \]
A first solution is to compute $|\Omega|$ and $|T|$. 
To do so use a decision tree with in the first level, events capturing the assignment of the first classroom:
\[ E^1_C = \{\omega \in \Omega : \omega_1 = C\}, \]
then, at the next two levels, events capturing the assignment of the second and third classroom:
\[ E^i_C = \{\omega \in \Omega : \omega_i = C\}. \]
Using the fact that the number of subsets of size $k$ from an inventory of size $n$ is $\binom{n}{k}$, you will find that $\Omega$ can be decomposed as a regular tree with branching factors $\binom{9}{4}$, $\binom{5}{3}$, $1$, and thus:
\[ |\Omega| = \frac{9!}{4! 3! 2!}. \]
Similarly for $T$, 
\[ |T| = \frac{8!}{3! 3! 2!}, \]
thus $\P(T) = 4/9$.

A second solution:
\[ \Omega = \{(a_1, a_2, \dots, a_9) : \cup \{a_i\} = \{1, 2, \dots, 9\}\}, \]
where $a_i$ is the assignment of student $i$, identifying $\{1, 2, 3, 4\}$ as the seats for the first classroom, $\{5, 6, 7\}$, for the second. Then use the events $E_i = \{\omega : \omega_5 = i\}$. We have $|E_i| = |E_j|$ and the event of interest is $E_1 \cup E_2 \cup E_3 \cup E_4$. Thus:
\[ \P(E_1 \cup E_2 \cup E_3 \cup E_4) = \frac{4 |E_1|}{9 |E_1|} = 4/9. \]


\subsection{Random variables}

\point{Informal definition:}
\begin{itemize}
  \item A random variable if often used to encode a measurement, for example, in the previous train example, whether the train goes left or right at time step 3.
  \item Random variables are used to model partial observability of the world. In contrast to each outcome $\omega \in \Omega$, which contains all the information possible within the model, a random variable can be defined to ``forget'' information. For example, you may only known the position of the train at time steps 1 and 6, and what happens in between is unknown.
  \item Another use of random variables is to express a quantity that is unknown, but that we would like to have the probability of. A concept that I call a ``query.''
\end{itemize}

\point{Pre-requisite for the formal definition:} if $f : S \to T$ is some function, we can lift function evaluation, which takes as input points and returns points $f(x) \in T$ for $x \in S$, into function evaluation taking as input a set and returning a set, i.e. for $A \subset S$, $f(A) = \{f(x) : x \in A\}$. 
\begin{center}
	\includegraphics[width=0.3\linewidth]{figures/maps}
\end{center}
Note that we abuse the notation and use the same symbol for lifted function evaluation, but no confusion can arise because of the type of the input. We do the same for lifted function inverse: for $B \subset T$, $f^{-1}(B) = \{x \in S : f(x) \in B\}$. The lifted inverse is nice because it always exists (the set it returns can be empty or have cardinality larger than one, but it is still a set.

\point{We can now formally define random variables:}
\begin{enumerate}
  \item Let $(\Omega, \events, \P)$ denote a probability space,
  \item then a (real) random variable $X$ is:
  \begin{enumerate}
    \item a map, $X : \Omega \to \R$,\footnote{Technically, we add two points to the real line, $+\infty$ and $-\infty$, to ensure for example that limits of say increasing random variables are guaranteed to be random variables even if the sequence diverges for some outcomes $\omega$.} 
    \item \label{rv:condition} such that $X^{-1}(A) \in \events$ for all $A \in \borels$.
  \end{enumerate}
\end{enumerate}
\begin{center}
	\includegraphics[width=0.6\linewidth]{figures/rvs}
\end{center}

\point{Why} do we need condition \ref{rv:condition}? Often, we will ask questions like ``what states of the worlds can yield an observation in $A$?'' In set theory, the set of such states (outcomes) is $X^{-1}(A)$. Now, we want to be able to compute the probability of this set of outcomes, so we require that $X^{-1}(A)$ be an event (i.e. in the \sigmaalg).
\begin{center}
	\includegraphics[width=0.5\linewidth]{figures/inverse}
\end{center}

\point{Note:} random variables are especially interesting when we define more than one on the same sample space. 

\point{Synonym:} ``$X$ is a measurable function.'' 

\point{Notes:}
\begin{itemize}
  \item Observations are not always real numbers, for example, they could be colours, the nodes in a discrete graph, or even a graph. Let $\states$ denotes the set of say all colours that could be potentially observed, $\states = \{$blue, red, yellow$\}$. We can modify our definition above to get colour-valued random variables (terminology: ``random colours''). This is done as follows:
  \begin{itemize}
    \item In the definition of real random variable, replace ``$\R$'' by ``$\states$.''
    \item Since we need a collection of sets on $\states$ in \ref{rv:condition}, let us assume we have a (second) \sigmaalg\ $\events_\states$ on $\states$ as well as on $\Omega$.
    \item Therefore, we see that all we really need is a \sigmaalg\ on the input space $\Omega$, and one on the output space \sigmaalg. Notation: a random colour is a $(\events \to \events_\states)$-measurable function.
  \end{itemize}
  \item Using this idea, we will later define random vectors (vector-valued random variable), random graphs, random sets, random functions, and even random probability measures!
  \item (Real) random variables and random vectors are special though, as they will later allow us to define the notion of expectation (not possible with colours, as we cannot for example ``add colours''). More on this later!
\end{itemize}

\point{Terminology:} values in the set $\states$ are called \emph{realizations}, and are often denoted with the small-cap version of the random variable, e.g. a realization of $X$ is denoted $x = X(\omega)$. This distinction is useful for example to distinguish $f(x)$, which is just function evaluation, from $f(X)$, which is the construction of a new random variable obtained by composing $f$ and $X$.

\point{Warning:} make sure you follow the type conventions! For example, $\P($red$)$ or $\P(X)$ are not defined!\footnote{Some authors do give a meaning to the latter, but it is not what you think! We will see this meaning, namely the expectation of $X$, later.}


\subsection{Compositions of random variables}\label{sec:composition-of-rv}

\point{Recall:} $g \circ h$ means a new function that first use $g$, then $h$: $g \circ h(x) = g(h(x))$. 

\point{Convention:} use capital letters only for the random variables mapping elements from the sample space $\Omega \to \states$. Use standard function notation ($g, h$, etc) for subsequent transformations $g : \states \to \states'$  of the output of $X$. E.g. $g \circ X = g(X)$. 

\point{Exercise:} show that the composition $g(X)$ of a random variable $X$ with a measurable function $g$ is a random variable.

\point{Convention:} from now on, we will implicitly assume all functions involved are measurable. 


\subsection{The graph of a random variable}

In the special case where $\Omega = \R$, we can plot the \emph{graph} real random variables $X : \Omega \to \R$. While in practice $\Omega$ is usually much more complicated, looking at simple examples where $\Omega = \R$ is a powerful technique to get some intuition on several results we will cover in the next few weeks. Note that measurability is much weaker than continuity, so the the graph of random variables can be quite pathological in general. 
\begin{center}
	\includegraphics[width=0.3\linewidth]{figures/graph}
\end{center}


\subsection{The probabilist's event notation}\label{sec:probabilist}

It is tedious to write expressions like $X^{-1}(\{r \in \R : 6 \le r\})$. Probabilists noticed that the notation $(6 \le X)$ was not defined, and decided to give it a new, precise meaning: $X^{-1}(\{r \in \R : 6 \le r\})$. Some other examples:
\begin{itemize}
  \item $(X \in A) \defeq X^{-1}(A)$,
  \item $(X = x) \defeq X^{-1}(\{x\})$.
\end{itemize}
\begin{center}
	\includegraphics[width=0.5\linewidth]{figures/inverse}
\end{center}

\point{More generally:} \[(\text{logical statement }s\text{ containing a random variable }X) \defeq \{\omega \in \Omega : s(X(\omega))\text{ is true}\}. \]


\subsection{Constant random variables}

The simplest example of a random variable is a constant function, e.g. $X(\omega) = 42$ for all $\omega$ or $Y(\omega) = 3.14$ for all $\omega$, denoted $X = 42$ and $Y = 3.14$ respectively. These are boring but important building blocks. We will abuse notation and denote the random variables such as $X$ and $Y$ here by just $42$ and $3.14$ respectively. For example $Z + 3$ if a shorthand for the composition $g(Z, W)$, where $g(z, w) = z + w$ and $W(\omega) = 3$.

\subsection{Indicator random variables}\label{sec:indicator}

Often we need random variable taking binary values (either zero or one). These are called indicator random variables. 

For any set $A$, define the \emph{indicator function} or just indicator for short as follows: \[ \1_A(\omega) \defeq \1[\omega \in A] \defeq \bracearraycond{1 &\text{if }\omega\in A\\0&\text{otherwise.}} \]

The indicator random variable is just an indicator on an event.

If $A$ can be defined using the probabilist's notation introduced in Section~\ref{sec:probabilist}, for example if $A = (X = x)$, we will use the notation $\1(X = x)$ for $\1_A$.


\subsection{Modelling randomized algorithms as random variables}

\point{Recall:} we mentioned randomized algorithms as one of the motivations for learning Probability Theory. Randomized algorithms can be defined as algorithms having  access to the realization of one random variable. Since in practice we need several random variables, randomized algorithms use \emph{pseudo-random generators} to turn one random number into a potentially very large list of random variables. In this section we see how to model randomized algorithms and pseudo-random generators using random variables. 

\point{Example of a simple randomized algorithm:} which simulates 100 coin flips and counts the number of times the coin comes heads. More precisely, the algorithm takes as input a \emph{random seed} $r$ (an integer, but modelled in a computer as being in a finite range of values, from 1 and say $2^{32}$), and proceeds as follows:

\begin{enumerate}
	\item $s \gets 0$
	\item For $i = 1, 2, \dots, 100$:
	\begin{enumerate}
		\item Extract a pseudo-random coin flip from the least significant binary digit of $r$: \[x_i = r \mod 2.\]
		In other words, $x_i$ is equal to one if and only if $r$ is an odd number, otherwise it is equal to zero.
		\item increment the sum counter, $s \gets s + x_i$
		\item Get a new pseudo-random integer by ``flipping'' an artificial wheel of fortune (wheel because we do modular arithmetic modulo $m$, the number of slots in the wheel), first nudging it
		by a multiplicative amount $a$, then by a constant amount $c$:
		\[ r \gets (a r + c) \mod m.\]
	\end{enumerate}
	\item Return the sum $s$
\end{enumerate}

\point{Note:} Here, $m$, $a$ and $c$ are fixed constants. For example, the famous Numerical Recipes book \cite{press_numerical_2007} recommends $m = 2^{32}$, $a = 1664525$ and $c = 1013904223$.

\point{Illustration of how the flipping works:} from wikipedia, created by user Cmglee, distributed under CC BY-SA 3.0

\begin{center}
\includegraphics[width=0.9\linewidth]{figures/pseudo-random}
\end{center}

\point{Model for the algorithm?} Take $\Omega$ to be the set of integers $1, \dots, 2^{32}$. A seed corresponds to an outcome $\omega \in \Omega$. Given a seed, everything is deterministic in the algorithm. In particular, we can think about the value $x_1$ as the output of a deterministic function taking a seed as input. Let us call this function, $X_1 : \Omega \to \{0, 1\}$, and as our choice of letter suggest, it is indeed a random variable. Similarly, we can define random variables $X_2, X_3, \dots, X_{100}$, each one being a deterministic function (of increasing complexity) of the input random seed $\omega$. E.g.:
\[ X_2(\omega) = ((a\omega + c) \mod m) \mod 2. \]

\point{Practical importance.} This model illustrates a key design pattern useful when building any randomized algorithm: 
\begin{itemize}
	\item Make the random seed an explicit input of the program.
	\item Given this input, all computations should be deterministic.
\end{itemize}
This is useful because you can re-run exactly the same program, i.e. \emph{reproduce your results}. This comes handy if there is a bug you need to fix and you want to replay the crash to analyze in detail what happened. It is also handy if someone wants to replicate somebody else's published work.  

\point{Better pseudo-random generators:} the recipe used here ($r \gets (ar + c) \mod m$) is called a \emph{linear congruential generator}. This class of generators have been largely superseded by other methods, a good choice for example is the Mersenne Twister pseudo-random generator \cite{matsumoto_mersenne_1998}.


\subsection{Distribution of a random variable}\label{sec:distribution}

\point{Idea:} you give me a probability space $(\Omega, \events, \P)$ and a random variable $X : \Omega \to \states$, and I create a new probability $\P'$. This new probability is defined on the values $\states$ that the random variable takes.

\point{Definition:} for any $B \in \events_\states$, set $\P'(B) \defeq \P(X \in B)$.
\begin{center}
	\includegraphics[width=0.5\linewidth]{figures/inverse}
\end{center}

\point{Notation:} we denote this new probability $\P'$ by $\P_X$, and call it the \emph{distribution of $X$}.

\point{Exercise:} suppose $X$ is an indicator variable on an event $A$, with $p = \P(A)$. Find the distribution of $X$. The answer is called ``Bernoulli with parameter $p$'', denoted $\P_X = \Bern(p)$. The shorthand $X \sim \Bern(p)$ is also widely used in statistics. 

\point{Possible confusion:} ``probability distribution'' is a synonym of ``probability measure.'' Here we defined the ``distribution \emph{of} $X$,'' which is a specific way of constructing a probability measure.


\subsection{Cumulative distribution function}

\point{Idea:} a probability is a function taking inputs from a tricky space ($\events$). This makes it hard to plot naively. It would be nice to be able to summarize it with a function taking inputs in a more familiar space, $\R$.

\point{Note:} the following definition only works for $\states = \R$.

\point{Definition:} the \CDF\ of a random variable is given for all $x \in \R$ by: \[ F_X(x) \defeq \P(X \le x). \]
\begin{center}
	\includegraphics[width=0.3\linewidth]{figures/cdf}
\end{center}

\point{Exercise:} this is the same as $\P_X((-\infty, x])$.


\subsection{Equality in distribution}\label{sec:equal-in-distribution}

\point{Example:} Suppose the only probability distribution available in some programming language is the uniform distribution on $[0, 1)$. How can we transform it into a coin toss?

\point{An answer:} $X_1 = \1_{[0, 1/2)}$.

\point{Question:} is this answer unique? No! For example, $X_2 = \1_{[0, 1/4)} + \1_{[1/2, 3/4)}$ will also do!
\begin{center}
	\includegraphics[width=0.3\linewidth]{figures/eq-dist}
\end{center}

\point{Note:} 
\begin{enumerate}
  \item $X_1 \neq X_2$ (the two functions are not equal, for example $X_1(1/4) = 1 \neq 0 = X_2(1/4)$
  \item \label{item:dist} but: $\P_{X_1} = \P_{X_2}$.
\end{enumerate}

\point{Definition:} We call \ref{item:dist} \emph{equality in distribution}, denoted $X_1 \deq X_2$. 


\subsection{Densities (first definition)}\label{sec:density-first-def}

A random variable is said to have \emph{density} $f \ge 0$ if: \[ F_X(x) = \int_{-\infty}^x f(z) \ud z. \] Note that we will cover a more general definition of density later in this course.

\point{Example:} a density for the exponential distribution is given by \[ f(x) = \1[x \ge 0] \lambda e^{-x\lambda}. \]

\point{Exercise:} find an example where there is a $x$ such that a density has $f(x) > 1$.


\subsection{Limit properties of probability measures}\label{sec:limit-pr-measures}

\point{Lemma: monotonicity.} If $B \subset A$ are events, then $\P(B) \le \P(A)$.

\point{Proof idea:} Use the ``donut decomposition,'' $A = B \sqcup (A \backslash B)$, where we use the symbol $\sqcup$ to denote a union while asserting that the two sets we are taking the union over are disjoint.
\begin{center}
	\includegraphics[width=0.3\linewidth]{figures/donut-1}
\end{center}

\point{Notation:} to express the following limit properties, we make use of the following overloaded notations,
\begin{itemize}
  \item Monotone real numbers limits:
  \begin{itemize}
    \item If $r_1 \le r_2 \le \dots$, and $\lim r_i = r$, we write $r_i \uparrow r$,
    \item If $r_1 \ge r_2 \ge \dots$, and $\lim r_i = r$, we write $r_i \downarrow r$.
  \end{itemize}
  \item Monotone set limits:
  \begin{itemize}
    \item If $A_1 \subset A_2 \subset \cdots$, and $\cup A_i = A$, we write $A_i \uparrow A$,
    \item If $A_1 \supset A_2 \supset \cdots$, and $\cap A_i = A$, we write $A_i \downarrow A$.
  \end{itemize}
\end{itemize}

\point{Monotonicity of probability measures:} 
\begin{itemize}
  \item $A_i \uparrow A \Longrightarrow \P(A_i) \uparrow \P(A)$,
  \item $A_i \downarrow A \Longrightarrow \P(A_i) \downarrow \P(A)$.
\end{itemize}

\point{Proof idea for the increasing case:} generalize the donut decomposition and write 
\[ A = A_1 \sqcup (A_2 \backslash A_1) \sqcup (A_3 \backslash A_2) \sqcup \cdots, \]
then use countable additivity to get:
\[ \P(A) = \P(A_1) + \lim_{n \to \infty} \sum_{i=1}^n \P(A_{i+1} \backslash A_i), \]
using our previous monotonicity property, and telescoping the sum inside the limit, we get:
\[ \P(A) = \P(A_1) + \lim_{n \to \infty} [\P(A_n) - \P(A_1)] = \lim_{n\to\infty} \P(A_n). \]


\subsection{Limit properties of CDFs}\label{sec:limit-properties-cdfs}

Since the \CDF\ is derived from a probability measure, it shares similar continuity property. But being a function from the real to $[0,1]$, these monotonicity properties coincide with familiar notions from elementary real analysis. 

\point{Notation:} Throughout this section, $F$ denotes the \CDF\ of some random variable $X$, $F \defeq F_X$.

\point{Monotonicity of \CDFs:} $x \le y \Longrightarrow F(x) \le F(y)$ ($F$ is monotone increasing).

\point{Proof:} $x \le y \Longrightarrow (X \le x) \subset (X \le y)$, so we can use monotonicity of the probability measure $\P$ to conclude the proof.

\point{Semi-continuity property:} 
\begin{enumerate}
  \item \label{item:cont1} $x_i \uparrow x \Longrightarrow$ the limit $\lim F(x_i)$ exists,
  \item \label{item:cont2} $x_i \downarrow x \Longrightarrow F(x_i) \downarrow F(x)$.
\end{enumerate}

\point{Proof idea for the decreasing case:} we have $(X \le x_i) \supset (X \le x_{i+1})$, so by monotonicity of $\P$, we get
\[ \P(X \le x_i) \downarrow \P(\cap A_i), \]
where $\cap A_i = (X \le x)$, therefore $\P(\cap A_i) = F(x)$.
\begin{center}
	\includegraphics[width=0.3\linewidth]{figures/limit-cdf}
\end{center}

\point{Reason:} for the asymmetry between semi-continuity property \ref{item:cont1} and \ref{item:cont2}. First, do the proof for the increasing case as an exercise. You will see that in the increasing case, the limit of the probabilities is given by $\P(X < x)$, which is not guaranteed in general to be equal to $F(x)$ (because of a potential point mass as $x$).

\point{Terminology:} functions that satisfy the semi-continuity properties (\ref{item:cont1} and \ref{item:cont2}) are called cadlag, coming from ``continue \`a droite, limite \`a gauche'' (French for ``continuous to the right, limits to the left'').

\point{Proposition:} let $F : \R \to [0, 1]$ be some function. The following are equivalent:
\begin{enumerate}
  \item \label{item:inv-cdf-eq1} The function $F$ is cadlag and satisfies the boundary conditions $\lim_{x \to -\infty} F(x) = 0$ and $\lim_{x \to +\infty} F(x) = 1$ (denoted $F(-\infty) = 0$ and $F(+\infty) = 1$),
  \item \label{item:inv-cdf-eq2} There is some random variable $X$ with $F_X = F$.
\end{enumerate}

\point{Proof idea:} we proved the main steps of \ref{item:inv-cdf-eq1}$\Longleftarrow$\ref{item:inv-cdf-eq2}. The main idea for the other direction is to use the following construction:
\[ X(\omega) \defeq \sup\{x : F(x) < \omega\}. \]

\point{Note:} this is a first instance of an important idea in this course, namely to identify the distributions of random variables with simpler types of functions.



\subsection{Building random variables with a prescribed CDF}\label{sec:inverse-cdf}

\point{Example/exercise:} simulate an exponential random variable (defined below) from a uniform distribution on $[0, 1)$, where an exponential random variable is defined as follows:

\point{Definition:} we say $X$ is a standard exponential random variable, a statement denoted $X \sim \text{Exp}(\lambda)$, if \[F_X(x) = \1[x \ge 0] (1 - e^{-x \lambda}). \]
\begin{center}
	\includegraphics[width=0.4\linewidth]{figures/cdf-exp}
\end{center}

\point{Hint:} use the construction from the ``proof idea'' of Section~\ref{sec:limit-properties-cdfs}, which is called the ``inverse CDF.''


\subsection{Measure}

A measure $\mu : \events \to [0, \infty)$ is defined in the same way as a probability measures, except that we remove the requirement that $\mu(\Omega) = 1$. Instead, we just ask that $\mu(\emptyset) = 0$.

\point{Example:} the Lebesgue measure is the same as the uniform probability distribution, except that it is defined on $\R$ instead of $[0, 1)$. 


\subsection{$\sigma$-algebra, revisited}

Often it is useful to put some restrictions on what a random variable can depend on. For example, a model may have two random variables, $Y$ which is observed, and $X$ which is unknown. A statistical estimator $\delta$ should only depend on $Y$, not $X$. 

One way to do this is to force $\delta$ to be a composition based on $X$ only: $\delta = f(X)$ for some $f$. 

There is another equivalent way to do that based on the $\sigma$-algebra generated by a random variable, defined as:
\[ \sigma(X) = \{ (X \in B) : B \in \borels\}, \]
(Exercise: this is a $\sigma$-algebra). 
In an optional question in the assignment, you will show: $\sigma(\delta) \subset \sigma(X)$ if and only if $\delta = f(X)$ for some measurable $X$.

We use the shorthand $\delta \in \sigma(X)$ for $\sigma(\delta) \subset \sigma(X)$.

\point{Exercise:} To get a feeling for what this means, consider $X$ as the value on two dice, and $Y$ as the sum. We want an estimator for $X_1$. Consider $\delta_1 = 6 \1[Y > 6] + 1 \1[Y \le 6]$ and $Y = X_1$. Check $\delta_1 \in \sigma(Y)$ but $\delta_2 \notin \sigma(Y)$.

\point{Answer:} 
\begin{align*}
\Omega &= \{ (i, j) : i,j \in \{1, 2, \dots, 6\} \\
\sigma(X) = 2^{\Omega} \\
\sigma(Y) &= \sigma(\{\{(1,1)\}, \{(2,1), (1,2)\}, \{(1,3),(2,2),(3,1)\}, \dots, \{(6,6)\}\}) \\
&= \sigma( \{ \{(i,j) : i + j = k\}   : k = 1, 2, \dots, 12  \} ) \\
\sigma(\delta_1) &= \sigma( \{ \{(i,j) : i + j > 6\}, \{(i,j) : i + j \ge 6\}  \} ). 
\end{align*}


\subsection{Exercises}

\begin{enumerate}
  \item Let $X$ be a random variable with a uniform distribution on $[0, 1)$. Draw a possible graph of $X$, a density for $X$, and the \CDF\ of $X$.
  \item Solve the exercise in Section~\ref{sec:generated}.
  \item Solve the exercise in Section~\ref{sec:composition-of-rv}.
  \item Solve the exercise in Section~\ref{sec:density-first-def}.
  \item Solve the exercise in Section~\ref{sec:inverse-cdf}.
  \item If $X_1 \sim F$ (a notation that means that $\P(X_1 \le x) = F(x)$), and $X_1 \ge 0$, find the \CDF\ of $X_2 \defeq X_1^2$.
\end{enumerate}
 

%\subsection{Solutions}
%
%\begin{enumerate}
%  \item The three pictures should be as follows:
%\begin{center}
%	\includegraphics[width=0.9\linewidth]{figures/3-viz}
%\end{center}
%  \begin{enumerate}
%    \item Graph of $X$: The $x$-axis should be a bounded segment labelled $\Omega$. The $y$-axis should be the full real line. There are several choices for the function. For example the line $y = x$ or $y = 1 - x$ on the interval $[0, 1)$ are acceptable. Other choices are possible.
%    \item CDF: The $x$-axis should be the full real line. The $y$ axis should be the interval $[0, 1]$. The function should be zero in $(-\infty, 0]$, then affine in $[0, 1]$, then one in $[1, +\infty)$.
%    \item Density: the $x$ axis should be the full real line. The $y$ axis should be the positive real line. The function should be the indicator on the set $[0, 1)$.
%  \end{enumerate}
%  \item We need to check the three conditions given in the definition of \sigmaalg:
%  \begin{enumerate}
%    \item Since $\events$ is a \sigmaalg, $\Omega \in \events$ and since $\events'$ is a \sigmaalg, $\Omega \in \events'$, therefore $\Omega \in \events \cap \events'$.
%    \item We need to show that if $A_1, A_2, \dots$ are all in $\events \cap \events'$, then $\cap A_i \in \events \cap \events'$. By the definition of intersection, we have that $A_1, A_2, \dots$ are all in $\events$. Since $\events$ is a \sigmaalg, it follows that $\cap A_i \in \events$. By the same reasoning, $\cap A_i \in \events'$. Therefore, $\cap A_i \in \events \cap \events'$.
%    \item We need to show that if $A \in \events \cap \events'$, then $A^\complement \in \events \cap \events'$. By the definition of intersection, we have that $A \in \events$. Since $\events$ is a \sigmaalg, it follows that $A^\complement \in \events$. By the same reasoning, $A^\complement \in \events'$. Therefore, $A^\complement \in \events \cap \events'$.
%  \end{enumerate}
%  \item We have that $X: \Omega \to \states$ and $g : \states \to \states'$ are random variables. Let us denote by $\events_\Omega, \events_{\states}$ and $\events_{\states'}$ the \sigmaalg\ on $\Omega$, $\states$ and $\states'$ respectively. Let $A \in \events_{\states'}$. We have to show that $(g \circ X)^{-1}(A) \in \events_\Omega$. First, note that $(g \circ X)^{-1}(A) = X^{-1}(g^{-1}(A))$. Since $g$ is a random variable, $g^{-1}(A) \in \events_\states$. Next, since $X$ is a random variables, $g^{-1}(A) \in \events_\states$ implies that $X^{-1}(g^{-1}(A)) \in \events_\Omega$.
%   \item As in Section~\ref{sec:limit-properties-cdfs}, we pick $X(\omega) \defeq \sup\{x : F(x) < \omega\}$ (note that in this special case were $\Omega = [0, 1)$,  writing ``$F(x) < \omega$'' is well defined---for general $\Omega$ is would not).
%This construction has some nice properties:
%\begin{enumerate}
%  \item \label{point:inv-cdf-monotone} $X$ is monotone increasing (this follows directly from the definition),
%  \item \label{point:inv-cdf-incl-one} $X\circ F(x) \le x$ (draw a picture of a \CDF\ having flat regions to convince yourself),
%  \item \label{point:inv-cdf-incl-two}$F\circ X(\omega) \ge \omega$ (draw a picture of a \CDF\ having discontinuities to convince yourself).
%\end{enumerate}
%We first use properties \ref{point:inv-cdf-incl-one} and \ref{point:inv-cdf-incl-two} to prove the following set equality:
%\[ (X \le x) = \{\omega \in \Omega : \omega \le F(x) \}.\]
%We show that the LHS includes the RHS and vice versa:
%\begin{itemize}
%  \item $\{\omega \in \Omega : \omega \le F(x) \} \subseteq (X \le x)$:
%\begin{eqnarray*}
%\omega \le F(x) &\Longrightarrow& X(\omega)  \le X\circ F(x)\;\text{ (from \ref{point:inv-cdf-monotone})} \\
%&\Longrightarrow& X(\omega) \le x\;\text{(from \ref{point:inv-cdf-incl-one}).}
%\end{eqnarray*}
%  \item $(X \le x) \subseteq \{\omega \in \Omega : \omega \le F(x) \}$:
%\begin{eqnarray*}
%X(\omega) \le x &\Longrightarrow& F\circ X(\omega)  \le F(x)\;\text{ (monotonicity of CDFs)} \\
%&\Longrightarrow& \omega \le F(x)\;\text{(from \ref{point:inv-cdf-incl-two}).}
%\end{eqnarray*}
%\end{itemize}
%Finally, it follows that:
%\begin{eqnarray*}
%\text{CDF of }X &\defeq& \P(X \le x) \\
%&=& \P\{\omega \in \Omega : \omega \le F(x) \} \\
%&=& F(x) \;\text{ (by the definition of uniform probability).}
%\end{eqnarray*}
%Here in this special case: $X(\omega) = - \lambda^{-1} \log(1 - \omega)$.
%\item We have:
%\begin{eqnarray*}
%F_{X_2}(x) &\defeq& \P(X_2 \le x) \\
%&=& \P(X_1^2 \le x) \\
%&=& \P(-\sqrt{x} \le X_1 \le \sqrt{x}) \\
%&=& \P(X_1 \le \sqrt{x}) \;\text{(Non-negativity assumption)} \\
%&=& F(\sqrt{x}).
%\end{eqnarray*}
%\end{enumerate}


\section{Integration and expectation} 

\subsection{Overview}

\point{How to define the mean?} At an undergraduate level, this is usually done as follows for continuous random variables:
\[ \E[X] \defeq \int_{-\infty}^{+\infty} x f(x) \ud x, \]
where $f$ is the density of $X$. There are two limitations with this definition:
\begin{enumerate}
  \item it is not very intuitive (why multiply the density with an $x$?),
  \item we need a separate definition for discrete random variable (and what about cases where we have both continuous and discrete parts?).
\end{enumerate}

\point{Better definition:} the expectation is the area under the graph of $X$!

\point{Note:} we will need to generalize the notion of ``area,'' to cover cases where $\Omega \neq \R$.

\point{Terminology:} the definition of integral we will cover today is called the Lebesgue integral (not to be confused with the Lebesgue measure). It is the default definition in measure theory, and in general in probability theory.

\point{Note:} we will get the undergraduate definition of expectation of a continuous random variable as a special case arising when $X$ has a density. However, the Lebesgue expectation does not need to assume existence of a density.


\subsection{Notation, inputs and outputs}

The integral you know from calculus (called the Riemann integral) takes one input (a function $f : \R \to \R$) and return one real number.

In contrast, the Lebesgue integral needs \emph{two} inputs:
\begin{enumerate}
  \item a probability measure $\P : \events \to [0, 1]$, where $\events$ is a \sigmaalg\ on a sample space $\Omega$,
  \item a random variable $X : \Omega \to \R$.
\end{enumerate}

\point{Notations from real analysis:} you will see different notations depending on the author/community to encode this operator on two inputs, for example (these are all synonyms):
\begin{itemize}
  \item $\int X \ud \P$
  \item $\int X(\omega) \P(\ud \omega)$
  \item $\P X$
  \item $(\P, \mu)$.
\end{itemize}

In probability theory and Bayesian statistics, there is often a ``global'' probability $\P$. When this is the case:

\point{Notation in probability theory/Bayesian statistics:} 
\[ \E[X] \defeq \int X \ud \P. \]

In frequentist Bayesian statistics, there is often a collection of probabilities indexed by a parameter $\theta$, i.e. $\{\P_\theta : \theta \in \Theta\}$. When this is the case:

\point{Notation in frequentist statistics:}
\[ \E_\theta[X] \defeq \int X \ud \P_\theta. \]


\subsection{Generalizing the notion of the ``area'' of a ``rectangle''}

Let us start by defining the notion of area under the curve for something simple: an indicator function multiplied by a constant, $Z = a \1_A$. To make it easier to visualize, let us make this assumption from now on (we will relax it at some point later):

\point{Assumption:} assume that all random variables take values $\ge 0$.

Now if $A$ is an interval, the graph is just a rectangle! The height is $a$. What should be the base? Since we are given a probability, let us use it to measure the base, giving $\P(A)$ for the base. This suggests:

\point{Definition:} the Lebesgue integral for indicator function is given by 
\[ \E[Z] = \int Z \ud \P \defeq a \P(A). \]
\begin{center}
	\includegraphics[width=0.5\linewidth]{figures/rectangle}
\end{center}

\point{Note:} $A$ could actually be complicated (e.g., the Cantor set), but this definition still holds as long as $A \in \events$.

\point{Note:} there will be cases (especially when we talk about limits) where the base has measure zero, but the height is infinite. We would like our definition to return zero in these cases (since the function blows up on a negligeable set). For this reason, we define $0 \times \infty = 0$.

\point{Exercise:} compute the expectation of a constant.

\point{Important special case:} following from the definition directly, the expectation of an indicator on an event $A$ is just the probability of $A$, i.e. \[\E[\1_A] = \P(A).\] In light of the answer of the exercise in Section~\ref{sec:distribution}, we only need the distribution of the indicator random variable to compute its expectation. As we will see soon, this is always true, i.e. we only need the distribution of a random variable in order to compute its expectation. In other words, all random variables sharing the same distribution have the same expectation (and many distinct random variables do share the same distribution, see Section~\ref{sec:equal-in-distribution}). 

\point{Exercise:} based on the above special case, can you guess the general formula for computing the expectation of a random variable given only the distribution of the random variable, not the random variable itself?


\subsection{Area under the graph of ``simple functions''}\label{sec:area-simple-function}

\point{Simple function:} a simple function is a random variable of the form
\[ Y = \sum_{i = 1}^N a_i \1_{A_i}, \]
where the $A_i$ are assumed to be disjoint.

\point{Definition:} motivated by linearity we extend the definition of the Lebesgue integral to simple functions:
\[ \E[Y] = \int Y \ud \P \defeq \sum_{i=1}^N a_i \P(A_i). \]

\point{Note:} our previous definition for indicators is just a special case of this, so we are not contradicting ourselves.

\point{Property:} the above definition is motivated by linearity, and important property we want expectations to enjoy:
\[ \E[Y + Y'] = \E[Y] + \E[Y']. \]

\point{Exercise:} prove this property holds for the above definition of integral of simple functions.


\subsection{Area under graph of non-negative random variables}

Now, how to define the area under the graph of an arbitrary non-negative random variable? We make use of our previous definition for simple functions:

\point{Definition:} let $X \ge 0$ (meaning $X(\omega) \ge 0$ for all $\omega \in \Omega$), 
\[ \E[X] = \int X \ud \P \defeq \sup\left\{ \int Y \ud \P : Y\text{ is simple and } 0 \le Y \le X\right\}. \] 

\point{Exercises:} show $\E[X] \ge 0$ and monotonicity: $X \le Y \Longrightarrow \E[X] \le \E[Y]$. 


\subsection{Algorithmic construction}

To avoid having to deal with an uncountable collection we follow a two steps strategy:
\begin{enumerate}
  \item expresss the random variable to integrate $X$ as the limit of a sequence of increasing and simple random variables. This is done using: \\\point{Proposition:} (approximation by simple functions) For any random variable $X \ge 0$, there exists a sequence of random variables $0 \le Y_1 \le Y_2 \le \dots$ such that:
   \begin{enumerate}
     \item each $Y_n$ is simple, and
     \item for all $\omega \in \Omega$, $\lim_{n\to\infty} Y_n(\omega) = X(\omega)$ (this property is known as \point{pointwise convergence}, denoted $Y_n \to X$, or in this case since the r.v. are additionally increasing, $Y_n \uparrow X$).
   \end{enumerate}
  \item We will use the \point{monotone convergence theorem (MCT)} to exchange the limit and integral: if $0 \le Y_1 \le Y_2 \le \dots$ are non-negative random variables (not necessarily simple, although they are in this specific context), then 
\[ \underbrace{ \int (\lim_{n\to\infty} X_n) \ud \P}_{\text{hard!}} = \lim_{n\to\infty} \underbrace{\int X_n \ud \P.}_{\text{easier!}} \]
\end{enumerate}

\point{Proof idea} of the proposition on approximation by simple functions: 
\begin{enumerate}
  \item Recall that in the case of a Riemann integral, we do something similar, i.e. breaking the x-axis of the graph of $X$ into a grid and making this grid finer and finer.
  \begin{center}
  	\includegraphics[width=0.5\linewidth]{figures/riemann}
  \end{center}
  \item In general, this cannot work here, because the x-axis, $\Omega$, is not necessarily $\R$. 
  \item Idea: break the $y$-axis instead! Then use the inverse of the random variable $X^{-1}$, to get the $A_i$'s required in the definition of simple functions.
  \begin{center}
  	\includegraphics[width=0.7\linewidth]{figures/lebesgue}
  \end{center}
\end{enumerate}


\subsection{Proving tool: simple function approximation +  MCT}\label{sec:approxAndMCT}

The previous section sets the stage for a powerful proving strategy:
\begin{enumerate}
  \item Suppose you want to prove an identity involving expectations.\\\point{Example:} linearity for non-negative random variables $X, X' \ge 0$, \[ \E[X + X'] = \E[X] + \E[X']. \]
  \item First prove that the identity holds for simple random variables \\\point{Example:} that was an earlier exercise in the case of linearity.
  \item Then, use the approximation theorem to get simple $Y_n \uparrow X$ and $Y'_n \uparrow X'$. 
  \item Use MCT to conclude. \\\point{Example:}
\begin{eqnarray*}
\E[X + X'] &=& \E[ \lim (Y_n + Y'_n) ] \\
&=& \lim \E [Y_n + Y'_n] \;\;\;\text{ (we can use MCT here since }0 \le Y_n + Y'_n \uparrow X + X'\text{)} \\
&=& \lim ( \E[Y_n] + \E[Y'_n] ) \;\;\;\text{ (easy to prove since }Y_n, Y'_n\text{ are simple)} \\
&=& \lim \E[Y_n] + \lim \E[Y'_n] \;\;\;\text{ (properties of limits of real sequences)} \\
&=& \E[X] + \E[X'] \;\;\;\text{ (MCT again, twice).}
\end{eqnarray*}
\end{enumerate}

\point{Easy extension:} $\E$ is a linear operator: $\E[aX + b] = a\E[X] + b$.

\point{Note:} this proving strategy is incredibly useful in practice, in part thanks to how simple the statement of the monotone convergence theorem is. Similar statements with Riemann integrals are not as simple. Just this proving method arguably justifies the effort of learning to be a user of measure theory. 


\subsection{Integrals of random variables taking negative values}

For a general random variable $X$:
\begin{center}
	\includegraphics[width=0.8\linewidth]{figures/decomposition}
\end{center} 
\begin{enumerate}
  \item Write $X = X^{+} + X^{-}$, where $X^{+}$ and $X^{-}$ are the negative and positive parts respectively. For example, $X^{-} = \1[X < 0] X$. 
  \item Note: $-X^{-}$ is non-negative.
  \item Compute $I^{+} \defeq \E[X^+]$ and $I^{-} \defeq \E[-X^-]$.
  \item If both $I^{+} = I^- = \infty$, return an error (``the Lebesgue integral is not defined''),
  \item else define $\E[X] \defeq I^{+} - I^{-}$.
\end{enumerate}

\point{Terminology:} 
\begin{itemize} 
  \item If at least one of $I^{+}$ and $I^-$ is finite, we say the Lebesgue integral of $X$ is defined,
  \item when both $I^{+}$ and $I^-$ are finite, we say $X$ is \emph{integrable}, denoted $X \in \L_1$. 
\end{itemize}

\point{Exercise:} find a random variable $X \neq \L_1$ such that $\E X$ exists. Find a random variable $Y$ where $\E Y$ is not defined. 


\subsection{Integrals with respect to a measure}

So far, we have assumed that the Lebesgue integral was computed with respect to probability measure $\P : \events \to [0, 1]$. 

\point{Exercise:} go over the above argument again with a measure $\mu : \events \to [0, \infty)$ instead of a probability measure $\P$ and check that everything goes through.

\point{Notes:}
\begin{enumerate}
  \item The previous definitions are again special case of the new one, so we are not contradicting ourselves.
  \item The last definition is always well defined, but could be $+\infty$.
  \item However, the last definition seems hard to compute algorithmically because the sup is over an uncountable collection.
\end{enumerate}


\subsection{More on exchanging limits and integrals}

The monotone convergence theorem (Section~\ref{sec:approxAndMCT}) says that we can exchange integrals and limits \emph{when the sequence of function is increasing} ($X_1 \le X_2 \le \dots$). Is this necessary?

\point{Example showing that it is:} consider $X_n = n \1_{(0, 1/n]}$. 
\begin{center}
	\includegraphics[width=0.4\linewidth]{figures/limit-exchange-warn}
\end{center}

\point{Exercise:} compute $\lim X_n$ and $\E X_n$. Conclude that limits and integrals cannot be exchanged in this case.

\point{However:} there are non-monotone cases where you can exchange limits and integrals. For example, when they have \point{an integrable envelope}, defined as a random variable such that:
\begin{enumerate}
  \item $|X_i| \le Y$,
  \item $\E|Y| < \infty$.
\end{enumerate}

\point{Theorem:} (Dominated Convergence Theorem, DCT) if $X_1, X_2, \dots$ have an integrable envelope, and $\lim X_i$ exists, then $\lim \E X_i = \E \lim X_i$.


\subsection{Measure zero sets and almost sure statements}

As a direct consequence of the axioms of probability, an empty event has probability zero: $\P(\emptyset) = 0$. The converse is not true though. For example under the uniform probability an event that contains only a single point still has probability zero $\P(\{0.2\}) = 0$. In fact, under the uniform probability, events containing countably many points have probability zero.\footnote{Even more surprising, there are set containing uncountably many points that still have probability zero. Read about the Cantor set if you are curious.} 

For this reason, it is often possible to relax a statement like ``$|X_i| \le Y$'' in the previous statement to a statement like ``$|X_i| \le Y$ except for a set of probability zero.'' This second statement is formalized as $\P(|X_i| \le Y \text{ for all }i) = 1$, and denoted ``$|X_i| \le Y$ a.s.''


\subsection{Convexity and integration}

\point{Review:} convexity. A function $g : \R \to \R$ is convex if for all $x_1, x_2 \in \R$ and $\lambda \in [0, 1]$, 
\[ \lambda g(x_1) + (1 - \lambda) g(x_2) \ge g(\lambda x_1 + (1 - \lambda) x_2). \]
\begin{center}
	\includegraphics[width=0.6\linewidth]{figures/convex}
\end{center}

\point{Exercise:} convince yourself that $\varphi(\E X) \neq \E[\varphi(X)]$ in general (with some exceptions to this, e.g. when $\varphi$ is linear). This is unfortunate because $\varphi(\E X)$ is often easier to compute than $\E(\varphi(X))$.

\point{However:} if $\varphi$ is convex, we can at least get the following bound.

\point{Jensen's inequality:} if $\varphi$ is convex, then $\varphi(\E X) \le \E[\varphi(X)]$.

\point{Proof:} to prove Jensen's inequality, we will use the following result from convex analysis:

\point{Lemma:} for all convex function $\varphi$, there is a sequence of linear functions $L_n(x) = a_n x + b_n$ such that 
\[ \varphi(x) = \sup_n L_n(x). \]
\begin{center}
	\includegraphics[width=0.3\linewidth]{figures/convex-support}
\end{center}

\point{Exercise:} provide an example showing that the above lemma does not hold in the case for non-convex functions.

\point{Back to Jensen's:} using our lemma, we have $\varphi(X) \ge L_n(X)$ by construction. ``Taking expectations on both sides'' (i.e. using monotonicity of expectations):
\begin{eqnarray}
\E\varphi(X) &\ge& \E[L_n(X)] \\
&=& L_n(\E X) \;\text{ since }L_n\text{ is linear.}
\end{eqnarray}
Finally, taking sup over $n$ on both sides:
\begin{eqnarray}
\E[\varphi(X)] &\ge& \sup L_n(\E X) \\ 
&=& \varphi(\E X).
\end{eqnarray}

\point{Example:} since $\varphi(x) = x^2$ is convex, Jensen's gives us the following inequality: $(\E X)^2 \le \E[X^2]$.

\point{Application:} the variance is defined as $\var[X] \defeq \E[(X - \mu)^2]$, where $\mu = \E X$. By computing the square and linearity, this last equation is equal to $\E[X^2] - (\E X)^2$.  Therefore, Jensen's inequality gives us another proof that the variance is non-negative.


\subsection{Markov's inequality and its friends}

\point{Motivation:} let $X$ be the water level near a dam of height of 7.5m. What is the probability of a flood? All you know is that the mean water level is 5m.

\point{Proposition (Markov's inequality):} if $X \ge 0$, then for all $\alpha \ge 0$, 
\[ \P( X \ge \alpha) \le \frac{\E X}{\alpha}. \]

\point{Exercise:} solve the dam problem using Markov's inequality.

\point{Proof:} Convince yourself by looking at the graph of $X$ that the following identities hold: 
\begin{eqnarray*} 
X &\ge& \1[X \ge \alpha] X\;\text{ (follows from }X \ge 0\text{)} \\
&\ge& \alpha \1[X \ge \alpha].
\end{eqnarray*}
\begin{center}
	\includegraphics[width=0.5\linewidth]{figures/markov}
\end{center}
Taking expectations on both sides:
\begin{eqnarray*}
\E X &\ge& \alpha \E[\1[X \ge \alpha]] \\
&=& \alpha \P(X \ge \alpha).
\end{eqnarray*}

\point{Note:} non-negativity is necessary. Consider for example a random variable with the discrete uniform distribution on $\{-1, +1\}$. 

\point{Note:} the bound will sometimes be greater than one. On the other hand, there are random variables $X$ and $\alpha$ such that the bound it tight, i.e. where $\P( X \ge \alpha) = \frac{\E X}{\alpha}$. For example $X$ such that $\P(X = 2) = 1/2$, $\P(X = 0) = 1/2$, $\alpha = 2$.

\point{Corollary 1:} for all random variable $X$, $p \ge 0$,
\[ \P(|X| \ge \alpha) \le \frac{\E |X|^p}{\alpha^p}. \]

\point{Proof:} since the power function is strictly increasing, $(|X| \ge \alpha) = (|X|^p \ge \alpha^p)$. Why? Manipulations of this kind are explained in gory details in the next few section, see specifically \ref{sec:rewriting-equalities} for this step. Therefore $\P(|X| \ge \alpha) = \P(|X|^p \ge \alpha^p)$. Since $|X|^p \ge 0$, we can apply Markov's inequality on this last probability.

% note: does not work well with the dam example because of the left tail

\point{Corollary 2 (Chebyshev's inequality):} for any random variable $Y$ with $\mu \defeq \E Y$, $|\mu| < \infty$ (non-negativity not needed anymore!),
\[ \P(|Y - \mu| \ge \alpha) \le \frac{\var Y}{\alpha^2}. \]

\point{Proof:} define $X \defeq |Y - \mu|$, and apply the preceding lemma with $p = 2$.

\point{Corollary 3 (Chernoff bound):} if $X$ is any random variable, then
\[ \P(X \ge \alpha) \le \inf_t \frac{\E[e^{tX}]}{e^{t\alpha}}. \]

\point{Proof:} For any fixed $t$, the function $f(x) = e^{tx}$ is strictly increasing, so we can use the same reasoning as Chebyshev's. Since the inequality if true for all $t$, the left-hand side is bounded by the infimum over $t$.

\point{Note:} the quantity $\E[e^{tX}]$, viewed as a function of $t$, is called the \emph{moment generating function}. We will meet this object again. 

We will also revisit Chernoff's bound soon once we have introduced independence of random variables. 


\subsection{Rewriting events involving equalities}\label{sec:rewriting-equalities}

This section and the next two provide additional details as well as generalize one of the key step used in the proof of Markov's, Chebyshev's, and Chernoff's inequalities.

Some background: when dealing with equations involving non-random variables, a common heuristic is to ``add on both sides,'' e.g. $x + 5 = 2$ becoming $x = -3$. Let us call these two logical statements $s_1$ and $s_2$. Since $s_1(X(\omega))$ is true if and only if  $s_2(X(\omega))$ is true, it follows that the two events, $(X + 5 = 2)$ and $(X = -3)$, are equal. In particular, $\P(X + 5 = 2) = \P(X = -3)$.

More generally, if $f$ is one-to-one, then $(X = Y) = (f(X) = f(Y))$. For example $(-X = -5) = (X = 5)$. 

In fact, we only need $f$ to be one-to-one on the set of possible values attained by $X$ and $Y$ (i.e. the union of their ranges). For example, if $X$ is non-negative (denoted $X \ge 0$), then $(X = 2) = (X^2 = 4)$. Indeed, even though $f(x) = x^2$ is not one-to-one, it is when restricted to the positive real line.

Many steps in probability arguments follow that kind of logic-based reasoning.


\subsection{Rewriting events involving inequalities}\label{sec:rewriting-inequalities}

Be a bit more careful when dealing with inequalities, we need a more specialized form a one-to-one mapping: if $f$ is \emph{strictly increasing}, then $(X \le Y) = (f(X) \le f(Y))$. For example $(X \le 0) = (\exp(X) \le 1)$.

This is indeed necessary as $(-X \le -5) = (X \ge 5) \neq (X \le 5)$.


\subsection{Bounding events}

When we cannot rewrite the event using the methods described in Sections~\ref{sec:rewriting-equalities} and \ref{sec:rewriting-inequalities}, we can settle on bounding the event instead. Suppose event $A_1$ is defined using statement $s_1(Y)$ (e.g. $A_1 = (Y = 2)$) and similarly, event $A_2$, using $s_2(Y)$ (e.g. $A_2 = (Y^2 = 4)$. Suppose $s_1 \Longrightarrow s_2$. Then we have $A_1 \subseteq A_2$. In our example, $y = 2 \Longrightarrow y^2 = 4$, but the converse is not true if the random variable $Y$ can take negative values, as $y = -2$ also yields $y^2 = 4$. So all we get for general random variables is that $(Y = 2) \subset (Y^2 = 4)$.

We will soon see in Section~\ref{sec:limit-pr-measures} that $A_1 \subseteq A_2$ implies $\P(A_1) \le \P(A_2)$.


\subsection{Exercises}

\begin{enumerate}
  \item Solve the first exercise (linearity in the case of simple functions) of Section~\ref{sec:area-simple-function}.
\end{enumerate}


%\subsection{Solutions}
%
%\begin{enumerate}
%  \item First, we show that $Y + Y'$ is a simple function (this will allow us to use the easy definition of expectation given in Section~\ref{sec:area-simple-function}). Write $Y = \sum_{i=1}^N a_i \1_{A_i}$, and $Y' = \sum_{j=1}^M b_j \1{B_j}$ and assume without loss of generality that $A_i$ forms a partition, and that $B_j$ forms another partition.\footnote{Otherwise, add one more block $A_{N+1} = \cup_{i=1}^N A_i$ with $a_{N+1} = 0$. Same for $B_j$.} Consider the sets $\{C_{i,j} = A_i \cap B_j : 1\le i \le N, 1\le j \le M\}$. Note that the $C_{i,j}$ are clearly disjoint, and that on $C_{i,j}$, $(Y + Y')$ is equal to $a_i + b_j$. Therefore:
%\begin{eqnarray*}
%\E[Y + Y'] &=& \sum_{i,j} (a_i + b_j) \P(C_{i,j}) \\
%&=& \sum_{i,j} a_i \P(C_{i,j}) + \sum_{i,j} b_j \P(C_{i,j}) \\
%&=& \sum_{i} a_i \sum_j \P(C_{i,j}) + \sum_{j} b_j \sum_i \P(C_{i,j}) \\
%&=& \sum_{i} a_i  \P(\cup_j C_{i,j}) + \sum_{j} b_j  \P(\cup_i C_{i,j}) \;\text{(since the }C_{i,j}\text{ are disjoint)} \\
%&=& \sum_{i} a_i  \P(A_i) + \sum_{j} b_j  \P(B_j) \;\text{(since }\{A_{i}\}\text{ is a partitions, similarly for }\{B_{j}\} \text{ )} \\
%&=& \E[Y] + \E[Y'].
%\end{eqnarray*}
%\end{enumerate}


\section{Independence}

\subsection{More than one random variables (random vectors)}\label{sec:random-vectors}

\point{Motivation:} A man and a woman try to meet at a certain place between 1:00pm and 2:00pm.  Suppose each person pick an arrival time between 1:00pm and 2:00pm uniformly at random (denote $X$ and $Y$ respectively), and waits for the other at most 10 minutes. What is the probability that they meet?

\point{Practical question:} How to compute a probability of the form $\P((X, Y) \in S)$?

\point{Theoretical question:} Given two random variables on the same space, $X: \Omega \to \R$ and $Y : \Omega \to \R$, what is $(X, Y)$ exactly?

\point{Definition:} A random vector is a random variable that takes values in $\states = \R^2$: \[ (X, Y) : \Omega \to \R^2. \]

\point{Recall:} The definition of random variable requires a \sigmaalg\ on $\states$. What should we pick? We know that we will at the very least need to compute the probability of $X$ falling in a \emph{rectangle}:

\point{Definition:} a rectangle is a set of the form:
\begin{eqnarray} 
R &=& A \times B \;\text{ for some }A \in \borels, B \in \borels \\
&=& \{(x, y) : x \in A, y \in B\}.
\end{eqnarray}
\begin{center}
	\includegraphics[width=0.4\linewidth]{figures/product}
\end{center}

\point{Unfortunately:} $\generators = \{R : R\text{ is a rectangle}\}$ is not a \sigmaalg\ (why?). Also, it does not contain the set that we would need to answer the ``practical question'' that kicked off this section.
\begin{center}
	\includegraphics[width=0.4\linewidth]{figures/strip}
\end{center}

\point{Solution:} generate a \sigmaalg\ from $\generators$. The result of this is called the product \sigmaalg:
\[ \events^{\otimes 2} \defeq \events \otimes \events \defeq \sigma(\generators). \]

\point{Exercise:} show that the set $S$ in the ``practical question'' is in $\events \otimes \events$.


\subsection{Distribution, CDF and density of a random vector}

These definitions follow directly from the univariate case:

\point{Definition:} the joint distribution of a random vector $(X, Y)$, denoted $\P_{X,Y} : \events \otimes \events \to [0, 1]$, is defined by:
\[ \P_{X,Y}(S) \defeq \P((X, Y) \in S). \]

\point{Definition:} the joint CDF of a random vector $(X, Y)$, denoted $F_{X,Y} : \R^2 \to [0, 1]$, is defined by:
\[ F_{X, Y}(x, y) \defeq \P_{X,Y}((-\infty, x] \times (-\infty, y]). \]

\point{Note:} This last definition seems slightly arbitrary. Why pick this particular class of sets $S$ (``quarter-planes'')? As we will see in the next section, this is because this class \emph{characterizes} the joint distribution. In other words, given a joint CDF, you can in principle obtain the probability of $X$ falling in any set $S$. 

\point{Definition:} a function $f:\R^2 \to [0, \infty)$ is called a joint density of the random vector $(X,Y)$ if:
\[ F_{(X,Y)}(x,y) = \int_{-\infty}^x \int_{-\infty}^y f(x,y) \ud x \ud y. \]

\point{Note:} these definitions can be generalized to more than two dimensions.


\subsection{Determining classes}

In this section, we explain the tool used to prove the characterization statement of the previous section.

\point{Tool:} $\pi$-$\lambda$ theorem. 
\begin{enumerate}
\item Let $\pi$ denote a collection of sets satisfying the following condition (called a $\pi$-system condition):
\begin{enumerate}
  \item $A, B \in \pi \Longrightarrow A\cap B\in \pi$.
\end{enumerate}
\item Let $\lambda$ denote a collection of sets satisfying the following conditions (called a $\lambda$-system condition):
\begin{enumerate}
  \item $\Omega \in \lambda$,
  \item $A, B \in \lambda, A \subseteq B \Longrightarrow B \backslash A \in \lambda$,
  \item $A_i \uparrow A, A_i \in \lambda \Longrightarrow A \in \lambda.$
\end{enumerate}
\item Then, the following holds: $\pi \subset \lambda \Longrightarrow \sigma(\pi) \subset \lambda$.
\end{enumerate}

\point{Proposition:} Supposet $\P_1$ and $\P_2$ are probability measures on $\events = \sigma(\generators)$, where $\generators$ is closed under finite intersection. Then the following are equivalent:
\begin{enumerate}
  \item $\P_1(A) = \P_2(A)$ for all $A \in \events$,
  \item $\P_1(A) = \P_2(A)$ for all $A \in \generators$.
\end{enumerate}

\point{Exercise:} prove this using the $\pi$-$\lambda$ theorem. Hint: let $\lambda = \{A \in \events : \P_1(A) = \P_2(A)\}$.

\point{Corollary:} $F_{X,Y}$ determines $\P_{X,Y}$.

\point{Proof:} $(-\infty, x] \cap (-\infty, y] = (-\infty, \min\{x, y\}]$.
\begin{center}
	\includegraphics[width=0.5\linewidth]{figures/intersection}
\end{center}

\point{Corollary:} if $(X,Y)$ has joint density $f$, then
\[ \P((X, Y) \in S) = \int_S f(x,y) \ud x \ud y. \]


\subsection{Independence of random variables}

% TODO: come back to Pollards' example of independence to motivate why learning measure theory

\point{Definition:} the random variables $X_1, X_2, \dots, X_n : \Omega \to \R$ are independent if, for all $A_i \in \borels$, \[ \P(X_1 \in A_1, X_2 \in A_2, \dots, X_n \in A_n) = \prod_{i=1}^n \P(X_i \in A_i). \]

\point{Equivalent notation:} \[ \P_{X_1, X_2, \dots, X_n}(A_1 \times A_2 \times \dots \times A_n) = \prod_{i=1}^n \P_{X_1}(A_i). \]

\point{Exercise:} using $\pi-\lambda$ show that the above statement can be checked by showing that the CDF factorizes as 
\[ F_{X_1, X_2, \dots, X_n} = \prod_{i=1}^n F_{X_i}. \]

\point{Note:} similarly to the above exercise, if the random variables have a joint density $f_{X_1, X_2, \dots, X_n}$, independence can be checked by factorizing the density into \emph{marginal densities} $f_{X_i}$ of each individual random variable:
\[ f_{X_1, X_2, \dots, X_n} = \prod_{i=1}^n f_{X_i}. \]

\point{Definition:} we say the random variables $X_1, X_2, \dots, X_n$ are pairwise independent if each pair is independent.

\point{Exercise:} find 3 random variables such that these random variables are pairwise independent but not independent. Hint: this can be done using indicator random variables.


\subsection{Chernoff's bound, continued}

We encountered the moment generating function $f(t) = \E[e^{tX}]$ when talking about Chernoff's bound. When $X$ is a sum of independent random variables, $X = X_1 + X_2 + \dots + X_n$, we get this nice formula:
\[ f(t) = \prod_{i=1}^n f_i(t), \]
where $f_i(t) = \E[e^{tX_i}]$. Informally, adding random variables multiplies their moment generating functions.

Since the moment generating function for individual variables are often easy to compute, this gives a powerful way to evaluate the right hand side of Chernoff's bound 

\point{Example: Chernoff-Hoeffding theorem} if the $X_i$ are independent and each have a Bernoulli($p$) distribution (terminology: ``are iid Bernoulli''), and $\epsilon > 0$:

\begin{align*}
	\P \left (\frac{1}{n} \sum X_i \geq p + \varepsilon \right ) &\leq \left (\left (\frac{p}{p + \varepsilon}\right )^{p+\varepsilon} {\left (\frac{1 - p}{1-p- \varepsilon}\right )}^{1 - p- \varepsilon}\right )^n  \\
	\P \left (\frac{1}{n} \sum X_i \leq p - \varepsilon \right ) &\leq \left (\left (\frac{p}{p - \varepsilon}\right )^{p-\varepsilon} {\left (\frac{1 - p}{1-p+ \varepsilon}\right )}^{1 - p+ \varepsilon}\right )^n 
\end{align*}

\point{Proof:} compute the moment generating function of a Bernoulli. Then use calculus to minimize over $t$. Then plug-in the minimum. Use the variable $Y_i = 1 - X_i$ to establish the other side of the bound.

\point{Other versions:} see the wikipedia page for many more bounds derived from this.  


\section{Computing expectations in practice}


% \subsection{Expectations in the discrete case}
% 
% % Discrete expectation can arise in at least two different ways:
% % 
% % \begin{itemize}
% %   \item $X(\Omega)$ is discrete.  
% %   \item $\P$ is discrete, i.e. there are $A = \{\omega_1, \omega_2, \dots, \omega_n\} \subset \Omega$, called atoms, such that $\P(A) = 1$. 
% % \end{itemize}
% 
% Discrete expectations arise when we can find a set of \emph{atoms}, defined as a set 
% 
% \noindent In both cases:
% \[ \E[X] = \sum_{x \in X(\Omega)} x \P(X = x). \]

\subsection{Computing integral using calculus}\label{sec:calculus}

When $\Omega = [a, b] \subset \R$, $\mu$ is uniform, and $f$ is bounded, then the Lebesgues integral $\int f \ud \mu$ behaves in the same way as the standard Riemann integral (as long as the latter exists, which is true iff the set of discontinuities of $f$ have measure zero). Hence you can in principle use the \point{fundamental theorem of calculus} to compute these integrals: if there is a function $F$ such that $F'(x) = f(x)$ for all $x \in \Omega$, then 
\[ \int f \ud \mu = \int f(x) \ud x = F(b) - F(a). \]
It also follows that $F$ is the \CDF\ of $X$.  

The limitation of this approach is that even when $f$ is a simple close-form expression (expressed in terms of $+, -, *, /, \exp, \log$), the CDF $F$ might not have such a close-form expression (example: $f(x) = \exp(-x^2)$).


\subsection{Computing expectations using the distribution of the random variable}\label{sec:change-of-var}

\point{Motivating example:} consider a space $\Omega$ containing the following four objects: a circle, a triangle, a square, and a pentagon, and a probability $\P$ that give then equal probabilities (1/4). Let $X$ denote a random variable that takes a shape as input and output the number of faces. Suppose we want to compute $\E[g(X)]$, where $g(x) = \1[x \text{ is an even integer}]$. We will see two methods for doing this:

\point{From the definition:} let $Y = g(X)$. We see that $Y$ can take two possible values, zero and one, therefore it is simple. Applying the definition of expectation of simple function:
\begin{eqnarray*} 
\E[Y] &=& \int Y \ud \P \\
&=& 1\times \P(Y = 1) + 0\times \P(Y = 0) \\
&=& \P(Y^{-1}(\{1\}) \\
&=& \P(X^{-1}(g^{-1}(\{1\}))) = 1/2.
\end{eqnarray*}

\point{Another way:} first, derive the distribution of $X$:
\[ \P_X(A) = \frac{1}{4}\left( \1[0 \in A] + \1[3 \in A] + \1[4 \in A] + \1[5 \in A] \right). \]
then, compute an integral of $g$ \emph{with respect to the distribution of $X$}:
\[ \int g \ud \P_X. \]
Here, $g$ is an indicator, so it is a simple function, so we can use our definition of integral of simple functions:
\begin{eqnarray*}
\int g \ud \P_X &=& 1 \times \P_X(\{x : g(x) = 1\}) + 0 \times \P_X(\{x : g(x) = 0\}) \\
&=& \P_X(\{\dots, -6, -4, -2, 0, 2, 4, 6, \dots\}) \\
&=& 1/2.
\end{eqnarray*}

\point{Proposition (``change of variable''):} these two methods are equivalent. More precisely, if $Y = g(X)$ is a random variable and either $g \ge 0$ or $Y \in \L_1$, then:
\[ \int Y \ud \P = \int g \ud \P_X. \]

\point{Note:} by ``either $g \ge 0$ or $Y \in \L_1$'' I mean that the above can actually be slit into two propositions, one assuming the first condition, and a second proposition assuming the second condition.

\point{Note:} an important special case is $g(x) = x$.

\point{Note:} the nice thing with the second way is that you do not have to know $\Omega$ and $\P$, which are often not explicitly given to you. Often all I tell you is $X \sim F$, which characterizes $\P_X$, and I ask $\E[g(X)]$. Using our proposition we can solve this using an integral over the real line with a measure on the reals provided by $\P_X$.

\point{Proof:} assume first that $g = \1_A$. From the same computation as in the example above, we have on one hand:
\[ \int Y \ud \P = \P(Y = 1), \]
and on the other hand:
\begin{eqnarray*} 
\int g \ud \P_X &=&  \P_X(\{x : g(x) = 1\}) \\
&=& \P(X \in \{x : g(x) = 1\}) \\
&=& \P(g(X) = 1) \\
&=& \P(Y = 1).
\end{eqnarray*}

\noindent Next, assume $g$ is simple, i.e. $g = a_1 \1_{A_1} + \dots + a_n \1_{A_n}$. We have:
\begin{eqnarray*}
\int g(X) \ud \P &=& \int \sum_i a_i \1_{A_i}(X) \ud \P \\
&=& \sum_i a_i \int \1_{A_i}(X) \ud \P \;\text{ (using linearity)} \\
&=& \sum_i a_i \int \1_{A_i} \ud \P_X \;\text{ (using first part of proof)}\\
&=& \int \left(\sum_i a_i \1_{A_i}\right) \ud \P_X\;\text{ (using linearity)} \\
&=& \int g \ud \P_X.
\end{eqnarray*}

\noindent Third, assume $g \ge 0$ (or $g\in \L_1$). Use the approximation theorem to get $g_i \uparrow g$, so that:
\begin{eqnarray*}
\int g(X) \ud \P &=& \int \lim g_i(X) \ud \P \\
&=& \lim \int g_i(X) \ud \P \;\text{(using MCT (or DCT))} \\
&=& \lim \int g_i \ud \P_X \;\text{(using second part of proof)} \\
&=& \int \lim g_i \ud \P_X \;\text{(using MCT)} \\
&=& \int g \ud \P_X.
\end{eqnarray*}.


\subsection{How probability space and random variables are constructed in practice}\label{sec:using-rv-to-construct-prs}

So far we have often insisted in explicitly constructed $\Omega$, $\events$, $\P$, and designed random variables $X$ by providing, for each $\omega$, the value $X(\omega)$. This level of detail is useful for certain proofs (e.g. Markov's inequality), but for many day-to-day calculations, it is not necessary to go into that much detail. 

\point{Idea:} instead of defining $\Omega$, $\events$, $\P$ and $X$, just specify the distribution of $X$. I.e. say something like ``let $\Omega$, $\events$, $\P$ and $X$ be such that $X \sim \Bern(p)$, i.e. such that $X$ has a Bernoulli $p$ distribution.'' 

\point{How do we know that there are such $\Omega$, $\events$, $\P$ and $X$?} Thanks to the inverse CDF construction, introduced in Section~\ref{sec:inverse-cdf}. 

\point{This does not uniquely define $\Omega$, $\events$, $\P$ and $X$:} recall from Section~\ref{sec:equal-in-distribution} that there are several ways to build $\Omega$, $\events$, $\P$ and $X$ that yield the same distribution on $X$.

\point{However:} in light of the preceding Section (\ref{sec:change-of-var}), we only need the distribution in order to compute arbitrary expectations. So even though $\Omega$, $\events$, $\P$ and $X$ are not technically fully specified, they are constrained enough for our purpose.

For this reason, often we do not even mention $\Omega$, $\events$, $\P$, and just assume there is a ``global'' probability space on which the random variables are defined based on their distributions. 


\subsection{Computing expectations using densities}

\point{Generalization of density:} we say $X$ has a density $f$ with respect to $\mu$ (typically, $\mu$ is the uniform measure on $\R$, but other choices are possible), if:
\[ \P_X(A) = \int \1_A f \ud \mu \;\;\text{ for all }A\in\borels. \]

\point{Note:} this generalizes our previous definition of density given in Section~\ref{sec:density-first-def}:
\begin{eqnarray*} 
F_X(b) - F_X(a) &=& \P_X([a, b]) \\
&=& \int_a^b f(x) \ud x \;\text{ (by Section~\ref{sec:calculus})}
\end{eqnarray*}
Therefore by letting $a \to -\infty$ we recover the formula from Section~\ref{sec:density-first-def}.

\point{Critical question in first assignment:} show that if $g \ge 0$ or $g \in \L_1$, 
\[ \E[g(X)] = \int f(x) g(x) \mu(\ud x). \]

\point{Why this is useful:} because we can now compute expectations using calculus, even when $\Omega \neq \R$, and by knowing only $\P_X$.


\subsection{Computing the expectation of a function of independent random variables}\label{sec:expectation-indep}

\point{Motivation:} back to the motivating problem of Section~\ref{sec:random-vectors}. How to formalize the compution of the expectation, $\E[\1[|X-Y| \le 1/6]]$. Let us define $g(x,y) \defeq \1[|x-y| \le 1/6]$. By our change of variable formula (Section~\ref{sec:change-of-var}), \[ \E[ g(X,Y) ] = \int g \P_{X,Y}. \]

Now our independence assumption on the arrival times of the woman and man means that $\P_{X_1,X_2}(A_1 \times A_2) = \P_{X_1}(A_1) \P_{X_2}(A_2)$. To solve this last integral, we use this independence statement and the following theorem:

\point{Fubini:} if $g \ge 0$, and $\mu \times \mu$ is such that $\mu \times \mu(A_1 \times A_2) = \mu_1(A_1) \mu_2(A_2)$ (think of $\mu \times \mu$ as $\P_{X,Y}$, $\mu_1$ as $\P_X$ and $\mu_2$ as $\P_Y$), then:
\begin{eqnarray*}
\int g \ud(\mu_1 \times \mu_2) &=& \int \left( \int g \ud \mu_1 \right) \mu_2 \\
&=& \int \left( \int g \ud \mu_2 \right) \mu_1,
\end{eqnarray*}
in other words, we can approach the problem as iterated univariate integrals, and do so in any order we wish.

\noindent This is nice because the inner integral is just over the real line, so we can use our density $f$ of $X_1$ and calculus at this point.

\point{Corollary:} $X$ and $Y$ are independent if and only if for all measurable $g_i \ge 0$, \[ \E[g_1(X) g_2(Y) ] = \E[g_1(X)] \E[g_2(Y)]. \]

\point{Important:} the above should not be confused with \emph{uncorrelated random variables}: $X$ and $Y$ are uncorrelated if and only if \[ \E[XY] = \E[X] \E[Y]. \]

\point{Note:} $X$ and $Y$ uncorrelated implies independence\footnote{Assuming say $XY \in \L_2$.} but the converse is not true!

\point{Exercise:} consider $(X,Y)$ with a uniform density on the unit circle. 
\begin{center}
\includegraphics[width=0.7\linewidth]{figures/uniform-on-circle}
\end{center}

\begin{enumerate}
  \item Find $\E[XY], \E[X], \E[Y]$. Hint: use symmetry.
  \item Find $g_i$'s such that $\E[g_1(X) g_2(Y)] = 0$ but $\E[g_1(X)] \E[g_2(Y)] > 0$. Hint: use the indicators shown in the figure below.
\end{enumerate}
\begin{center}
	\includegraphics[width=0.2\linewidth]{figures/circle-indicators}
\end{center}


\subsection{Declaring independent random variables}

Let us continue the discussion of Section~\ref{sec:using-rv-to-construct-prs}, on implicit specification of probability spaces. Suppose now we want to declare more than one random variables. Often we do so by declaring that (1) they are independent, and (2) the distribution of each random variable. For example: ``let $X_1, X_2, \dots$ be independent $\Bern(0.5)$ random variables.'' A shorthand: ``$X_1, X_2, \dots$ are i.i.d. $\Bern(0.5)$'' (identically and independently distributed). Or just:
\[ X_i \iidsim \Bern(p). \]

\point{There is always some $\Omega$, $\P$, $\events$, $X_1$, $X_2$, \dots satisfying the constraints (1) and (2):} you can take this as a fact (search Kolmogorov's Extension Theorem if you are curious). 

\point{This does not uniquely specify $\Omega$, $\P$, $\events$, $X_1$, $X_2$, \dots:} for the same reason as in Section~\ref{sec:using-rv-to-construct-prs}.

\point{But:} in the light of Section~\ref{sec:expectation-indep}, all we need to know to compute expectation is (1) to know that the random variables are independent and (2) the distribution of each random variable $X_i$ (called the ``marginal distributions'').


\section{Asymptotics}

\subsection{Infinitely often and eventually}\label{sec:io-ev}

\point{Definition:} Let $A_n$ denote an infinite collection of events, $n\in\{1,2,3,\dots\}$ (not necessarily nested). We create two new events from these:
\begin{eqnarray*}
(A_n\ev) &\defeq& \{\omega \in \Omega : \exists N \in \{1, 2, \dots\}, \forall n\ge N, \omega \in A_n \} \\
(A_n\io) &\defeq& \{\omega \in \Omega : \forall N \in \{1, 2, \dots\}, \exists n\ge N, \omega \in A_n \}.
\end{eqnarray*}

\point{Examples:} consider the ``drunk train'' example from the first lecture.  
\begin{itemize}
  \item Let $X_i = 2Y_i - 1$ denote the direction the train takes at each step where $Y_i \sim \Bern(1/2)$ are iid (independent and identically distributed)
  \item Let $S_n = X_1 + X_2 + \dots + X_n$ denotes the current position of the train. 
  \item Let $A_n$ denote the event that the train is at ``home'' at step $n$, i.e. $A_n = (S_n = 0)$.
  \item The event that the train never gets lost: call this the event $A$, i.e. that the train returns to zero infinitely often. $A \defeq (A_n\io)$.
  \item The event that train eventually gets lost: $B \defeq (S_n \neq 0\ev)$.
\end{itemize}

\point{Proposition:} $(A_n\io) = (A_n^\complement\ev)^\complement$.

\point{Proof:} by definition:
\begin{eqnarray*}
(A_n\io) &=& \bigcap_{N=1}^\infty \bigcup_{n\ge N} A_n \\
(A_n\ev) &=& \bigcup_{N=1}^\infty \bigcap_{n\ge N} A_n.
\end{eqnarray*}
The proposition follows from De Morgan's law.


\subsection{Borel-Cantelli (BC) lemma 1}

\point{Proposition:} If \[\sum_{n=1}^\infty \P(A_n) < \infty,\] then $\P(A_n\io) = 0$.

\point{Notes:}
\begin{itemize}
  \item $A_n$ do not need to be independent/disjoint/nested!
  \item But independence will be needed for a partial converse (BC 2)
\end{itemize}

\point{Example:} A drunk bird eventually gets lost (with probability one).
\begin{itemize}
  \item Let $X_i^{(3)} \defeq (X^{(1)}_i, X'^{(1)}_i, X''^{(1)}_i)$, where the three coordiates are indendent copies of the random walk of Section~\ref{sec:io-ev}. I.e. diagonal moves are permitted.
  \item Similarly, $S_n^{(3)} = X_1^{(3)} + X_2^{(3)} + \dots + X_n^{(3)}$.
  \item The claim can be rewritten as $\P(S^{(3)}_n = (0,0,0)\io) = 0$.
  \begin{enumerate}
    \item First, by the binomial formula, \[\P(S^{(1)}_{2n} = 0) = \binom{2n}{n} 2^{-2n}.\]
    \item Next, apply Stirling's formula to get \[\P(S^{(1)}_{2n} = 0)  \sim \frac{c}{\sqrt{n}},\] where $c$ is a constant, and $a_n \sim b_n$ means that $a_n/b_n \to 1$.
    \item This means that for the whole process,  \[\P(S^{(3)}_{2n} = 0)  \sim \frac{c}{n^{3/2}},\] which is summable, i.e. \[ \sum_{n=1}^\infty \frac{c}{n^{3/2}} < \infty,\] because $3/2 > 1$.
    \item Hence, by BC 1, $\P(S^{(3)}_n = (0,0,0)\io) = 0$.
  \end{enumerate}
\end{itemize}

\point{Proof of BC:} let $N$ denote the number of $A_n$'s that occur: \[ N = \sum_{n=1}^\infty \1_{A_n}. \]We have:
\begin{enumerate}
  \item $(N = \infty) = (A_n\io)$,
  \item By MCT: 
\begin{eqnarray*}
\E[N] &=& \E\left[\sum_{n=1}^\infty \1_{A_n}\right] \\
&=& \sum_{n=1}^\infty \P(A_n).
\end{eqnarray*} 
  \item Hence by the assumption, $\E[N] < \infty$.
  \item It follows that $\P(N = \infty) = 0$.
\end{enumerate}

\point{Converse?} It is NOT true that \[ \P(A_n\io) = 0 \Longrightarrow \sum \P(A_n) < \infty. \] Counter-example:
\begin{itemize}
  \item Take $A_n = [0, 1/n]$.
  \item $\P(A_n \io) = 0$.
  \item But $\sum \P(A_n) = \infty$.
\end{itemize}

\point{But if we add independence, it is true:} (BC 2) If \[ \sum_{n=1}^\infty \P(A_n) = \infty,\]and the events $A_n$ are independent, then $\P(A_n\io) = 1$.

\point{Example:} a monkey on a typewriter will write the complete work of Shakespeare infinitely often. Some notation first:
\begin{itemize}
  \item Denote the work of Shakespeare by $x_0, x_1, x_2, \dots, x_{k-1}$ where each $x_i$ is a letter.
  \item Let $M_i$ denote the letter pressed by the monkey at step $i$. Assume the $\{M_i\}$ are independent.
  \item Define $A_n \defeq (M_n = x_0, M_{n+1} = x_1, \dots, M_{n+k-1} = x_{k-1})$.
\end{itemize}
Problem? $A_n$ are not independent! We will actually prove something stronger. Informally, the idea is that we are going to show the monkey write the work of Shakespeare \emph{and with the first letter written say Jan 1st} infinitely often. Formally:
\begin{itemize}
  \item Define $A'_n \defeq (M_{kn} = x_0, M_{kn+1} = x_1, \dots, M_{kn+k-1} = x_{k-1}).$
  \item The $A'_n$ are independent by construction.
  \item Hence, by BC 2, $\P(A'_n \io) = 1$.
  \item Now, $(A'_n\io) \subset (A_n \io)$, so $\P(A_n \io) \ge \P(A'_n\io) = 1$.
\end{itemize} 


\subsection{Weak law of large number (WLLN)}

Let $X_i : \Omega \to \R$ be iid, with $\E[X_i] = \mu, |\mu| < \infty$, and defined $S_n = X_1 + \dots + X_n$. To capture our intuition of how repeated random processes behave (``frequencies approach probabilities''), we would like to be able to write something like: \[ \frac{1}{n} S_n \to \mu. \]That raises the question: what do we mean by ``$\to$''? So far, the definition we used was \emph{pointwise limits}, meaning that for all $\omega\in\Omega$, \[ \lim_{n\to\infty} \frac{1}{n} S_n(\omega) = \mu.\]Note here that the LHS is a random variable, so the right hand side should be interpreted as a constant random variable (i.e. such that$\mu(\omega) = \mu$ for all $\omega)$. 

\point{It is too much to ask} for such convergence to hold in general. Consider for instance the train example, where $\omega$ corresponds to an infinite trajectory. Then for the trajectory $\omega_0$ where the train always goes right (s.t. $X_i(\omega_0) = +1$ for all $i$). Then \[ \lim \frac{1}{n} S_n(\omega_0) = 1 \neq \mu = 0.\]

\point{Instead:} we will relax the notion of convergence. There are several ways to do this, which are useful in different contexts. Let us start with one that gives us a simple proof of the LLN.

\point{Proposition:} let $X_i : \Omega \to \R$ be iid, with $\E[X_i] = \mu, |\mu| < \infty$, and defined $S_n = X_1 + \dots + X_n$. Then for all $\epsilon > 0$, \[ \lim_{n\to\infty} \P\left( \left| \frac{1}{n} S_n - \mu\right| \ge \epsilon \right) = 0. \]

\point{Visualization:} suppose $\mu = 0$, and note that $\left( \left| \frac{1}{n} S_n -0\right| \ge \epsilon \right) = ( |S_n| > \epsilon n)$. On a graph where the x-axis is $n$ and the y-axis is $S_n$ (the same picture we used for train trajectories), consider the cone specified by $\epsilon n$ and $-\epsilon n$. The event is then that the trajectory is inside the cone at step $n$.
\begin{center}
\includegraphics[width=0.7\linewidth]{figures/LLN}
\end{center}


\point{Proof:} we will start with a proof that uses an extra assumption, $\var X < \infty$. I will then give a sketch of how to get rid of this condition.
\begin{enumerate}
  \item First, assume without loss of generality that $\mu = 0$ (otherwise, set $X'_i = X_i - \mu$). 
  \item Use Chebyshev:
  \begin{eqnarray*}
    \P\left(\left|\frac{1}{n}S_n\right| \ge \epsilon \right) &\le& \frac{\E\left| \frac{1}{n} S_n \right|^2}{\epsilon^2} \\
    &=& \frac{\E[X_1 + \dots + X_n]^2}{n^2 \epsilon^2} \\
    &=& \frac{\sum_{i=1}^n \E[X_i^2] + \sum_{(i,j):i\neq j} \E[X_i X_j]}{n^2 \epsilon^2} \\
    &=& \frac{n \E X_i^2}{n^2 \epsilon^2} \\
    &=& \frac{\text{constant}}{n}.
  \end{eqnarray*}
  \item Taking limits on both sides: \[ \lim_{n\to\infty} \P\left(\left|\frac{1}{n}S_n\right| \ge \epsilon \right) \le \lim_{n\to\infty} \frac{\text{constant}}{n} = 0. \]
\end{enumerate}

\point{Note:} we actually only used pairwise uncorrelation in this proof.

\point{Sketch:} for how to lift the finite variance condition. The idea is to first fix $x > 0$, and to write the following decomposition:
\[ X_i = \underbrace{X_i \1[|X_i| \le x]}_{Y_i} + \underbrace{X_i \1[|X_i| > x]}_{Z_i}. \]
Now we can use our previous result on $Y_i$ since a bounded random variable will necessarily have finite variance. As for $Z_i$, we can get rid of it by letting $x \to \infty$ and using DCT (where we use $|Z_i| \le |X_i|$ and our assumption that $|\mu| < \infty$ and hence $\E|X_i| < \infty$).


\subsection{Convergence in probability}

The definition of ``$\to$'' used in the previous section is used in other contexts, so let us give a name to it:

\point{Definition:} a sequence of random variables $Y_i : \Omega \to \R$ converges in probability to a random variable $Y: \Omega \to \R$ if, for all $\epsilon > 0$,
\[ \lim_{n\to\infty} \P(|Y_n - Y| > \epsilon) = 0. \]

\point{Notation:} $Y_n \pcv Y$.


\subsection{Convergence almost surely}

\point{Question:} can we come up with other notions of convergence? Yes, we will see many alternative, starting with ``almost sure'' convergence (``$Y_n \ascv Y$''). Why is this useful? With almost sure convergence, it is harder to prove the LLN, but once this is done, it is easier to establish corrolaries, e.g. that $Y_n \ascv Y$ and $Y'_n \ascv Y'$ implies $Y_n + Y'_n \ascv Y + Y'$.

\point{Definition:} a sequence of random variables $Y_i : \Omega \to \R$ converges almost surely (a.s.) to a random variable $Y: \Omega \to \R$ if \[ \P(Y_n \to Y) = 1,\] where \[ (Y_n \to Y) \defeq \{ \omega \in\Omega : \lim_{n\to\infty} |Y_n(\omega) - Y(\omega)|\text{ exists and is }=0\}. \]

\point{Note:} with this last notation, another way to write convergence pointwise is $(Y_n \to Y) = \Omega$. From this, it is clear that convergence pointwise implies convergence a.s. but not vice-versa.

\point{Notation:} $Y_n \ascv Y$.

\point{Lemma} connecting this notion of convergence back to convergence in probability as well as the notions of i.o. and ev.: $X_n \ascv X$ if and only if for all $\epsilon > 0$, $\P(|X_n - X| > \epsilon\ev) = 0$.

\point{Proof:} we have: \[\omega \in (X_n \to X) \Longleftrightarrow \forall \epsilon>0, \omega\in (|X_n-X| < \epsilon \ev),\] and hence:
\begin{eqnarray*}
X_n \ascv X &\Longleftrightarrow& \forall \epsilon >0, \P(|X_n - X| \le \epsilon \ev) = 1 \\
&\Longleftrightarrow& \forall \epsilon > 0, \P(|X_n - X| > \epsilon \io) = 0.
\end{eqnarray*}

\point{Strong law of large numbers:} under the same conditions as the WLLN, this says \[ \frac{1}{n} S_n \ascv \mu. \]

\point{Proposition:} convergence in probability does not imply convergence a.s. in general.

\point{Counter-example:} moving blip. Let $\Omega = $ points on a circle, $\P$ be uniform on the circle, and $X_i$ be defined as:
\begin{eqnarray*}
X_1 &=& \1_{[0,1/2]\text{ mod }2\pi}\\
X_2 &=& \1_{[1/2,1/2+1/3]\text{ mod }2\pi}\\
X_3 &=& \1_{[1/2+1/3,1/2+1/3+1/4]\text{ mod }2\pi}\\
&\vdots&
\end{eqnarray*}

\point{Proposition:} convergence a.s. implies convergence in probability.

\point{Proof:} according to our earlier lemma, we have for all $\epsilon > 0$, $\P(|X_n - X| > \epsilon\ev) = 0$. Then, if we let $A_n \defeq (|X_n - X| > \epsilon)$:
\begin{eqnarray*}
0 &=& \P\left( \bigcap_{k=1}^\infty \bigcup_{n \ge k} A_n \right) \\
&=& \lim_{k\to\infty} \P\left( \bigcup_{n\ge k} A_n \right) \text{ (by monotonicity of }\P\text{)} \\
&\ge& \lim_{k\to\infty} \P(A_k) \ge 0.
\end{eqnarray*}
It follows that $\lim_{k\to\infty} \P(A_k) = 0$.


\subsection{Types of convergence: big picture}\label{sec:conv-types-big-picture}

\point{Covered so far:} 
\[ X_n \to X \Longrightarrow X_n \ascv X \Longrightarrow X_n \pcv X. \]

\point{Next:}
\[ X_n \pcv X \Longrightarrow X_n \dcv X. \]


\subsection{Weak convergence}

\point{Motivation:} so far, the two types of convergence we have covered assume that all random variables and the limit live in the same space: $X_i : \Omega \to \R$, $X : \Omega \to \R$. This could create problem when we formalize the central limit theorem. Why?

\point{Solution:} define convergence with respect to objects that get rid of $\Omega$ while characterizing the distribution of the random variables.

\point{Definition 1:} we say $X_n : \Omega_n \to \R$ converges in distribution to $X : \Omega \to \R$, denoted $X_n \dcv X$, if for all bounded continuous function $g : \R \to \R$ (called ``test function''), we have 
\[ \lim_{n\to\infty} \int g \ud \P_{X_n} = \int g \ud \P_X. \]

\point{Note:} more generally, weak convergence is really about convergence of measures. Similarly to the above definition, the notation $\mu_n \dcv \mu$ means that for all bounded continuous function $g$, \[ \lim_{n\to\infty} \int g \ud \mu_n = \int g \mu. \]

\point{Definition 2:} we say $X_n : \Omega_n \to \R$ converges in distribution to $X : \Omega \to \R$, denoted $X_n \dcv X$, if for all $x$ with $\P(X=x) = 0$, $F_{X_n}(x) \to F(x)$. In other words, convergence of the CDFs at points that are not atoms under $X$.

\point{Proposition (Portmanteau):} definition 1 and 2 are in fact equivalent.

\point{Intuition} regarding why we do not require convergence of the CDF at atomic points: otherwise, our nice hierarchy in Section~\ref{sec:conv-types-big-picture} would not hold! Consider: $X_n = 1/n$.

\point{We can now formalize the CLT:} if $X_1, X_2, \dots$ are idd with $E X_1^2 = 1$ and $\E X_1 = 0$, then:
\[ \frac{X_1 + \dots + X_n}{\sqrt{n}} \dcv Y, \]
where $Y$ is a standard normal. 

\point{Note:} this statement is not true in probability or a.s. (this can be proven using `Kolmogorov zero-one law'.

The next questions we will explore are (1) how to prove this CLT result and extend it? (2) How is it used in practice. To answer these questions, we will need to talk about properties of convergence, as well a generating functions.


\subsection{Properties of convergence}

\point{Proposition:} if $g$ is continuous, and $X_n \dcv X$, then $g(X_n) \dcv g(X)$.

\point{Template for many results:} ``$X_n \stackrel{\square}{\longrightarrow} X$ and $Y_n \stackrel{\square}{\longrightarrow} Y$ implies $X_n \circ Y_n \stackrel{\square}{\longrightarrow} X \circ Y$'' where you should replace $\square$ and $\circ$ by any of these combinations (note that some combinations require extra assumptions):
\begin{enumerate}
  \item $\square = $a.s. and $\circ = +,$ or $-, *, /$.
  \item $\square = $p and $\circ = +,$ or $-, *, /$.
  \item $\square = $d and $\circ = +$, AND $X_n, Y_n, X, Y$ independent.
  \item $\square = $d and $\circ = +,$ or $-, *, /$, AND $Y = $ constant (``Slutsky's theorem'').
\end{enumerate}

\point{Example:} of why we have to be careful, especially with convergence in distribution. Let $Y, X, X_n$ all be iid N(0,1), and $Y_n = -X_n$. Then $X_n \dcv X$ and $Y_n \dcv Y$, but $X_n + Y_n \dcv 0 \stackrel{d}{\neq} X+Y$.

\point{Main tools:} used to proved these results.
\begin{itemize}
  \item Subsequence characterization of convergence in probability: $X_n \pcv X$ if and only if for all subsequence $n_k$, there is a further subsequence $n_{k_i}$ such that $X_{n_{k_i}} \ascv X$.
  \item Skorokhod representation: suppose $X_n \dcv X$, then there exists $Y_n$ and $Y$ such that $Y_n, Y: \Omega \to \R$, $Y_n \deq X_n$, $Y \deq X$, and $Y_n \ascv Y$.
\end{itemize}

\subsection{Generating functions and Central Limit Theorem} 

Please let me know if you have missing details in your notes for this section, I will fill this section ASAP.


\section{Poisson theory}

\subsection{Poisson convergence}

\point{Motivation:} consider the following historical dataset, containing the number of accidental horse-kick deaths per year in the Prussian army in the 1881--1896 period: 7 deaths in 1881, 1 in 1882, 3, 2, 7, 6, 1, 3, 2, 2, 6, 4, 4, 1, 6, 2. Question: can we approximate the probability that there are no deaths in 1897?

\point{Proposition:} if for some constant $\lambda > 0$,  $X_n \sim \Bin(n, \lambda/n)$ and $X \sim \Poi(\lambda)$, then $X_n \dcv X$.

\point{Proof:} a short proof consists in using characteristic functions. As an exercise, show that the characteristic functions of the binomial random variables in the sequence are given by 
\[ \varphi_{X_n}(t) = \left( 1 - \frac{\lambda}{n} + \frac{\lambda}{n} e^{it} \right)^n, \]
and the characteristic function for the Poisson is given by:
\[ \varphi_{X}(t)  = \exp(\lambda(e^{it} - 1)). \]
It is easy to show that for all $t$, $\varphi_{X_n}(t) \to  \varphi_{X}(t)$. 

\point{Exercise:} show how to solve the motivation problem using this proposition.

\point{Generalization using ``triangular arrays:''} let $X_{n,m}$ denote an array with the variables in each row being independent:
\begin{itemize}
  \item $X_{2,1}, X_{2,2}$ are independent.
  \item $X_{3,1}, X_{3,2}, X_{3,3}$ are independent.
  \item etc.
\end{itemize}
Assume $X_{n,j} \sim \Bern(p_{n,j})$, where
\begin{enumerate}
  \item we have convergence of row sums to $\lambda$: $\lim_{n\to\infty} \sum_{j=1}^n p_{n,j} = \lambda$,
  \item and as we look at larger and larger row indices $n$ the Bernoulli probabilities become uniformly rare across the columns $j$: $\lim_{n\to\infty} \max_j p_{n,j} = 0$.
\end{enumerate}
Then: $S_n = X_{n,1} + X_{n,2} + \dots + X_{n,n}$ is such that $S_n \dcv X$, where $X \sim \Poi(\lambda)$.

\point{Exercise:} show that the earlier result is a special case of this one.

\point{Proof:} based on the Stein-Chen coupling method. See textbook p. 459. 


\subsection{Poisson processes: motivation, definition and construction}

\point{Prerequisite definition:} a \emph{Radon-Nikodym derivative} (RN) is like a density, but where we build a measure instead of a probability measure. Compare:
\begin{itemize} 
  \item The probability measure $\nu$ has density $f$ with respect to a measure $\mu$ if \[ \nu(A) = \int_A f(x) \mu(\ud x). \]
  \item The measure $\nu$ has RN derivative $f$ with respect to a measure $\mu$ if \[ \nu(A) = \int_A f(x) \mu(\ud x). \]
\end{itemize}

\point{Poisson process---motivation and connexion to Poisson convergence:}  
\begin{itemize}
  \item Let $n$ denote the number of nucleotides in your genome (about 3e9 base pairs). 
  \item Say you are exposed to a small dose of radiation. 
  \item Let $Y_i$ denote the indicator that nucleotide $i$ in a cell mutates. 
  \item Say $Y_i \iidsim \Bern(1e-9)$, and let $S_n = Y_1 + Y_2 + \dots + Y_n$.  
  \item By the Poisson convergence section, $S_n$ is approximately Poisson distributed, with rate parameter $\lambda$.
  \item Now let us view the genome as one line segment (a continuous approximation of the discrete DNA strand). 
  \item What is the approximate probability of the number of mutation in the first half of the genome? Last quarter? Are those independent?
\end{itemize}

\point{Poisson distribution vs. process:} the distribution only keeps track of the total number of mutation, while the process keeps track of where they occur (equivalently, the number in \emph{any} subset of genome).

\point{Notation:} 
\begin{itemize}
  \item $\ppspace$: the space in which each point sits in (for example, the interval approximating the genome).
  \item For $A \subset \ppspace$, let $N_A$ denote the random number of points that fall in $A$.
\end{itemize}

\point{From the motivating example:} we are interested in cases where:
\begin{enumerate}
  \item $A \cap B = \emptyset \Longrightarrow N_A$ and $N_B$ are independent.
  \item $N_A \sim \Poi($length of $A)$. Let us denote the length of $A$ by $\mu(A)$.
\end{enumerate}

\point{Recall:} a nice property of these random variables is:
\[ N_{A\sqcup B} = N_A + N_B \sim \Poi(\underbrace{\mu(A) + \mu(B)}_{\mu(A\sqcup B)} \]

\point{Note:} this is an instance of what is called \emph{Kolmogorov consistency}.

\point{Example 2:} random positions of animals in the forest $\ppspace = [0,1]\times[0,1]$, $\mu = \mu^2$. Exercise: what are the assumptions we implicitly make if we say the animal positions are distributed according to a Poisson process?

\point{Example 3:} customers entering a store. Here, $\ppspace = [0, \infty)$ represents time. Problem: same expected number of customers between 1am - 2am and 1pm - 2pm. Solution? Non-uniform $\mu$, defined via a RN derivative. 

\point{Next:} a formal definition of Poisson processes. 

\point{Assumptions on} $\mu$, called the intensity measure:
\begin{enumerate}
  \item A measure on $(\ppspace, \events_\ppspace)$,
  \item Can be broken into a countable collection of finite measure segments: exists countable partition $A_i$ of $\ppspace$ such that $\mu(A_i) < \infty$.
  \item The intensity measure is non-atomic: for all $x \in \ppspace$, $\mu(\{x\}) = 0$. 
\end{enumerate}

\point{Formal definition of a Poisson Process (PP):} under the above assumption, the collection of random variables $\{N_A : A \in \ppspace\}$ is called a Poisson process with intensity $\mu$ if:
\begin{enumerate}
  \item $A \cap B = \emptyset \Longrightarrow N_A$ and $N_B$ are independent.
  \item $N_A \sim \Poi(\mu(A))$.
\end{enumerate}

\point{Constructive definition:}
\begin{enumerate}
  \item Simulate the point for one of the block $A_i$ at the time:
  \begin{enumerate}
    \item Simulate the number of points in $A_i$, $N_{A_i} \sim \Poi(\mu(A_i))$.
    \item For $i = 1, \dots, N_{A_i}$:
    \begin{enumerate}
      \item $X_i ~ \mu / \mu(A_i)$
    \end{enumerate}
    \item The random set of points in $A_i$ is $S_i = \{X_1, \dots, X_{N_{A_i}}\}$
  \end{enumerate}
  \item Return the union of the points over all the $S_i$'s
\end{enumerate}

Note that equivalently, the PP can be viewed as a random set of points, denoted $S \sim \PP(\mu)$.


\subsection{Reference for the Poisson process}

The textbook (Gut 2013) has a limited coverage of Poisson processes. The best reference in my opinion for this part of the material is the beautiful short monograph by Kingman (1993), ``Poisson Processes.'' It is out of print, but a quick Google revealed that it seems to be currently available as pdf. 


\subsection{Poisson process with constant intensity on the real line}

\point{Special case:} linking our definition of PP to the undergraduate definition. Assume:
\begin{itemize}
  \item $\ppspace = [0, \infty)$
  \item $\mu =$ uniform.
\end{itemize}
Let $T_1, T_2, \dots$ denote the arrival times, i.e. $T_i = \inf\{t : N_{[0,t]} \ge 1\}$.
Then:
\begin{eqnarray}
\P(T_1 > t) &=& \P(N_{[0,t]} = 0) \\
&=& \frac{e^{t - 0} (t-0)^n}{n!} \;\;n=0 \\
&=& 1 - \text{CDF of an exponential}
\end{eqnarray}
Similarly, each inter-arrival times $T_i - T_{i-1}$ are also exponential(1).


\subsection{Superposition and thinning}

\point{Motivation:} consider the example concerned with animals in the forest. What if we consider two species simultaneously, assuming they do not interact?

\point{Proposition (superposition):} let $\{S_i\}_{i = 1}^n$ denote a collection of PPs, $S_i \sim \PP(\mu_i)$, where the intensity measures are defined on a common space, $\mu_i : \events_{\ppspace} \to [0, \infty)$. Then $S \defeq \cup_i S_i \sim \PP(\sum_i \mu_i)$.

\point{Exercise:} prove this using the algorithmic construction and the fact that the sum of Poissons is Poisson. 

\point{Note:} this can be generalized to countable unions of PPs, provided that the sum of intensities is still $\sigma$-finite.

\point{Definition (thinning):} Let $S = \{X_1, X_2, \dots\} \sim \PP(\mu)$, $Y_i \sim \Bern(p)$. We defined the thinned process by using the iid coin flips $Y_i$ to decide, for each point in $S$, whether we keep this point or not: $T \defeq \{X_i \in S : Y_i = 1\}$.

\point{Proposition (thinning):} The random set $T$ defined above by the thinning procedure is a Poisson process, and its intensity is obtained by scaling down the original intensity $\mu$: $T \sim \PP(p\mu)$, where $(p\mu)(A) \defeq p \mu(A)$.

\point{Proof:} will be easier once we talk about conditioning next week. 


\subsection{Mapping}

\point{Motivation:} consider the animals in the forest example. What is the distribution of the $x$-coordinates of the random animal locations?

\point{More general setup:} suppose we have a mapping $\phi : \ppspace \to \ppspace'$. In the above example, $\ppspace = [0,1]^2$, $\ppspace' = [0,1]$, and $\phi(x, y) = x$.

\point{Proposition:} Let $S \sim \PP(\mu)$. Assume $\mu^\star(A) \defeq \mu(\phi^{-1}(A))$ has no atom and is $\sigma$-finite. Then $\phi(S) \sim \PP(\mu^\star)$.


\subsection{Compound PP}

\point{Observation:} let us view the collection of random variable $N_A$ differently, writing it as $N(\cdot)$ instead. What is this? A random measure! Now let us fix a real-valued function $f : \ppspace \to \R$. We can then define the random integral:
\[ Y = \int f \ud N = \sum_{X \in S} f(X). \]

\point{Definition:} $Y$ is called a compound Poisson process.

\point{Next:} we would like to gain information about the distribution of $Y$. We will see that we can compute its characteristic function:
\[ \E\left[e^{itY}\right] = \exp\left\{ \left(e^{itf(x)} - 1\right) \mu(\ud x) \right\}. \]
See the reference by Kingman for details and more.


\section{Conditioning}

\subsection{Background: $\sigma$-algebra and information}\label{sec:sigma-algebra-and-information}

\point{Motivation:} let $\Omega$ denote the space of outcomes corresponding to throwing two independent dice. Let $X_1$ denote the first throw, and $X_2$, the sum of the two throws. We would like to guess $X_2$ from observing only $X_1$.

\point{Notation:} we call a random variable $\delta$ trying to predict $X_2$ an \emph{estimator} of $X_1$. Consider the following suggestions:
\begin{itemize}
  \item $\delta_1 = 3 + X_1$,
  \item $\delta_2 = X_2$.
\end{itemize}
Clearly, there is a problem with $\delta_2$: it is cheating! Solution: require an estimator to be a function of only the observed random variables, $\delta = f(X_1)$.

\point{Equivalently:} from the bonus problem in assignment 1, require that $(\sigma \in I) \in \sigma(X_1) = \{(X\in I) : I \in \borels\}$.


\subsection{Conditioning on an event}

\point{Motivation:} a couple has two children. Your initial beliefs over the sex of the second child is 1/2 boy, 1/2 girl. You get one piece of information: at least one of the two chilren is a girl. What is the optimal way of updating your beliefs?

\point{Conditioning on an even:} consists in an operator that takes as an input:
\begin{enumerate}
  \item some \emph{a priori} beliefs (a probability distribution $\P(\cdot)$), 
  \item as well as an observed event $E$. 
\end{enumerate}
The interpretation of $E$ is that you know the true outcome is somewhere in $E$, but you still do not know which outcome in $E$. The output of conditioning on an event: a new, updated belief, denoted $\P(\cdot | E)$. The optimal value for this updated belief, for any query set $A$, is given by:
\[ \P(A | E) = \frac{\P(A\cap E)}{\P(E)}. \]

\point{Example:} the updated belief for the motivation question is 1/3 boy, 2/3 girl.


\subsection{Conditioning on a random variable}

\point{Interpretation:} let $X$ and $Y$ be two random variables defined on the same probability space:
\begin{itemize}
  \item $Y$ is observed,
  \item $X$ is the variable you would like to predict.
\end{itemize}
The conditional expectation of $X$ given $Y$ is a new estimator random variable $\delta = f(Y)$, which can be interpreted as the ``best'' estimator of $X$ based on $Y$. Notation: this random variable $\delta$ is denote by $\E[X|Y]$.

\point{Notion of optimality:} consider the optimization program
\[ \text{minimize} \E(f(Y) - X)^2, \]
where minimization is performed over functions $f$ (technically, over measurable functions). If a function $f^*$ maximizes this program and give a finite value to the objective function, then $\E[X|Y] \defeq f^*(Y)$.

\point{Exercise:} show that in the two children problem, you obtain:
\[ f^*(x) = \bracearraycond{0 \;\;&\text{if }x=0\\ 2/3\;\;&\text{if }x=1\\\text{any value}\;\;&\text{otherwise.}} \]

\point{Note:} by the discussion in Section~\ref{sec:sigma-algebra-and-information}, note that we only used $Y$ via $\sigma(Y)$. For this reason, we can define the notion of conditioning on a $\sigma$-algebra $\events$, denoted $\E[X|\events]$, in the same way as we did above.

\point{Notation:} $\|X\|_2 \defeq \sqrt{\E[X^2]}$, $\L_2 \defeq \{\text{r.v. } X : \|X\|_2 < \infty\}$.

\point{Note:} we will see that the above definition always works when $Y \in \L_2$. To generalize this to $Y \in \L_1$, we will make use of a generalization of a property which is itself very important in practice: the law of total expectation.


\subsection{The law of total probability and expectation}

\point{Useful property:} the law of total expectation:
\[ \E[X] = \E[\E[X|Y]], \]
and its associated law of total probability, a special case where $X = \1_A$:
\[ \P(X \in A) = \E[ \P(X \in A|Y) ]. \]

\point{Example/exercise:} suppose that
\begin{eqnarray*}
U &\sim \Unif(0,1) \\
X | U &\sim \Bin(n, U),
\end{eqnarray*}
where the second line means that if $Z \sim \Bin(n,p)$, then $\E[g(X) | U] = \E[g(Z)]|_{p=U}$. Show that $\P(X = i) = 1/(n+1)$ using the law of total probability. 

\point{Discrete case:} to get some intuition, let us first assume that $Y$ is simple, $Y = \sum_{i=1}^n a_i \1_{A_i}$, where $A_i$ forms a partition. By simple set properties:
\begin{eqnarray*}
\P(A) &=& \sum_{i=1}^n \P(A \cap (Y=a_i)) \\
&=& \sum_{i=1}^n \P(A|Y=a_i) \P(Y = a_i).
\end{eqnarray*}
Now, let: 
\begin{equation} 
f(y) = \bracearraycond{
  \P(A|Y=a_i) &\text{if }y\in\{a_1, \dots, a_n\} \\ 
  \text{arbitrary,} & \text{otherwise.}
}
\end{equation}
We will see shortly that $\P(A|Y) = f(Y)$.

\point{Next:} it is easy to generalize this property to cases where we select only a subset of the blocks in the partition. To formalize this, let $B \subset \{a_1, \dots, a_n\}$, then a set of blocks can be denoted as $H \defeq (Y \in B)$, i.e. $H \in \sigma(Y)$. With this notation, show that:
\[ \P(A \cap H) = \E[ \1_H \P(A | Y). \]

\point{Definition/property:} we will use this property, which we have shown to hold for simple $Y$, as the basis of our fully general definition of conditional expectation.  Let $X, Y \in \L_1$ be defined on a common probability space. Then there exists a random variable, denoted $\E[X|Y]$, such that:
\begin{enumerate}
  \item $\E[X|Y] \in \sigma(Y)$, (i.e. $\E[X|Y]$ is an estimator of $X$ based on $Y$)
  \item for all $H \in \sigma(Y)$, $\E[ \1_H X ] = \E[ \1_H \E[X|Y] ]$.
\end{enumerate} 
Moreover this random variable is almost sure unique.

\point{Proof:} if you are interested, see \\
{\footnotesize \url{http://bibserver.berkeley.edu/205/WorkInProgress/CondExpExistUnique.pdf}}

\point{Alternative proof:} there is an alternative argument based on Radon-Nikodym theorem, that is that $\nu$ has density $f$ with respect to $\mu$ if and only if $(\mu(A) = 0 \Rightarrow \nu(A) = 0)$ (a property called $\nu$ is absolutely continuous with respect to $\mu$, denoted $\nu \ll \mu$). I am happy to go over it at the office hours.


\subsection{Key properties}

\point{Important exercise:} suppose $(X, Y)$ have joint density $f(x,y)$. Then $\E[X|Y] = \psi(Y)$, where 
\begin{eqnarray*} 
\psi(y) &\defeq& \bracearraycond{\int x f_{X|Y}(x|y) \ud x &\text{if }f_Y(y) > 0 \\ \text{arbitrarily,} &\text{otherwise,}} \\
 f_{X|Y}(\cdot|y) &\propto& f(\cdot, y) \\
&\defeq& \frac{f(\cdot,y)}{f_Y(y)} \\
f_Y(y) &\defeq& \int f(x, y) \ud x.
\end{eqnarray*}

\point{Terminology:} the function $f_Y(\cdot)$ is called the marginal density, and the function $f_{X|Y}(\cdot|y)$, the conditional density. Note that these are indeed densities. 

\point{Property:} when $X,g(Y) \in \L_1$, \[ \E[ X g(Y) | Y] = g(Y) \E[X|Y]. \]

\point{Proof:} we have to show that the RHS satisfies the two axioms of conditional expectation. For (1), clearly it is of the form $f(Y)$. For (2), let $H \in \sigma(Y)$, and suppose first that $g(y) = \1_B$, implying that $g(Y) = \1_G$, $G \in \sigma(Y)$. We get:
\begin{eqnarray*}
\E[ \1_H X \1_G ] &=& \E[ \1_H \1_G \E[X|Y] ] \Leftrightarrow \\
\E[ \1_{H'} X ] &=& \E[ \1_{H'} \E[X|Y] ],
\end{eqnarray*}
which is true since $H' \in \sigma(Y)$. Complete the proof using the DCT.


\subsection{Equivalence of the two definitions}

So far, we have given two definitions of conditional expectations, one that works for $\L_2$ only, and one that works for $\L_1$ and $\L_2$. Here we show that for the $\L_2$ setup, our new, general definition agrees with the $\L_2$ specific earlier definition:
\begin{eqnarray*}
\E[ (f(Y) - X)^2 ] &=& \E[ \E[ (f(Y) - X)^2 | Y ]] \\
&=& \E [ \underbrace{ \E[ f(Y)^2 | Y ]}_{(f(Y))^2} - \underbrace{ 2 \E[ f(Y) X | Y] }_{2 f(Y) \E[X|Y]} + \E[ X^2 | Y] ] \\
&=& \E[ \underbrace{(f(Y) - \E[X|Y])^2}_{\ge 0} + \underbrace{ \E[X^2 | Y] - (\E[X|Y])^2 }_{\defeq \var[X|Y]} ].
\end{eqnarray*}
Note:
\begin{enumerate}
  \item the RHS$\ge \E[ \var[X|Y]]$.
  \item if $f(Y) = \E[X|Y]$ a.s., then RHS$=\E[\var[X|Y]]$.
\end{enumerate}


\subsection{The Bayes estimator}

\point{Motivation:} a complete order over estimator.

\point{Frequentist notion of optimality:} since in the MLE (Maximum Likelihood Estimation) framework we do not place a prior on $\theta = X$, the performance of an estimator $\delta$ depends on the true parameter $\theta$:
\[ \text{MSE}_\theta(\delta) = \E_\theta[(\delta - \theta)^2] = \E[(\delta - \theta)^2 | \theta]. \]
The MLE mimizes this objective function.

\point{Issue:} the space of functions (of $\theta$ in this case) is not a complete order. There is not a notion of an a.s. best estimator under the above criterion.

\point{Solution:} restrict the class of estimators, e.g. to unbiased ones.

\point{Criticism:} can be restrictive/artificially rule out good estimators. 

\point{Bayesian alternative:} average over $\theta$, to get a real number summary of each estimator:
\[ \text{mse}(\delta) = \E[ \text{MSE}_\theta(\delta) ]. \]


\subsection{Geometric view of expectation and further properties}

Suppose in the remaining of the section that $Y \in \L_2$. 

\point{Recall:}
\begin{itemize}
  \item Real vector space: a set $V$ of points in $\R^d$ and a $+$ and $\cdot$ operations such that a set of useful properties hold (associativity, $v_1 + (v_2 + v_3) = (v_1 + v_2) + v_3$, distributivity $a(v_1 + v_2) = a v_1 + a v_2$, etc (see wikipedia)).
  \item Abstract vector space: a set of objects $V$ with two operations satisfying the same properties.
\end{itemize}

\point{Example:} $V = \L_2 = \{\text{r.v. }X: \Omega \to \R, \E X^2 < \infty\}$. Easy exercise: read the axioms on wikipedia and check they are satisfied with $+$ denoting the addition of functions and $\cdot$, the multiplication of a function by a constant.

\point{Three important ideas from linear algebra.}

\point{1. Norm:} $\|\cdot\| : V \to [0, \infty)$. Key defining property (see wiki for the other ones): $\|v + w\| \le \|v\| + \|w\|$ (triangle inequality).

\point{Examples:}
\begin{enumerate}
  \item $V = \R^2$, $\|v\|_2 = \sqrt{v_1^2 + v_2^2}$, triangle inequality is the Pythagorean theorem.
  \item $V = \L_2$, $\|X\| = \sqrt{\E X^2 }$, triangle inequality is called Minkowski's inequality (see textbook, p.129).
\end{enumerate}

\point{2. Subspace:} a ``closed'' subset of a vector space, $W \subset V$, i.e. such that $v_1, v_2 \in W \Rightarrow v_1 + v_2 \in W$. 

\point{Examples:}
\begin{enumerate}
  \item $\{$points of the form $(0, x)\} \subset \R^2$,
  \item $W = \{Z \in \L_2 : Z \in \sigma(Y)\} = \{Z \in \L_2 : Z = f(Y), f\text{ measurable}\}$. Here $W$ is closed because $f_1, f_2$ measurable implies $a f_1 + f_2$ is also measurable.
\end{enumerate}

\point{3. Projection:} of a point $v$ into a subspace $W$. This projection is defined as:
\[ \argmin_{w \in W} \| v - w\|. \]

\point{Key example:} when $V = \L_2$, $W$ as point 2l above:
\begin{eqnarray*}
\E[X|Y] &=& \argmin_{Z \in W} \| Z - X \| \\
&=& \text{projection of $X$ into $W$.}
\end{eqnarray*}

\point{Application:} if $\events_1 \subset \events_2$, then:
\begin{eqnarray*}
\E[\E[X|\events_1]|\events_2] &=& \E[\E[X|\events_2]|\events_1] \\
&=& \E[X | \events_1].
\end{eqnarray*}

\point{Proof sketch:} draw a picture!

\point{Last few properties of conditional expectations:}
\begin{enumerate}
  \item Linearity: $\E[a X + Y|\events] = a \E[X|\events] + \E[Y|\events]$.
  \item Monotonicity.
  \item Jensen's inequality.
  \item Chebyshev.
\end{enumerate}


\subsection{Conditional independence and Bayes Ball algorithm}

Compare and contrast the following definition with the definition of independence:

\point{Definition:} two random variables $X$ and $Y$ are conditionally independent given $Z$ if $\E[g_1(X) g_2(Y)|Z] = \E[g_1(X)|Z] \E[g_2(Y)|Z]$. 

\point{Exercise:} independence does not imply conditional independence, and conditional independence does not imply independence.

\point{Reference:} the Bayes ball algorithm is a method for identifying certain conditional independence relationships from a directed graphical model (Bayes net). It is best explained with pictures. See {\footnotesize \url{http://www.cs.princeton.edu/courses/archive/spr09/cos513/scribe/lecture02.pdf}}, where you can also find a proof.


\section{Markov chains}

\subsection{Basic definitions and examples}

\point{Informal:} A sequence of random variables where the future is independent of the past given the present.

\point{From the graphical model point of view:} a chain-shaped graphical model.

\point{Formal:} a sequence of random variables $X_i : \Omega \to \states$ is Markov if for all $A \in \events_\states$, 
\[ P(X_{n+1} \in A | X_0, X_1, \dots, X_n) = \P(X_{n+1} \in A | X_n). \]

\point{Examples:}
\begin{enumerate}
  \item Let $E$ denote a set of undirected edges over $\states$. Define \[ \P(X_{n+1} = x | X_n) = \frac{1}{Z(X_n)} \1[\{x, X_n\} \in E]. \]Note that the normalization $Z(x)$ is given by the number of nodes connected to $x$. To make this more interesting, consider the vertices of the graph given by the location of a knight on a game of chess (8x8 square), and the edges given by the moves allowed for the knight (L shaped moves moving by one square in one axis, and 2 in the other axis). 
  \item The Wright-Fisher model: suppose that $N$ bacteria can live in a Petri dish. There are two species, blue and green bacteria. In the first day, there are $X_1$ green and $N-X_1$ blue bacteria. In the next day, there are still $N$ bacteria, the descendants from the previous generation. The parent of each of the $N$ in day 2 are selected independently and uniformaly from those in day 1. The color is inherited without mutation. This mean: \[ \P(X_n = k | X_n) = \binom{N}{k} \left(\frac{X_n}{N}\right)^k \left( 1 - \frac{X_n}{N}\right)^{N-k}. \]Note that $X_n = 0 \Rightarrow X_{n+1} = 0$, and $X_n = N \Rightarrow X_{n+1} = N$. These are called absorbing states.  
\end{enumerate}


\subsection{Representation under the homogeneity condition}

\point{Note:} from the previous part on conditioning, we have that $\P(X_{n+1} = y | X_n) = f(X_n)$ for some $f(x)$. In fact, this function will also depend on $y$ and $n$. We denote it by $p_n(x \to y)$ and call it the transition probability.

\point{Definition:} a Markov chain is homogeneous is $p_n(x \to y) = p_{n+1}(x \to y)$ for all $n, x, y$. We denote it by $p(x \to y)$. 

\point{Visualization:} of $p(x \to y)$ via a \emph{state diagram}. Consider a directed graph where the nodes are the points in $\states$ and where the is an edge $(x \to y) \in E$ if and only if $p(x \to y) > 0$. Informally, this encodes the sparsity patterns of the transition probabilities.

\point{Note:} if $X_n$ is finite and homogeneous, it is characterized by 2 objects:
\begin{enumerate}
  \item the transition probabilities, $p = p_n$,
  \item an initial distribution with pmf $\mu$, $\P(X = x) = \mu(x)$. 
\end{enumerate} 

\point{Notation:} it is often useful to have the initial distribution put all the mass on a single point $x$, in which case we write $\P_x$. E.g. $\P_x(X_1 = y) = p(x \to y)$. 


\subsection{First connection with linear algebra: Chapman-Kolmogorov equation}

\point{Matrix notation for transition probabilities:} to create connections with linear algebra, it will be useful to organize the transition probabilities into a matrix (assuming without loss of generality that $\states = \{1, 2, \dots, K\}$): $M_{x,y} = p(x \to y)$.

\point{Vector notation for pmfs over the states:} if $\mu$ is a pmf over $\states$ (for example, an initial distribution), we can view it as a vector, $\mu = (\mu(1), \mu(2), \dots, \mu(K))$. 

\point{How to find the 2-step transition,} i.e. $\P(X_{n+2} = y | X_n = x)$? As usual we reintroduce the random variable $X_{n+1}$ using marginalization/the law of total probability:
\begin{eqnarray*}
\P(X_{n+2} = y | X_n = x) &=& \sum_{z \in \states} \P(X_{n+2} = y, X_{n+1} = z | X_n = x) \\
&=& \sum_{z \in \states} \P(X_{n+1} = z | X_n = x) \P(X_{n+2} = y | X_n = x, X_{n+1} = z) \;\;(\text{chain rule}) \\
&=& \sum_{z \in \states} \P(X_{n+1} = z | X_n = x) \P(X_{n+2} = y | X_{n+1} = z) \;\;(\text{Markov assumption}) \\
&=& \sum_{z \in \states} p(x \to z) p(z \to y) \\
&=& (M^2)_{x,y}.
\end{eqnarray*} 

\point{Exercise:} show that more generally, $\mu M^n = (\P(X_n = 1), \P(X_n = 2), \dots, \P(X_n = K))$.


\subsection{Hitting probabilities}

\point{Setup:} suppose we have an homogeneous finite state Markov chain $X_n$ with exactly two absorbing states, denoted $x_0$ and $x_1$.

\point{Example/exercise:} create a state space to model two-players snakes and ladders. The interpretation of $x_0$ is that the first player wins, and $x_1$, that the second player wins. 

\point{Definition:} a hitting time $T_x$ is the first time (possibly infinity) that a state $x$ is reached, i.e. $T_x = \inf \{n : X_n = x\}$. 

\point{Goal:} computing $h(x) = \P_x(T_{x_1} < T_{x_2})$.

\point{Some easily obtained constraints on $h$:}
\begin{enumerate}
  \item $h(x_1) = 1$ and $h(x_2) = 0$.
  \item $0 \le h \le 1$.
  \item Exercise (using an argument similar to the one used for the Chapman-Kolmogorov equation): \[ h(x) = \sum_{y\in\states} p(x\to y) h(y). \]
\end{enumerate}

\point{Main result:} these constraints can be used to find the numerical value of $h(x)$. More precisely, $h(x)$ is the minimum function satisfying the above three conditions.


\subsection{Asymptotic behavior: overview}

\point{Assumptions:} In the following, we will:
\begin{itemize}
  \item always assume homogeneity,
  \item start by assuming finite $\states$, then relax later on.
\end{itemize}

We will cover two main results, used in different contexts:
\begin{enumerate}
  \item The law of large number for Markov chains. Informally, that sums converge to a constant: \[ \frac{1}{N} \sum_{n=1}^N f(X_n) \ascv c,\]where this constant is obtained by an expectation, $c = \E[f(X_\infty)]$, and $\pi(x) = \P(X_\infty = x)$ is called the stationary distribution. This is used for example in the context of Markov chain Monte Carlo (MCMC), covered next week. This only requires a simple condition called irreducibility in our setup.
  \item Convergence of marginals: \[ \P(X_n = x) \to \P(X_\infty = x), \]used for example to determine how many times you need to shuffle a deck of cards. It uses a second condition called aperiodicity in addition to the irreducibility condition mentioned above.
\end{enumerate}


\subsection{Law of large number for Markov chains}

\point{Definition:} a directed path $x \leadsto y$ in the state diagram of a Markov chain is a list of connected edges $x = x_1 \to x_2 \to \dots \to x_n = y$ where $(x_i \to x_{i+1}) \in E$.

\point{Definition:} a Markov chain is irreducible if there is a directed path between each ordered pair of states.

\point{Theorem:} if $X_n$ is:
\begin{enumerate}
  \item Markov,
  \item homogeneous,
  \item finite, 
  \item irreducible,
\end{enumerate}
then \[ \frac{1}{N} \sum_{n=1}^N f(X_n) \ascv \E[f(X_\infty)],\]
where \[\pi(x) = \P(X_\infty = x) = \frac{1}{\E_x T_x}. \] This is true for any initial distribution $\mu$.

\point{Idea:} $i$-block, which is a subset of the Markov chain trajectory from one visit to a fixed arbitrary state $i$ to the next visit to $i$. We will related the average length of these blocks to the inverse of the expected time spent at $i$. Now these blocks are iid, which will allow us to use the LLN.

\point{Lemma/exercise:} $E[T_i] < \infty$ since the chain is finite and irreducible (recall that $T_x$ is a hitting time, defined in the previous section).

\point{Main steps of the proof of LLN:}
\begin{enumerate}
  \item It is enough to show the theorem is true for the test function $f(x) = \1[x = i]$ for some fixed reference state $i$.
  \item Define $N_n = \sum_{j=1}^n \1[X_j = i]$, the number of visits to $i$ in the first $n$ steps. We will show that $\frac{1}{n} N_n \to 1/\E_x [T_x]$, almost surely.
  \item Define $R(k) = \inf\{n:N_n = k\}$, the time of the $k$-th return at $i$.
  \item Note: is we let $|B_j|$ denote the length of the $j$-th $i$-block, $R(k) = |B_1| + |B_2| + \dots + |B_{k-1}|$.
  \item Hence, by the LLN, $R(k) / (k-1) \to \E_x [T_x]$ almost surely.
  \item Note: $R(N_n) \le n \le R(N_n + 1)$ (by definition, draw a picture to convince yourself).
  \item Dividing everything in the above inequalities by $N_n$, we get $\frac{1}{n} N_n \to 1/\E_x [T_x]$, almost surely.
\end{enumerate}


\subsection{Extension to countably infinite spaces}

The main difficulty is that the fact that $\events$ is counable and $X_n$ is irreducible does not imply that $\E_x T_x < \infty$.

\point{Counter-example:} from earlier in the course, drunk bird might not return home. Recall, using BC 1, if we let $A_n = (X_n = (0,0,0))$, then $\P(A_n \io) = 0$, implying that $\P_x(T_x = \infty) > 0$, so that $\E_x T_x = \infty$. 

\point{Solution:} add an assumption. We say that a state is positive recurrent if $\E_x T_x < \infty$. With this additional assumption, the LLN holds in countably infinite spaces.

\point{Note on terminology:} why ``positive'' recurrent? First, recurrent is a weaker condition, $\E_x N_x = \infty$. Now this is subdivided into two sub-cases, positive recurrent, defined above, and null recurrent, where $\E_x T_x = \infty$ but $\E_x N_x = \infty$. Finally, the opposite of recurrent is transient.


\subsection{Convergence of the marginals}

Now, we would like to investigate the convergence of the marginals, which, using our linear algebra, boils down to investigating $\lim_{n\to\infty} M^n$ (note that we get back to the finite case for now). 

\point{Problem:} just the conditions we used for the LLN are not enough! Counter example: $M = (0 1; 1 0)$.

\point{Solution:} add an assumption, aperiodicity. To define it, we will need a few definitions.

\point{Definition:} the period of $x\in\states$ is defined as $d_x = \gcd\{n : (M^n)_{x,x} > 0\}$.

\point{Definition:} a state is aperiodic if $d_x = 1$. A chain is aperiodic if all states are aperiodic.

\point{Theorem:} if $X_n$ is:
\begin{enumerate}
  \item Markov,
  \item homogeneous,
  \item finite, 
  \item irreducible,
  \item aperiodic,
\end{enumerate}
\[ \P(X_n = x) \to \P(X_\infty = x), \]for any initial distribution $\mu$.

\point{Other notation for the result:} the above means that $\lim_{n\to \infty}$ exists and is composed of identical rows. Let us denote this limit by $L$. Let us denote its identical rows by $\pi$. 

\point{Note:} this means that $L = LM$, i.e. $\pi = \pi M$, or \[ \pi(y) = \sum_{x \in \states} \pi(x) p(x \to y). \]This is called the stationary equation or global balance equation.

\point{Application:} debugging of MCMC algorithms (more on this later).

\point{Note:} this provide another connection with linear algebra, namely that the stationary equation is an eigenvector of $M^T$.

\point{Proof idea for the convergence of the marginals:} coupling.  
\begin{enumerate}
  \item We build two chains $\tilde X_n$ and $\tilde Y_n$, both with transition probabilities $p(x \to y)$.
  \item $\tilde X_0 \sim \pi$, while $\tilde Y_0 \sim \mu$. 
  \item $\tilde X_n$ and $\tilde Y_n$ are independent until they meet for the first time, at which point they stay together forever.
  \item Note: $\tilde X_0 \sim \pi \Rightarrow X_n \sim \pi$ for all $n$. 
  \item It is therefore enough to show that the two chain meet almost surely. This is where aperiodicity is used!
\end{enumerate}

\point{For details}, see: \\{\footnotesize \url{http://bibserver.berkeley.edu/205/WorkInProgress/limit_theorem.pdf}}


\section{Application: MCMC}

\subsection{Motivation}

\point{Setup:} let $y$ denote an observation and $x$ denote an unknown quantity (parameter and/or future or interpolated observation, discrete latent variables, etc). 

\point{Two approaches:} 
\begin{enumerate}
  \item In practice, at the core of maximum likelihood approaches, a key operation consists in maximizing a likelihood, $x^* = \text{argmax } p_{Y|X}(y|x)$, often via some optimation tools.
  \item In practice, at the core of most practical Bayesian approaches, a key operation consists in sampling from a posterior distribution $x^{(i)} \sim p_X(x) p_{Y|X}(y|x)$, often via an approximate sampling method such as MCMC (Markov chain Monte Carlo) or SMC (sequential Monte Carlo).
\end{enumerate}

\point{Cases where the second method is advantageous:}
\begin{enumerate}
  \item Obtaining uncertainty estimates over combinatorial structures. In this case, typical method to get confidence intervals around maximum likelihood estimate either fail (e.g. those based on CLT), or are very inefficient (e.g. the bootstrap).
  \item Cases where the maximum of a density is not a good summary. This can arise in situations from partial unidentifiability and stochastic processes for example.
\end{enumerate}


\subsection{How to use posterior samples}

Let us say we are given $x^{(1)}, x^{(2)}, \dots \sim p_X(x) p_{Y|X}(y|x)$. How should these be used? There is often something more optimal than say taking the sample with highest posterior (something beginners often resort to). 

\point{Bayesian recipe:} the Bayesian framework specifies a 3-steps recipe to approach this question.
\begin{enumerate}
  \item Specify a loss function $L$ over the possible output (decisions/things you are trying to predict). Example: rand loss over clusterings.
  \item Compute the posterior distribution. In practice, this is done using an approximate method such via samples $x^{(1)}, x^{(2)}, \dots$ coming from MCMC.
  \item Minimize the posterior expected loss:
\[ \text{argmin}_x \E[L(x, X)|Y] \approx \text{argmin} \sum_{i=1}^N L(x, x^{(i)}). \] Note that we are optimizing, but not a density as in maximum likelihood, rather we are optimizing an integrated loss function. Example/exercise: write the objective function in the case of a rand loss on clusterings.
\end{enumerate}

\point{Pros:}
\begin{itemize}
  \item Statistical efficiency (admissibility, asymptotic efficiency, etc).
  \item Can be automated via probabilistic programming.
  \item Combinatorial latent variables supported.
  \item Correct behavior under partial unidentifiability.
\end{itemize}

\point{Con:} the main con is computational. Many sampling problems have been shown to belong to a provably computationally difficult class of problems called ``\#P hard problems'' (a trickier version of NP hard problems). 


\subsection{Examples of MCMC algorithms on Ising models}

\point{Motivation:} computer vision, spatial statistics (see lecture slides for some visualizations).

\point{Basic model:} consider a 3x3 grid with each node representing a binary variable. The state space is $\states = {+1, -1}^{3\cdot 3}$. Notation: if $x \in \states$, we write $x_{i,j}$ for the value of the variable at node at row $i$ and column $j$ in the grid. We write $(i,j) \sim (i',j')$ if two nodes $(i,j)$ and $(i',j')$ are immediate neighbors, i.e. if $|i-i'| + |j-j'| = 1$. Define the following distribution:
\begin{equation}\label{eq:target}
\pi(x) = \P(X = x) = \frac{1}{Z} \exp\left( \sum_{(i,j)\sim(i',j')} x_{i,j} x_{i',j'} \right).
\end{equation}

\point{Note:} $Z$ can quickly become very hard to compute as the grid gets larger. We have: \[Z = \sum_{x \in \states} \exp\left( \sum_{(i,j)\sim(i',j')} x_{i,j} x_{i',j'} \right),\]and just for a 100x100 grid, this means we would have to sum over $2^10000$ vectors!

\point{Example of query:} what is $\P(X_{1,1} = +1) = \E[\1[X_{1,1} = +1]]$. Here the ``test function'' is $g(x) = \1[x_{1,1} = +1]$. 

\point{Idea:} 
\begin{enumerate}
  \item Build/simulate a Markov chain $X^{(1)}, X^{(2)}, \dots$ where $X^{(t)}$ is a vector $X^{(t)} = (X^{(t)}_1, \dots, X^{(t)}_9)$ taking values in $\states$, and such that the stationary distribution is equal to Equation~(\ref{eq:target}).
  \item Use the Law of large numbers for Markov chain!
\end{enumerate}

\point{Challenge:} how to create a Markov chain with a prescribed stationary distribution? We will cover two methods (in the analysis, we will reveal that the first is actually a special case of the second):
\begin{enumerate}
  \item Gibbs sampling.
  \item Metropolis-Hastings (MH) algorithms.
\end{enumerate}


\subsection{Gibbs sampling}

\point{Gibbs algorithm:}
\begin{enumerate}
  \item Initialize the 3x3 grid $x^{(0)}$ arbitrarily.
  \item Loop $i = 1, 2, 3, \dots, N$ (until enough samples are produced):
  \begin{enumerate}
     \item $x^{(i)} \gets$ copy of $x^{(i-1)}$
     \item Sample one of the 9 variable indices uniformly, $(i^*, j^*) \sim \text{Uni}((1,1), (1,2), \dots, (3,3))$ \label{line:sampling-variable}
     \item Sample a new value $x'_{i^*,j^*}\in \{-1,+1\}$ for $X^{(i)}_{i^*, j^*}$ by sampling from:  \begin{equation}\label{eq:conditional-Gibbs} \P(X_{i^*,j^*} = x'_{i^*,j^*} | X_{i,j} = x^{(i)}_{i,j}\text{ for all }(i,j) \neq (i^*,j^*)). \end{equation}
  \end{enumerate}
  \item Estimate the expectation(s) of interest using: \[ \frac{1}{N} \sum_{i=1}^N g(x^{(i)}). \]
\end{enumerate}

\point{Note:} by Bayes rule, Equation~(\ref{eq:conditional-Gibbs}) is proportional to $\pi(x') \1[x' \in N(x^{(i-1)})]$, where $N(x) = N_{i^*,j^*}(x)$ denotes the set of configurations $x'$ in $\states$ that can be reached from $x$ by changing only variable $i^*,j^*$. Formally: $N(x) = \{y \in \states : x_{i,j} = y_{i,j}\text{ for all }(i,j) \neq (i^*,j^*)\}$.

\point{Exercise:} compute Equation~(\ref{eq:conditional-Gibbs}) for the Ising example.

\point{Analysis:} we will now analyze the behavior of this algorithm as a Markov chain with transitions denoted by $p$.
\begin{itemize}
  \item State space: $\states$. Too large to build the transition matrix $M_{x,y} = p(x \to y)$ explicitly! But note that we do not have to if we just want to simulate. Key: each row is sparse. Why?
  \item Simplification: to start with assume we are always picking a fixed node, say $(2,2)$, in step \ref{line:sampling-variable} of the Gibbs algorithm, instead of picking it from a uniform distribution. We will relax this simplifying assumption soon.
  \item Under this simplification, the form of $p(y \to x)$ is just:
\[ p(y \to x) = \frac{ \pi(x) \1[x \in N(y)]}{\sum_{x' \in N(y)} \pi(x')}. \]
  \item Goal: to show that $p(x \to y)$ satisfies the stationary equation, i.e. that if $p$ is as the previous bullet point and $\pi$ as specified by our target distribution, Equation~(\ref{eq:target}), then we have $\pi(x) = \sum_{y\in\states} \pi(y) p(y \to x).$

\end{itemize}
Now we have:
\begin{eqnarray*}
\sum_{y\in\states} \pi(y) p(y \to x) &=& \sum_{y\in\states}  \pi(y) \frac{ \pi(x) \1[x \in N(y)]}{\sum_{x' \in N(y)} \pi(x')} \\
&=& \pi(x) \sum_{y\in\states} \pi(y) \frac{  \1[x \in N(y)]}{\sum_{x' \in N(y)} \pi(x')} \\
&=& \pi(x) \frac{ \sum_{y\in\states} \pi(y) \1[y \in N(x)]}{\sum_{x' \in N(y)} \pi(x')}\;\;(\text{Note that }y\in N(x) \Leftrightarrow x\in N(y)) \\
&=& \pi(x).
\end{eqnarray*}


\subsection{Metropolis-Hastings (MH) algorithms}

\point{Limitations of Gibbs:} it may be difficult to sample from Equation~(\ref{eq:conditional-Gibbs}) in certain problems.

\point{MH inputs:}
\begin{enumerate}
  \item A target distribution $\pi$ that we can evaluate pointwise. 
  \item A \emph{proposal distribution/transition} $q(x \to y)$ from which we can simulate $q(x \to \cdot)$ and evaluate pointwise.
  \item A test function $g$.
\end{enumerate}

\point{MH algorithm:}
\begin{enumerate}
  \item Initialize:
  \begin{enumerate}
     \item $x^{(0)}$ arbitrarily,
     \item $F \gets 0$
  \end{enumerate}
  \item Loop $i = 1, 2, 3, \dots, N$ (until enough samples are produced):
  \begin{enumerate}
     \item Propose a new state, $x' \sim q(x^{(i-1)} \to \cdot)$
     \item Compute: \[ A(x^{(i-1)} \to x') = \min\left\{ 1, \frac{\pi(x') q(x' \to x^{(i-1)})}{\pi(x^{(i-1)}) q(x^{(i-1)} \to x')} \right\}. \]
     \item Let $A^{(i)} \sim \text{Bern}(A(x^{(i-1)} \to x'))$
     \begin{enumerate}
       \item If $A^{(i)} = 1$, then $x^{(i)} \gets x'$
       \item If $A^{(i)} = 0$, then $x^{(i)} \gets x^{(i-1)}$
     \end{enumerate}
     \item $F \gets F + f(x^{(i)})$
  \end{enumerate}
  \item Return $F/N$
\end{enumerate}

\point{Note:} the density $\pi$ always appears in a ratio in the MH algorithm, therefore we do not need to know the normalization constant $Z$:
\[ \frac{\pi(x')}{\pi(x)} = \frac{\gamma(x')/Z}{\gamma(x)/Z} = \frac{\gamma(x')}{\gamma(x)}. \]

\point{Practical note:} it is often preferable to compute the numerator and denominator in log-scale and exponentiate only after taking the ratio.

\point{Special cases:} 
\begin{itemize}
  \item When $q$ is symmetric (for example, an isotropic normal), the $q$'s cancel out in the ratio. 
  \item When $q(x \to x')$ is independent of $x$, the algorithm is called an independence chain. Note however that the behavior of the algorithm is still dependent on the previous state because of the accept reject step.
\end{itemize}

\point{Analysis:} we will now analyze the behavior of this algorithm as a Markov chain with transitions denoted by $p$.

\point{Assume:} first that $x \neq y$. What is $p$? To move from $x$ to $y$, the chain needs to propose $y$, and then accept it:
\[ p(x \to y) = q(x \to y) A(x \to y). \] 

\point{Note:} make sure you understand the difference between the proposal $q$ and the Markov chain $p$ used to analyze the algorithm.

\point{Lemma:} detailed balance, $\pi(x) p(x \to y) = \pi(y) p(y \to x)$ for all $x,y\in\states$ implies global balance, $\pi(x) = \sum_{y\in\states} p(y \to x) \pi(y)$. 

\point{Proof of lemma:} sum over $y$ on both sides of the detailed balanced equation.

\point{Proof of MH $\pi$-invariance:} if $x \neq x'$, we have
\begin{eqnarray*}
\pi(x) p(x \to x') &=& \pi(x) q(x \to x') A(x \to x') \\
&=& \min\{ \pi(x) q(x \to x'), \pi(x') q(x' \to x) \} \\
&=& \min\{ \pi(x') q(x' \to x), \pi(x) q(x \to x')\} \\
&=& \pi(x') q(x' \to x) A(x' \to x) \\
&=& \pi(x') p(x' \to x).
\end{eqnarray*}
Finally, if $x = x'$, the result holds by inspection.

\point{Exercise:} show that if $q$ is a conditional distribution of the target distribution, the acceptance ratio is one. Conclude that the Gibbs sampler is a special case of the MH algorithm.


\subsection{Irreducibility of MCMC algorithms}

Several of the samplers we have defined so far (in particular, the Gibbs sampler) satisfy the global balance equation (i.e. are $\pi$-invariant), but they do not have a LLN. Why? Because they are not irreducible. Fortunately, it is easy to restore irreducibility. This is done via combinations of MCMC kernels.

\point{Combination of MCMC kernels.} Let us denote a collection of $\pi$-invariant kernels by $p_1, p_2, \dots, p_L$. For example in the Gibbs sampler over a $M$-by-$M$ grid, we would have one of these kernels for each of the $L = M^2$ nodes. We can combine them using the following methods:
\begin{description}
  \item[Mixture:] where at each step we first pick one of the $L$ kernels and do one MCMC iteration with it. Formally, this create a MCMC kernel given by \[ p_\text{mix}(x\to x') = \sum_l \frac{1}{L} p_l(x \to x'). \]Non-uniform distributions over the $L$ kernels could also be used.
  \item[Alternation:] apply the first kernel, then the second one, then the third one, .., the $L$-th one, and loop back to the first one. Formally, this create a MCMC kernel given by \[ p_\textrm{alt}(x \to x') = \sum_{x_1\in\states} \sum_{x_2\in\states} \cdots \sum_{x_{l-1}\in\states} p_1(x \to x_1) p_2(x_1 \to x_2) \cdots p_L(x_{l-1} \to x'). \]
  \item[Randomized alternation:] first, sample a permutation of $\{1, 2, \dots, L\}$, second, do one round over all kernels in the order specified by the first step.
\end{description} 

\point{Proposition:} if each kernel $p_l$ is $\pi$-invariant, then the three combinations described above are also $\pi$-invariant.  

\point{Proof for the mixture:}
\begin{eqnarray}
\sum_y \pi(y) p_\textrm{mix}(y \to x) &=& \sum_y \pi(y) \sum_l \frac{1}{L} p_l(y \to x) \\
&=& \sum_l \frac{1}{L} \sum_y \pi(y) p_l(y \to x) \\
&=& \sum_l \frac{1}{L} \pi(x) \\
&=& \pi(x).
\end{eqnarray}

\point{Exercise:} prove that the other combination schemes are also $\pi$-invariant. 

\point{Exercise:} conclude that the Ising Gibbs sampler is irreducible and hence that the LLN holds. 

%\point{Goal:} to show that 


% we define the $\L_2$ norm of a random variable by $\|X\| = \E[X^2]$ (exercise: show that this definition satisfies all the axioms of a norm).

 

% BACK TO WHERE YOU WERE/BEGINNING OF ZOO

% TODO: write it

% TODO: tonelli

% NEXT: TAIL SUMS

% consider a random variable $X$ with the following probability mass function $p_X(x) \defeq \P(X = x)$:
% \\
% \begin{tabular}{ll}
% \hline
% $x$ & $p_X(x)$ \\
% \hline
% -2 & 1/6 \\
% -1 & 1/6 \\
% 0  & 1/3 \\
% +1 & 1/6 \\
% +2 & 1/6 \\
% \hline
% \end{tabular}
% Now let $Y = X^2$, and suppose we want to compute $\E[Y]$. 
% 
% \point{Naive way:} 


% start with simple discrete example and g() = x^2, then prove

% then, introduce density


% need more on expectation --- PLAN: come back via assignment and another section after indep; e.g. will need Fubini for tail sums
% - change of vars
% - actual computations
% - indicator tricks
% - tail sums


% - philosophy: \Omega complicated, not accessible
% - todo: closure of r.v. under limits
% - add note that symbols are clickeable
% - todo: double-check statements of monotone/dominated does not require existence of lim 



\bibliographystyle{plain}
\bibliography{ref}


\end{document}
