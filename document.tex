\RequirePackage{etoolbox}
\csdef{input@path}{%
 {sty/}% 
 {figures/}%
}%

\documentclass{article}

\title{Notes for 547C (draft)}
\author{Alexandre Bouchard-C\^ot\'e}

\input{macros/typesetting-macros}
\input{macros/basic-math-macros}
\input{macros/notation}

\newcommand{\point}[1]{\vspace{1ex}\noindent{\bf #1}}

% use to create table of symbols
\makenomenclature

\begin{document}

\maketitle
\tableofcontents
 

\section{Foundations}


\subsection{Pre-requisites}

Some notions to review (come to office hours if you need help on these):
\begin{enumerate}
  \item Set theoretic notation such as power sets, countable vs. uncountable set, functions.
  \item Basic properties of limits of real numbers.
\end{enumerate}


\subsection{The axioms of probability}

A probability space contains three things:
\begin{enumerate}
  \item a set $\Omega$, called the sample space,
  \item a closed collection of events, $\events \subset 2^\Omega$, called a \sigmaalg,
  \item a probability measure (synonym: probability distribution), $\P : \events \to [0, 1]$.
\end{enumerate}
where:
\begin{enumerate}
  \item the \sigmaalg\ satisfies:
  \begin{enumerate}
    \item $\Omega \in \events$,
    \item $A_1 \in \events, A_2 \in \events, A_3 \in \events, \dots, \Longrightarrow \cap_{i=1}^\infty A_i \in \events$,
    \item $A \in \events \Longrightarrow A^\complement \in \events$.
  \end{enumerate} 
  \item and the probability measure satistifies:
  \begin{enumerate}
    \item $\P(\Omega) = 1$,
    \item if $A_1 \in \events, A_2 \in \events, A_3 \in \events, \dots$ are disjoint ($i\neq j \Longrightarrow A_i \cap A_j = \emptyset$), then 
\[ \P(\cup A_i) = \sum_{i=1}^{\infty} \P(A_i). \]
  \end{enumerate}
\end{enumerate}

In the first lecture, we discussed why the axioms were constructed in this fashion, using the simple example of ``train'' starting at 0 and moving at each hour by +1 with probability 1/3 and -1 with probability 2/3. These notes will be added shortly.


\subsection{Some basic properties}

Some examples of basic properties we can derive from these axioms:
\begin{enumerate}
  \item $\P(A^\complement) = 1 - \P(A)$,
  \item if $A$ and $B$ are events that are not necessarily disjoint, $\P(A \cup B) = \P(A) + \P(B) - \P(A \cap B)$.
\end{enumerate}


\subsection{Probability spaces as models}

\subsubsection{Coin tosses}

\point{Question:} you toss two coins, what is the probability of two heads? Note that you are not able to tell the two coins apart.

\point{Sample space:} which of these should we pick?
\begin{enumerate}
  \item $\Omega_1 = \{(H, H), (H, T), (T, H), (T, T)\}$
  \item $\Omega_2 = \{\{H, H\}, \{H, T\}, \{T, T\}\}$. 
\end{enumerate}

The best answer is $\Omega_1$, but why? Choosing between these two (essentially, selecting one of these two models) is not part of probability theory per se. Use have to use your intuition about the real world here. For example, note that if you painted one coin red and one blue, the setup of this experiment would not have changed. Hence, the model that is most useful is the one that uses lists even though we could not observe this distinction. Probability theory comes in once we have built a model, at which point inference can be carried using mathematical principles. Probability theory can help selecting model though, for example by making certain predictions for a given model, which can then be tested (for example, the long term behavior of frequencies, which is mathematically understood for a wide range of probability models).


\subsubsection{Reliability}

Reliable systems replicate a critical component (e.g. a power supply in a computer server) so that the whole system works as long as at least one of the two copies works. Consider an assembly line for computer servers. Suppose the first power supply assembly line is observed to put in a power supply that works $60\%$ of the time. The second  power supply assembly line  is observed to put in a power supply that works $70\%$ of the time. At delivery, both power supplies work $40\%$ of the time. What is the probability that both power supplies are broken at delivery?

If you answered $12\%$, you are using a property that is \emph{not} built into (or derivable from) the axioms of probability: namely that if $\P(A \cap B) = \P(A) \P(B)$. It is an extra assumption called \emph{independence} of the events $A$ and $B$. It is kept separate because there are situation where it is useful to describe the world, and others where it is not (can you imagine a scenario where the two assembly lines are not independent?). This contrast with the disjoint union axiom of probability, which models a universal aspect of reality. 


\subsection{Simple examples of $\sigma$-algebra}

\begin{description}
  \item[Power set:] the power set $2^\Omega$ is always a $\sigma$-algebras on $\Omega$. For example \[\events_0 \defeq \{\{a, b, c\}, \{a, b\}, \{a, c\}, \{b, c\}, \{a\}, \{b\}, \{c\}, \emptyset\}\] is a $\sigma$-algebras on $\Omega \defeq \{a, b, c\}$.
  \item[Another discrete example:] the collection of events \[\events_1 \defeq \{\{a, b, c\}, \{a, b\}, \{c\}, \emptyset\}\] is also a \sigmaalg\ on $\Omega \defeq \{a, b, c\}$.
  \item[A non-example:] the collection $\events_2 \defeq \events_1 \cup \{\{a\}\}$ is not a \sigmaalg. Why? We have $\{a\} \in \events_2$ yet $\{a\}^\complement \notin \events_2$.
\end{description}

\point{End of lecture 1 (Sep 9)}


\subsection{Generated $\sigma$-algebra}\label{sec:generated}

Let $\generators$ denote a collection of events. The machinery of this section is useful when the collection $\generators$ is ``broken,'' i.e. when it is not  a \sigmaalg\ (example: $\generators \defeq \events_2$ above). 

We would like to ``repair'' $\generators$ by adding more events until we get a closed collection of event. How can we do this is such a way that the output of the repair process is unique and well-defined? 

\begin{enumerate}
  \item \label{item:inter-sigma} Let $\events$ and $\events'$ be two \sigmaalg\ on $\Omega$. \point{Exercise:} convince yourself that their intersection $\events \cap \events'$ is also a \sigmaalg.
  \item The result in \ref{item:inter-sigma} can be generalized: if we have any collection of $\sigma$-algebras $\{\events_\alpha : \alpha \in I\}$, where $I$ is some index set (not necessarily countable), then \[ \bigcap_{\alpha \in I} \events_\alpha \] is also a \sigmaalg.
  \item Let us pick \[\{\events_\alpha : \alpha \in I\} = \{\events_\alpha : \events_\alpha \text{ is a }\sigma\text{-algebra containing }\generators\},\] then we get that the intersection of the $\events_\alpha$ is a \sigmaalg.
  \item We call this intersection the \sigmaalg\ \emph{generated} by $\generators$, denoted $\sigma(\generators)$. 
  \item Another way to think about $\sigma(\generators)$: the \emph{smallest \sigmaalg\ containing $\generators$}.
\end{enumerate}

\point{Some examples:}
\begin{enumerate}
  \item $\sigma(\events_2) = 2^{\{a,b,c\}}$.
  \item An important example where the generated \sigmaalg\ is smaller than $2^\Omega$: \emph{the Borel \sigmaalg}.
  \begin{enumerate}
    \item Take $\Omega \defeq [0, 1)$, and $\generators_\text{B} \defeq \{F : F\text{ is a finite collection of intervals}\}$.
    \item Convince yourself that $\generators$ is not a \sigmaalg.
    \item We call $\sigma(\generators_\text{B})$ the \emph{Borel \sigmaalg}, denoted $\borels$.
    \item Elements of $\borels$ are called Borel events.
    \item Some measure theory shows that $\borels$ is a proper subset of $2^{[0, 1)}$.
  \end{enumerate}
\end{enumerate}


\subsection{Simple examples of probability measures}

\begin{description}
  \item[Discrete probability measure:] assume you are given a countable sample space $\Omega$ and a function $p: \Omega \to [0, 1]$ called a \PMF. Define, for any event $A \in \events$: \[ \P(A) \defeq \sum_{\omega \in A} p(a). \] Check that this definition satisfies the axioms of probability. Note: do not confuse $p$ and $\P$, as they take different types of inputs!
  \item[The uniform probability measure:] some (surprisingly heavy) measure theory shows that there exists a probability measure on $\borels$ such that $\P([a, b)) = b - a$ for all $0 \le a \le b < 1$.
\end{description}

This second example is actually enough to build a rich theory! How? Using random variables and their distributions, our next topic.


\subsection{Random variables}

\point{Informal definition:}
\begin{itemize}
  \item A random variable if often used to encode a measurement, for example, in the previous 1D train example, whether the train goes left or right at time step 3.
  \item Random variables are used to model partial observability of the world. In contrast to each outcome $\omega \in \Omega$, which contains all the information possible within the model, a random variable can be defined to forget important information. For example, you may only known the position of the train at time steps 1 and 6, and what is in between is unknown.
  \item Another use of random variables is to express a quantity that is unknown, but that we would like to have the probability of. A concept that I call a ``query.''
\end{itemize}

\point{Formal definition:}
\begin{enumerate}
  \item Let $(\Omega, \events, \P)$ denote a probability space,
  \item then a (real) random variable $X$ is:
  \begin{enumerate}
    \item a map, $X : \Omega \to \R$,\footnote{Technically, we add two points to the real line, $+\infty$ and $-\infty$, to ensure for example that limits of say increasing random variables are guaranteed to be random variables even if the sequence diverges for some outcomes $\omega$.} 
    \item \label{rv:condition} such that $X^{-1}(A) \in \events$ for all $A \in \borels$.
  \end{enumerate}
\end{enumerate}

\point{Why} do we need condition \ref{rv:condition}? Often, we will ask questions like ``what states of the worlds can yield an observation in $A$?'' In set theory, the set of such states (outcomes) is $X^{-1}(A)$. Now, we want to be able to compute the probability of this set of outcomes, so we require that $X^{-1}(A)$ be an event (i.e. in the \sigmaalg).

\point{Note:} random variables are especially interesting when we define more than one on the same sample space. 

\point{Synonym:} ``$X$ is a measurable function.''

\point{Notes:}
\begin{itemize}
  \item Observations are not always real numbers, for example, they could be colours, the nodes in a discrete graph, or even a graph. Let $\states$ denotes the set of say all colours that could be potentially observed, $\states = \{$blue, red, yellow$\}$. We can modify our definition above to get colour-valued random variables (terminology: ``random colours''). This is done as follows:
  \begin{itemize}
    \item In the definition of real random variable, replace ``$\R$'' by ``$\states$.''
    \item Since we need a collection of sets on $\states$ in \ref{rv:condition}, let us assume we have a (second) \sigmaalg\ $\events_\states$ on $\states$ as well as on $\Omega$.
    \item Therefore, we see that all we really need is a \sigmaalg\ on the input space $\Omega$, and one on the output space \sigmaalg. Notation: a random colour is a $(\events \to \events_\states)$-measurable function.
  \end{itemize}
  \item Using this idea, we will later define random vectors (vector-valued random variable), random graphs, random sets, random functions, and even random probability measures!
  \item (Real) random variables and random vectors are special though, as they will later allow us to define the notion of expectation (not possible with colours, as we cannot for example ``add colours''). More on this later!
\end{itemize}

\point{Warning:} make sure you follow the type conventions! For example, $\P($red$)$ or $\P(X)$ are not defined!\footnote{Some authors do give it a meaning, but it is not what you think! We will see this meaning, namely the expectation of $X$, later.}


\subsection{Compositions of random variables}\label{sec:composition-of-rv}

\point{Recall:} $g \circ h$ means a new function that first use $g$, then $h$: $g \circ h(x) = g(h(x))$. 

\point{Convention:} use capital letters only for the random variables mapping elements from the main global sample space $\Omega \to \states$. Use standard function notation ($g, h$, etc) for subsequent transformations $g : \states \to \states'$  of the output of $X$. E.g. $g \circ X = g(X)$. 

\point{Exercise:} show that the composition $g(X)$ of two random variables is a random variable.


\subsection{The graph of a random variable}

In the special case where $\Omega = \R$, we can plot the \emph{graph} real random variables $X : \Omega \to \R$. While in practice $\Omega$ is usually much more complicated, looking at simple examples where $\Omega = \R$ is a powerful technique to get some intuition on several results we will cover in the next few weeks. Note that measurability is much weaker than continuity, so the the graph of random variables can be quite pathological in general. 


\subsection{The probabilist's event notation}

It is tedious to write expressions like $X^{-1}(\{r \in \R : 6 \le r\})$. Probabilists noticed that the notation $(6 \le X)$ was not defined, and decided to give it a new, precise meaning: $X^{-1}(\{r \in \R : 6 \le r\})$. Some other examples:
\begin{itemize}
  \item $(X \in A) \defeq X^{-1}(A)$,
  \item $(X = x) \defeq X^{-1}(\{x\})$.
\end{itemize}

\point{More generally:} \[(\text{logical statement }s\text{ containing a random variable }X) \defeq \{\omega \in \Omega : s(X(\omega))\text{ is true}\}. \]


\subsection{Indicator random variables}

For an event $A$, define the \emph{indicator function} as follows: \[ \1_A(\omega) \defeq \1[\omega \in A] \defeq \bracearraycond{1 &\text{if }\omega\in A\\0&\text{otherwise.}} \]


\subsection{Distribution of a random variable}

\point{Idea:} you give me a probability space $(\Omega, \events, \P)$ and a random variable $X : \Omega \to \states$, and I create a new probability $\P_2$! This new probability is defined on the values $\states$ that the random variable takes.

\point{Definition:} for any $A \in \events_\states$, set $\P(A) \defeq \P(X \in A)$.

\point{Notation:} $\P_X$.

\point{Possible confusion:} ``probability distribution'' is a synonym of ``probability measure.'' Here we defined the ``distribution \emph{of} $X$,'' which is a specific way of constructing a probability measure.


\subsection{Cumulative distribution function}

\point{Idea:} a probability is a function taking inputs from a tricky space ($\events$). This makes it hard to plot naively. It would be nice to be able to summarize it with a function taking inputs in a more familiar space, $\R$.

\point{Note:} the following definition only works for $\states = \R$.

\point{Definition:} the \CDF\ of a random variable is given for all $x \in \R$ by: \[ F_X(x) \defeq \P(X \le x). \]

\point{Exercise:} this is the same as $\P_X((-\infty, x])$.

\point{End of lecture 2 (Sep 14)}


\subsection{Using random variables to construct new probability measures}\label{sec:using-rv-to-construct-prs}

\point{Example 1:} Suppose the only probability distribution available in some programming language is the uniform distribution on $[0, 1)$. How can we transform it into a coin toss?

\point{An answer:} $X_1 = \1_{[0, 1/2)}$.

\point{Question:} is this answer unique? No! For example, $X_2 = \1_{[0, 1/4)} + \1_{[1/2, 3/4)}$ will also do!

\point{Note:} 
\begin{enumerate}
  \item $X_1 \neq X_2$ (the two functions are not equal, for example $X_1(1/4) = 1 \neq 0 = X_2(1/4)$
  \item \label{item:dist} but: $\P_{X_1} = \P_{X_2}$.
\end{enumerate}

\point{Definition:} We call \ref{item:dist} \emph{equality in distribution}, denoted $X_1 \deq X_2$. 

\point{Example 2/exercise:} simulate an exponential random variable (defined below) from a uniform distribution on $[0, 1)$. 

\point{Definition:} we say $X$ is a standard exponential random variable, a statement denoted $X \sim \text{Exp}(\lambda)$, if \[F_X(x) = \1[x \ge 0] (1 - e^{-x \lambda}). \]


\subsection{Densities (first definition)}

A random variable is said to have \emph{density} $f$ if: \[ F_X(x) = \int_{-\infty}^x f(z) \ud z. \] Note that we will cover a more general definition of density later in this course.

\point{Example:} a density for the exponential distribution is given by \[ f(x) = \1[x \ge 0] e^{-x\lambda}. \]

\point{Exercise:} find an example where there is a $x$ such that a density has $f(x) > 1$.


\subsection{Limit properties of probability measures}

\point{Lemma: monotonicity.} If $A \subset B$ are events, then $\P(A) \le \P(B)$.

\point{Proof idea:} Use the ``donut decomposition,'' $B = A \sqcup (B \backslash A)$, where we use the symbol $\sqcup$ to denote a union while asserting that the two sets we are taking the union over are disjoint.

\point{Notation:} to express the following limit properties, we make use of the following overloaded notations,
\begin{itemize}
  \item Monotone real numbers limits:
  \begin{itemize}
    \item If $r_1 \le r_2 \le \dots$, and $\lim r_i = r$, we write $r_i \uparrow r$,
    \item If $r_1 \ge r_2 \ge \dots$, and $\lim r_i = r$, we write $r_i \downarrow r$.
  \end{itemize}
  \item Monotone set limits:
  \begin{itemize}
    \item If $A_1 \subset A_2 \subset \cdots$, and $\cup A_i = A$, we write $A_i \uparrow A$,
    \item If $A_1 \supset A_2 \supset \cdots$, and $\cap A_i = A$, we write $A_i \downarrow A$.
  \end{itemize}
\end{itemize}

\point{Monotonicity of probability measures:} 
\begin{itemize}
  \item $A_i \uparrow A \Longrightarrow \P(A_i) \uparrow \P(A)$,
  \item $A_i \downarrow A \Longrightarrow \P(A_i) \downarrow \P(A)$.
\end{itemize}

\point{Proof idea for the increasing case:} generalize the donut decomposition and write 
\[ A = A_1 \sqcup (A_2 \backslash A_1) \sqcup (A_3 \backslash A_2) \sqcup \cdots, \]
then use countable additivity to get:
\[ \P(A) = \P(A_1) + \lim_{n \to \infty} \sum_{i=1}^n \P(A_{i+1} \backslash A_i), \]
using our previous monotonicity property, and telescoping the sum inside the limit, we get:
\[ \P(A) = \P(A_1) + \lim_{n \to \infty} [\P(A_n) - \P(A_1)] = \lim_{n\to\infty} \P(A_n). \]


\subsection{Limit properties of CDFs}\label{sec:limit-properties-cdfs}

Since the \CDF\ is derived from a probability measure, it shares similar continuity property. But being a function from the real to $[0,1]$, these monotonicity properties coincide with familiar notions from elementary real analysis. 

\point{Notation:} Throughout this section, $F$ denotes the \CDF\ of some random variable $X$, $F \defeq F_X$.

\point{Monotonicity of \CDFs:} $x \le y \Longrightarrow F(x) \le F(y)$ ($F$ is monotone increasing).

\point{Proof:} $x \le y \Longrightarrow (X \le x) \subset (X \le y)$, so we can use monotonicity of the probability measure $\P$ to conclude the proof.

\point{Semi-continuity property:} 
\begin{enumerate}
  \item \label{item:cont1} $x_i \uparrow x \Longrightarrow$ the limit $\lim F(x_i)$ exists,
  \item \label{item:cont2} $x_i \downarrow x \Longrightarrow F(x_i) \downarrow F(x)$.
\end{enumerate}

\point{Proof idea for the decreasing case:} we have $(X \le x_i) \supset (X \le x_{i+1})$, so by monotonicity of $\P$, we get
\[ \P(X \le x_i) \downarrow \P(\cap A_i), \]
where $\cap A_i = (X \le x)$, therefore $\P(\cap A_i) = F(x)$.

\point{Reason:} for the asymmetry between semi-continuity property \ref{item:cont1} and \ref{item:cont2}. First, do the proof for the increasing case as an exercise. You will see that in the increasing case, the limit of the probabilities is given by $\P(X < x)$, which is not guaranteed in general to be equal to $F(x)$ (because of a potential point mass as $x$).

\point{Terminology:} functions that satisfy the semi-continuity properties (\ref{item:cont1} and \ref{item:cont2}) are called cadlag, coming from ``continue \`a droite, limite \`a gauche'' (French for ``continuous to the right, limits to the left'').

\point{Proposition:} let $F : \R \to [0, 1]$ be some function. The following are equivalent:
\begin{enumerate}
  \item \label{item:inv-cdf-eq1} The function $F$ is cadlag and satisfies the boundary conditions $\lim_{x \to -\infty} F(x) = 0$ and $\lim_{x \to +\infty} F(x) = 1$ (denoted $F(-\infty) = 0$ and $F(+\infty) = 1$),
  \item \label{item:inv-cdf-eq2} There is some random variable $X$ with $F_X = F$.
\end{enumerate}

\point{Proof idea:} we proved the main steps of \ref{item:inv-cdf-eq1}$\Longleftarrow$\ref{item:inv-cdf-eq2}. The main idea for the other direction is to use the following construction:
\[ X(\omega) \defeq \sup\{x : F(x) < \omega\}. \]

\point{Note:} this is a first instance of an important idea in this course, namely to identify the distributions of random variables with simpler types of functions.


\subsection{Measure}

A measure $\mu : \events \to [0, \infty)$ is defined in the same way as a probability measures, except that we remove the requirement that $\mu(\Omega) = 1$. Instead, we just ask that $\mu(\emptyset) = 0$.

\point{Example:} the Lebesgue measure is the same as the uniform probability distribution, except that it is defined on $\R$ instead of $[0, 1)$. 


\subsection{Exercises}

\begin{enumerate}
  \item Let $X$ be a random variable with a uniform distribution on $[0, 1)$. Draw a possible graph of $X$, a density for $X$, and the \CDF\ of $X$.
  \item Solve the exercise in Section~\ref{sec:generated}.
  \item Solve the exercise in Section~\ref{sec:composition-of-rv}.
  \item Solve the exercise in Section~\ref{sec:using-rv-to-construct-prs}.
  \item If $X_1 \sim F$ (a notation that means that $\P(X_1 \le x) = F(x)$), and $X_1 \ge 0$, find the \CDF\ of $X_2 \defeq X_1^2$.
\end{enumerate}


\subsection{Solutions}

\begin{enumerate}
  \item The three pictures should be as follows:
  \begin{enumerate}
    \item Graph of $X$: The $x$-axis should be a bounded segment labelled $\Omega$. The $y$-axis should be the full real line. There are several choices for the function. For example the line $y = x$ or $y = 1 - x$ on the interval $[0, 1)$ are acceptable. Other choices are possible.
    \item CDF: The $x$-axis should be the full real line. The $y$ axis should be the interval $[0, 1]$. The function should be zero in $(-\infty, 0]$, then affine in $[0, 1]$, then one in $[1, +\infty)$.
    \item Density: the $x$ axis should be the full real line. The $y$ axis should be the positive real line. The function should be the indicator on the set $[0, 1)$.
  \end{enumerate}
  \item We need to check the three conditions given in the definition of \sigmaalg:
  \begin{enumerate}
    \item Since $\events$ is a \sigmaalg, $\Omega \in \events$ and since $\events'$ is a \sigmaalg, $\Omega \in \events'$, therefore $\Omega \in \events \cap \events'$.
    \item We need to show that if $A_1, A_2, \dots$ are all in $\events \cap \events'$, then $\cap A_i \in \events \cap \events'$. By the definition of intersection, we have that $A_1, A_2, \dots$ are all in $\events$. Since $\events$ is a \sigmaalg, it follows that $\cap A_i \in \events$. By the same reasoning, $\cap A_i \in \events'$. Therefore, $\cap A_i \in \events \cap \events'$.
    \item We need to show that if $A \in \events \cap \events'$, then $A^\complement \in \events \cap \events'$. By the definition of intersection, we have that $A \in \events$. Since $\events$ is a \sigmaalg, it follows that $A^\complement \in \events$. By the same reasoning, $A^\complement \in \events'$. Therefore, $A^\complement \in \events \cap \events'$.
  \end{enumerate}
  \item We have that $X: \Omega \to \states$ and $g : \states \to \states'$ are random variables. Let us denote by $\events_\Omega, \events_{\states}$ and $\events_{\states'}$ the \sigmaalg\ on $\Omega$, $\states$ and $\states'$ respectively. Let $A \in \events_{\states'}$. We have to show that $(g \circ X)^{-1}(A) \in \events_\Omega$. First, note that $(g \circ X)^{-1}(A) = X^{-1}(g^{-1}(A))$. Since $g$ is a random variable, $g^{-1}(A) \in \events_\states$. Next, since $X$ is a random variables, $g^{-1}(A) \in \events_\states$ implies that $X^{-1}(g^{-1}(A)) \in \events_\Omega$.
   \item As in Section~\ref{sec:limit-properties-cdfs}, we pick $X(\omega) \defeq \sup\{x : F(x) < \omega\}$ (note that in this special case were $\Omega = [0, 1)$,  writing ``$F(x) < \omega$'' is well defined---for general $\Omega$ is would not).
This construction has some nice properties:
\begin{enumerate}
  \item \label{point:inv-cdf-monotone} $X$ is monotone increasing (this follows directly from the definition),
  \item \label{point:inv-cdf-incl-one} $X\circ F(x) \le x$ (draw a picture of a \CDF\ having flat regions to convince yourself),
  \item \label{point:inv-cdf-incl-two}$F\circ X(\omega) \ge \omega$ (draw a picture of a \CDF\ having discontinuities to convince yourself).
\end{enumerate}
We first use properties \ref{point:inv-cdf-incl-one} and \ref{point:inv-cdf-incl-two} to prove the following set equality:
\[ (X \le x) = \{\omega \in \Omega : \omega \le F(x) \}.\]
We show that the LHS includes the RHS and vice versa:
\begin{itemize}
  \item $\{\omega \in \Omega : \omega \le F(x) \} \subseteq (X \le x)$:
\begin{eqnarray*}
\omega \le F(x) &\Longrightarrow& X(\omega)  \le X\circ F(x)\;\text{ (from \ref{point:inv-cdf-monotone})} \\
&\Longrightarrow& X(\omega) \le x\;\text{(from \ref{point:inv-cdf-incl-one}).}
\end{eqnarray*}
  \item $(X \le x) \subseteq \{\omega \in \Omega : \omega \le F(x) \}$:
\begin{eqnarray*}
X(\omega) \le x &\Longrightarrow& F\circ X(\omega)  \le F(x)\;\text{ (monotonicity of CDFs)} \\
&\Longrightarrow& \omega \le F(x)\;\text{(from \ref{point:inv-cdf-incl-two}).}
\end{eqnarray*}
\end{itemize}
Finally, it follows that:
\begin{eqnarray*}
\text{CDF of }X &\defeq& \P(X \le x) \\
&=& \P\{\omega \in \Omega : \omega \le F(x) \} \\
&=& F(x) \;\text{ (by the definition of uniform probability).}
\end{eqnarray*}
Here in this special case: $X(\omega) = - \lambda^{-1} \log(1 - \omega)$.
\item We have:
\begin{eqnarray*}
F_{X_2}(x) &\defeq& \P(X_2 \le x) \\
&=& \P(X_1^2 \le x) \\
&=& \P(-\sqrt{x} \le X_1 \le \sqrt{x}) \\
&=& \P(X_1 \le \sqrt{x}) \;\text{(Non-negativity assumption)} \\
&=& F(\sqrt{x}).
\end{eqnarray*}
\end{enumerate}


\section{Integration and expectation}

\subsection{Overview}

\point{How to define the mean?} At an undergraduate level, this is usually done as follows for continuous random variables:
\[ \E[X] \defeq \int_{-\infty}^{+\infty} x f(x) \ud x, \]
where $f$ is the density of $X$. There are two limitations with this definition:
\begin{enumerate}
  \item it is not very intuitive (why multiply the density with an $x$?),
  \item we need a separate definition for discrete random variable (and we also need to think about mixtures of continuous and discrete?).
\end{enumerate}

\point{Better definition:} the expectation is the area under the graph of $X$!

\point{Note:} we will need to generalize the notion of ``area,'' to cover cases where $\Omega \neq \R$.

\point{Terminology:} the definition of integral we will cover today is called the Lebesgue integral (not to be confused with the Lebesgue measure).

\point{Note:} we will get the undergraduate definition of expectation of a continuous random variable as a special case arising when $X$ has a density. However, the Lebesgue expectation does not need to assume existence of a density.


\subsection{Notation, inputs and outputs}

The integral you know from calculus (called the Riemann integral) takes one input (a function $f : \R \to \R$) and return one real number.

In contrast, the Lebesgue integral needs \emph{two} inputs:
\begin{enumerate}
  \item a probability measure $\P : \events \to [0, 1]$, where $\events$ is a \sigmaalg\ on a sample space $\Omega$,
  \item a random variable $X : \Omega \to \R$.
\end{enumerate}

\point{Notations from real analysis:} you will see different notations depending on the author/community to encode this operator on two inputs, for example (these are all synonyms):
\begin{itemize}
  \item $\int X \ud \P$
  \item $\int X(\omega) \P(\ud \omega)$
  \item $\P X$
  \item $(\P, \mu)$.
\end{itemize}

In probability theory and Bayesian statistics, there is also often a ``global'' probability $\P$. When this is the case:

\point{Notation from probability theory/Bayesian statistics:} 
\[ \E[X] \defeq \int X \ud \P. \]

In frequentist Bayesian statistics, there is often a collection of probabilities indexed by a parameter $\theta$, i.e. $\{\P_\theta : \theta \in \Theta\}$. When this is the case:

\point{Notation from on frequentist statistics:}
\[ \E_\theta[X] \defeq \int X \ud \P_\theta. \]


\subsection{Generalizing the notion of the ``area'' of a ``rectangle''}

Let us start by defining the notion of area under the curve for something simple: an indicator function multiplied by a constant, $Z = a \1_A$. To make it easier to visualize, let us make this assumption from now on (we will relax it at some point later):

\point{Assumption:} assume that all random variables take values $\ge 0$.

Now if $A$ is an interval, the graph is just a rectangle! The height is $a$. What should be the base? Since we are given a probability, let us use it to measure the base, giving $\P(A)$ for the base. This suggests:

\point{Definition:} the Lebesgue integral for indicator function is given by 
\[ \E[Z] = \int Z \ud \P \defeq a \P(A). \]

\point{Note:} $A$ could actually be complicated (e.g., the Cantor set), but this definition still holds as long as $A \in \events$.

\point{Note:} there will be cases (especially when we talk about limits) where the base has measure zero, but the height is infinite. We would like our definition to return zero in these cases (since the function blows up on a negligeable set). For this reason, we define $0 \times \infty = 0$. 


\subsection{Area under the graph of ``simple functions''}\label{sec:area-simple-function}

\point{Simple function:} a simple function is a random variable of the form
\[ Y = \sum_{i = 1}^N a_i \1_{A_i}, \]
where the $A_i$ are assumed to be disjoint.

\point{Definition:} motivated by linearity we extend the definition of the Lebesgue integral to simple functions:
\[ \E[Y] = \int Y \ud \P \defeq \sum_{i=1}^N a_i \P(A_i). \]

\point{Note:} our previous definition for indicators is just a special case of this, so we are not contradicting ourselves.

\point{Property:} the above definition is motivated by linearity, and important property we want expectations to enjoy:
\[ \E[Y + Y'] = \E[Y] + \E[Y']. \]

\point{Exercise:} prove this property holds for the above definition of integral of simple functions.


\subsection{Area under graph of non-negative random variables}

Now, how to define the area under the graph of an arbitrary non-negative random variable? We make use of our previous definition for simple functions:

\point{Definition:} let $X \ge 0$ (meaning $X(\omega) \ge 0$ for all $\omega \in \Omega$), 
\[ \E[X] = \int X \ud \P \defeq \sup\left\{ \int Y \ud \P : Y\text{ is simple and } 0 \le Y \le X\right\}. \] 

\point{Exercises:} show $\E[X] \ge 0$ and monotonicity: $X \le Y \Longrightarrow \E[X] \le \E[Y]$. 

\point{End of lecture 3 (Sep 16)}


\subsection{Algorithmic construction}

To avoid having to deal with an uncountable collection we follow a two steps strategy:
\begin{enumerate}
  \item expresss the random variable to integrate $X$ as the limit of a sequence of increasing and simple random variables. This is done using: \\\point{Proposition:} (approximation by simple functions) For any random variable $X \ge 0$, there exists a sequence of random variables $0 \le Y_1 \le Y_2 \le \dots$ such that:
   \begin{enumerate}
     \item each $Y_n$ is simple, and
     \item for all $\omega \in \Omega$, $\lim_{n\to\infty} Y_n(\omega) = X(\omega)$ (this property is known as \point{pointwise convergence}, denoted $Y_n \to X$, or in this case since the r.v. are additionally increasing, $Y_n \uparrow X$).
   \end{enumerate}
  \item We will use the \point{monotone convergence theorem (MCT)} to exchange the limit and integral: if $0 \le Y_1 \le Y_2 \le \dots$ are non-negative random variables (not necessarily simple, although they are in this specific context), then 
\[ \underbrace{ \int (\lim_{n\to\infty} X_n) \ud \P}_{\text{hard!}} = \lim_{n\to\infty} \underbrace{\int X_n \ud \P.}_{\text{easier!}} \]
\end{enumerate}

\point{Proof idea} of the proposition on approximation by simple functions: 
\begin{enumerate}
  \item Recall that in the case of a Riemann integral, we do something similar, i.e. breaking the x-axis of the graph of $X$ into a grid and making this grid finer and finer.
  \item In general, this cannot work here, because the x-axis, $\Omega$, is not necessarily $\R$. 
  \item Idea: break the $y$-axis instead! Then use the inverse of the random variable $X^{-1}$, to get the $A_i$'s required in the definition of simple functions.
\end{enumerate}


\subsection{Proving tool: simple function approximation +  MCT}\label{sec:approxAndMCT}

The previous section sets the stage for a powerful proving strategy:
\begin{enumerate}
  \item Suppose you want to prove an identity involving expectations.\\\point{Example:} linearity for non-negative random variables $X, X' \ge 0$, \[ \E[X + X'] = \E[X] + \E[X']. \]
  \item First prove that the identity holds for simple random variables \\\point{Example:} that was an earlier exercise in the case of linearity.
  \item Then, use the approximation theorem to get simple $Y_n \uparrow X$ and $Y'_n \uparrow X'$. 
  \item Use MCT to conclude. \\\point{Example:}
\begin{eqnarray*}
\E[X + X'] &=& \E[ \lim (Y_n + Y'_n) ] \\
&=& \lim \E [Y_n + Y'_n] \;\;\;\text{ (we can use MCT here since }0 \le Y_n + Y'_n \uparrow X + X'\text{)} \\
&=& \lim ( \E[Y_n] + \E[Y'_n] ) \;\;\;\text{ (easy to prove since }Y_n, Y'_n\text{ are simple)} \\
&=& \lim \E[Y_n] + \lim \E[Y'_n] \;\;\;\text{ (properties of limits of real sequences)} \\
&=& \E[X] + \E[X'] \;\;\;\text{ (MCT again, twice).}
\end{eqnarray*}
\end{enumerate}

\point{Easy extension:} $\E$ is a linear operator: $\E[aX + b] = a\E[X] + b$.


\subsection{Integrals of random variables taking negative values}

For a general random variable $X$: 
\begin{enumerate}
  \item Write $X = X^{+} + X^{-}$, where $X^{+}$ and $X^{-}$ are the negative and positive parts respectively. For example, $X^{-} = \1[X < 0] X$. 
  \item Note: $-X^{-}$ is non-negative.
  \item Compute $I^{+} \defeq \E[X^+]$ and $I^{-} \defeq \E[-X^-]$.
  \item If both $I^{+} = I^- = \infty$, return an error (``the Lebesgue integral is not defined''),
  \item else define $\E[X] \defeq I^{+} - I^{-}$.
\end{enumerate}

\point{Terminology:} 
\begin{itemize} 
  \item If at least one of $I^{+}$ and $I^-$ is finite, we say the Lebesgue integral of $X$ is defined,
  \item when both $I^{+}$ and $I^-$ are finite, we say $X$ is \emph{integrable}, denoted $X \in \L_1$. 
\end{itemize}

\point{Exercise:} find a random variable $X \neq \L_1$ such that $\E X$ exists. Find a random variable $Y$ where $\E Y$ is not defined. 

\point{End of lecture 4 (Sep 21)}


\subsection{Integrals with respect to a measure}

So far, we have assumed that the Lebesgue integral was computed with respect to probability measure $\P : \events \to [0, 1]$. 

\point{Exercise:} go over the above argument again with a measure $\mu : \events \to [0, \infty)$ instead of a probability measure $\P$ and check that everything goes through.

\point{Notes:}
\begin{enumerate}
  \item The previous definitions are again special case of the new one, so we are not contradicting ourselves.
  \item The last definition is always well defined, but could be $+\infty$.
  \item However, the last definition seems hard to compute algorithmically because the sup is over an uncountable collection.
\end{enumerate}


\subsection{More on exchanging limits and integrals}

The monotone convergence theorem (Section~\ref{sec:approxAndMCT}) says that we can exchange integrals and limits \emph{when the sequence of function is increasing} ($X_1 \le X_2 \le \dots$). Is this necessary?

\point{Example showing that it is:} consider $X_n = n \1[0 < X_n \le 1/n]$. 

\point{Exercise:} compute $\lim X_n$ and $\E X_n$. Conclude that limits and integrals cannot be exchanged in this case.

\point{However:} there are non-monotone cases where you can exchange limits and integrals. For example, when they have \point{an integrable envelope}, defined as a random variable such that:
\begin{enumerate}
  \item $|X_i| \le Y$,
  \item $\E|Y| < \infty$.
\end{enumerate}

\point{Theorem:} (Dominated Convergence Theorem, DCT) if $X_1, X_2, \dots$ have an integrable envelope, and $\lim X_i$ exists, then $\lim \E X_i = \E \lim X_i$.


\subsection{Measure zero sets and almost sure statements}

As a direct consequence of the axioms of probability, an empty event has probability zero: $\P(\emptyset) = 0$. The converse is not true though. For example under the uniform probability an event that contains only a single point still has probability zero $\P(\{0.2\}) = 0$. In fact, under the uniform probability, events containing countably many points have probability zero.\footnote{Even more surprising, there are set containing uncountably many points that still have probability zero. Read about the Cantor set if you are curious.} 

For this reason, it is often possible to relax a statement like ``$|X_i| \le Y$'' in the previous statement to a statement like ``$|X_i| \le Y$ except for a set of probability zero.'' This second statement is formalized as $\P(|X_i| \le Y \text{ for all }i) = 1$, and denoted ``$|X_i| \le Y$ a.s.''


\subsection{Convexity and integration}

\point{Review:} convexity. A function $g : \R \to \R$ is convex if for all $x_1, x_2 \in \R$ and $\lambda \in [0, 1]$, 
\[ \lambda g(x_1) + (1 - \lambda) g(x_2) \ge g(\lambda x_1 + (1 - \lambda) x_2). \]

\point{Exercise:} convince yourself that $\varphi(\E X) \neq \E[\varphi(X)]$ in general (with some exceptions to this, e.g. when $\varphi$ is linear). This is unfortunate because $\varphi(\E X)$ is often easier to compute than $\E(\varphi(X))$.

\point{However:} if $\varphi$ is convex, we can at least get the following bound.

\point{Jensen's inequality:} if $\varphi$ is convex, then $\varphi(\E X) \le \E[\varphi(X)]$.

\point{Proof:} to prove Jensen's inequality, we will use the following result from convex analysis:

\point{Lemma:} for all convex function $\varphi$, there is a sequence of linear functions $L_n(x) = a_n x + b_n$ such that 
\[ \varphi(x) = \sup_n L_n(x). \]

\point{Exercise:} provide an example showing that the above lemma does not hold in the case for non-convex functions.

\point{Back to Jensen's:} using our lemma, we have $\varphi(X) \ge L_n(X)$ by construction. ``Taking expectations on both sides'' (i.e. using monotonicity of expectations):
\begin{eqnarray}
\E\varphi(X) &\ge& \E[L_n(X)] \\
&=& L_n(\E X) \;\text{ since }L_n\text{ is linear.}
\end{eqnarray}
Finally, taking sup over $n$ on both sides:
\begin{eqnarray}
\E[\varphi(X)] &\ge& \sup L_n(\E X) \\ 
&=& \varphi(\E X).
\end{eqnarray}

\point{Example:} since $\varphi(x) = x^2$ is convex, Jensen's gives us the following inequality: $(\E X)^2 \le \E[X^2]$.

\point{Application:} the variance is defined as $\var[X] \defeq \E[(X - \mu)^2]$, where $\mu = \E X$. By computing the square and linearity, this last equation is equal to $\E[X^2] - (\E X)^2$.  Therefore, Jensen's inequality gives us another proof that the variance is non-negative.


\subsection{Markov's inequality and its friends}

\point{Motivation:} let $X$ be the water level near a dam of height of 7.5m. What is the probability of a flood? All you know is that the mean water level is 5m.

\point{Proposition (Markov's inequality):} if $X \ge 0$, then for all $\alpha \ge 0$, 
\[ \P( X \ge \alpha) \le \frac{\E X}{\alpha}. \]

\point{Exercise:} solve the dam problem using Markov's inequality.

\point{Proof:} Convince yourself by drawing the graph of $X$ that the following identities hold: 
\begin{eqnarray*} 
X &\ge& \1[X \ge \alpha] X\;\text{ (follows from }X \ge 0\text{)} \\
&\ge& \alpha \1[X \ge \alpha].
\end{eqnarray*}
Taking expectations on both sides:
\begin{eqnarray*}
\E X &\ge& \alpha \E[\1[X \ge \alpha]] \\
&=& \alpha \P(X \ge \alpha).
\end{eqnarray*}

\point{Note:} non-negativity is necessary. Consider for example a random variable with the discrete uniform distribution on $\{-1, +1\}$.

\point{Note:} the bound will sometimes be greater than one. On the other hand, there are random variables $X$ and $\alpha$ such that the bound it tight, i.e. where $\P( X \ge \alpha) = \frac{\E X}{\alpha}$. For example $X$ such that $\P(X = 2) = 1/2$, $\P(X = 0) = 1/2$, $\alpha = 2$.

\point{Corollary 1:} for all random variable $X$, $p \ge 0$,
\[ \P(|X| \ge \alpha) \le \frac{\E |X|^p}{\alpha^p}. \]

\point{Proof:} by monotonicity of the power function, $(|X| \ge \alpha) = (|X|^p \ge \alpha^p)$, therefore $\P(|X| \ge \alpha) = \P(|X|^p \ge \alpha^p)$. Since $|X|^p \ge 0$, we can apply Markov's inequality on this last probability.

% note: does not work well with the dam example because of the left tail

\point{Corollary 2 (Chebyshev's inequality):} for any random variable $Y$ with $\mu \defeq \E Y$, $|\mu| < \infty$ (non-negativity not needed anymore!),
\[ \P(|Y - \mu| \ge \alpha) \le \frac{\var Y}{\alpha^2}. \]

\point{Proof:} define $X \defeq |Y - \mu|$, and apply the preceding lemma with $p = 2$.



\subsection{Exercises}

\begin{enumerate}
  \item Solve the first exercise (linearity in the case of simple functions) of Section~\ref{sec:area-simple-function}.
\end{enumerate}


\subsection{Solutions}

\begin{enumerate}
  \item First, we show that $Y + Y'$ is a simple function (this will allow us to use the easy definition of expectation given in Section~\ref{sec:area-simple-function}). Write $Y = \sum_{i=1}^N a_i \1_{A_i}$, and $Y' = \sum_{j=1}^M b_j \1{B_j}$ and assume without loss of generality that $A_i$ forms a partition, and that $B_j$ forms another partition.\footnote{Otherwise, add one more block $A_{N+1} = \cup_{i=1}^N A_i$ with $a_{N+1} = 0$. Same for $B_j$.} Consider the sets $\{C_{i,j} = A_i \cap B_j : 1\le i \le N, 1\le j \le M\}$. Note that the $C_{i,j}$ are clearly disjoint, and that on $C_{i,j}$, $(Y + Y')$ is equal to $a_i + b_j$. Therefore:
\begin{eqnarray*}
\E[Y + Y'] &=& \sum_{i,j} (a_i + b_j) \1_{C_{i,j}} \\
&=& \sum_{i,j} a_i \1_{C_{i,j}} + \sum_{i,j} b_j \1_{C_{i,j}} \\
&=& \sum_{i} a_i \sum_j \1_{C_{i,j}} + \sum_{j} b_j \sum_i \1_{C_{i,j}} \\
&=& \sum_{i} a_i  \1_{\cup_j C_{i,j}} + \sum_{j} b_j  \1_{\cup_i C_{i,j}} \;\text{(since the }C_{i,j}\text{ are disjoint)} \\
&=& \sum_{i} a_i  \1_{A_i} + \sum_{j} b_j  \1_{B_j} \;\text{(since }\{A_{i}\}\text{ is a partitions, similarly for }\{B_{j}\} \text{ )} \\
&=& \E[Y] + \E[Y'].
\end{eqnarray*}
\end{enumerate}


\section{Independence}

\subsection{More than one random variables (random vectors)}

\point{Motivation:} A man and a woman try to meet at a certain place between 1:00pm and 2:00pm.  Suppose each person pick an arrival time between 1:00pm and 2:00pm uniformly at random (denote $X$ and $Y$ respectively), and waits for the other at most 10 minutes. What is the probability that they meet?

\point{Practical question:} How to compute a probability of the form $\P((X, Y) \in S)$?

\point{Theoretical question:} Given two random variables on the same space, $X: \Omega \to \R$ and $Y : \Omega \to \R$, what is $(X, Y)$ exactly?

\point{Definition:} A random vector is a random variable that takes values in $\states = \R^2$: \[ (X, Y) : \Omega \to \R^2. \]

\point{Recall:} The definition of random variable requires a \sigmaalg\ on $\states$. What should we pick? We know that we will at the very least need to compute the probability of $X$ falling in a \emph{rectangle}:

\point{Definition:} a rectangle is a set of the form:
\begin{eqnarray} 
R &=& A \times B \;\text{ for some }A \in \borels, B \in \borels \\
&=& \{(x, y) : x \in A, y \in B\}.
\end{eqnarray}

\point{Unfortunately:} $\generators = \{R : R\text{ is a rectangle}\}$ is not a \sigmaalg\ (why?). Also, it does not contain the set that we would need to answer the above ``practical question.''

\point{Solution:} generate a \sigmaalg\ from $\generators$. The result of this is called the product \sigmaalg:
\[ \events^{\otimes 2} \defeq \events \otimes \events \defeq \sigma(\generators). \]

\point{Exercise:} show that the set $S$ in the ``practical question'' is in $\events \otimes \events$.


\subsection{Distribution, CDF and density of a random vector}

These definitions follow directly from the univariate case:

\point{Definition:} the joint distribution of a random vector $(X, Y)$, denoted $\P_{X,Y} : \events \otimes \events \to [0, 1]$, is defined by:
\[ \P_{X,Y}(S) \defeq \P((X, Y) \in S). \]

\point{Definition:} the joint CDF of a random vector $(X, Y)$, denoted $F_{X,Y} : \R^2 \to [0, 1]$, is defined by:
\[ F_{X, Y}(x, y) \defeq \P_{X,Y}((-\infty, x] \times (-\infty, y]). \]

\point{Note:} This last definition seems slightly arbitrary. Why pick this particular class of sets $S$ (``quarter-planes'')? As we will see in the next section, this is because this class \emph{characterizes} the joint distribution. In other words, given a joint CDF, you can in principle obtain the probability of $X$ falling in any set $S$. 

\point{Definition:} a function $f:\R^2 \to [0, \infty)$ is called a joint density of the random vector $(X,Y)$ if:
\[ F_{(X,Y)}(x,y) = \int_{-\infty}^x \int_{-\infty}^y f(x,y) \ud x \ud y. \]

\point{Note:} these definitions can be generalized to more than two dimensions.


\subsection{Determining classes}

In this section, we explain the tool used to prove the characterization statement of the previous section.

\point{Tool:} $\pi$-$\lambda$ theorem. 
\begin{enumerate}
\item Let $\pi$ denote a collection of sets satisfying the following condition (called a $\pi$-system condition):
\begin{enumerate}
  \item $A, B \in \pi \Longrightarrow A\cap B\in \pi$.
\end{enumerate}
\item Let $\lambda$ denote a collection of sets satisfying the following conditions (called a $\lambda$-system condition):
\begin{enumerate}
  \item $\Omega \in \lambda$,
  \item $A, B \in \lambda, A \subseteq B \Longrightarrow B \backslash A \in \lambda$,
  \item $A_i \uparrow A, A_i \in \lambda \Longrightarrow A \in \lambda.$
\end{enumerate}
\item Then, the following holds: $\pi \subset \lambda \Longrightarrow \sigma(\pi) \subset \lambda$.
\end{enumerate}

\point{Proposition:} Supposet $\P_1$ and $\P_2$ are probability measures on $\events = \sigma(\generators)$, where $\generators$ is closed under finite intersection. Then the following are equivalent:
\begin{enumerate}
  \item $\P_1(A) = \P_2(A)$ for all $A \in \events$,
  \item $\P_1(A) = \P_2(A)$ for all $A \in \generators$.
\end{enumerate}

\point{Exercise:} prove this using the $\pi$-$\lambda$ theorem. Hint: let $\lambda = \{A \in \events : \P_1(A) = \P_2(A)\}$.

\point{Corollary:} $F_{X,Y}$ determines $\P_{X,Y}$.

\point{Proof:} $(-\infty, x] \cap (-\infty, y] = (-\infty, \min\{x, y\}]$.

\point{Corollary:} if $(X,Y)$ has joint density $f$, then
\[ \P((X, Y) \in S) = \int_S f(x,y) \ud x \ud y. \]

%%new-fig:name


\subsection{Independence of random vectors}

\point{Definition:} the random variables $X_1, X_2, \dots, X_n : \Omega \to \R$ are independent if, for all $A_i \in \borels$, \[ \P(X_1 \in A_1, X_2 \in A_2, \dots, X_n \in A_n) = \prod_{i=1}^n \P(X_i \in A_i). \]

\point{Exercise:} using $\pi-\lambda$ show that the above statement can be checked by showing that the CDF factorizes as 
\[ F_{X_1, X_2, \dots, X_n} = \prod_{i=1}^n F_{X_i}. \]

\point{Note:} similarly to the above exercise, if the random variables have a joint density $f_{X_1, X_2, \dots, X_n}$, independence can be checked by factorizing the density into \emph{marginal densities} $f_{X_i}$ of each individual random variable:
\[ f_{X_1, X_2, \dots, X_n} = \prod_{i=1}^n f_{X_i}. \]

\point{Definition:} we say the random variables $X_1, X_2, \dots, X_n$ are pairwise independent if each pair is independent.

\point{Exercise:} find 3 random variables such that these random variables are pairwise independent but not independent. Hint: this can be done using indicator random variables.


\section{Computing expectations}

% start with simple discrete example and g() = x^2, then prove

% then, introduce density


% need more on expectation --- PLAN: come back via assignment and another section after indep; e.g. will need Fubini for tail sums
% - change of vars
% - actual computations
% - indicator tricks
% - tail sums


% - philosophy: \Omega complicated, not accessible
% - todo: closure of r.v. under limits
% - add note that symbols are clickeable
% - todo: double-check statements of monotone/dominated does not require existence of lim 


\end{document}
